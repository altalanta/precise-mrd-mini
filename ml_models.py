"""Advanced machine learning models for MRD variant calling."""

from __future__ import annotations

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
import warnings

from .config import PipelineConfig
from .performance import ml_performance_decorator, get_ml_performance_tracker

# ML model imports with fallbacks
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, average_precision_score, classification_report


class EnhancedFeatureEngineer:
    """Enhanced automated feature engineering for variant calling."""

    def __init__(self, config: PipelineConfig):
        """Initialize enhanced feature engineer.

        Args:
            config: Pipeline configuration
        """
        self.config = config
        self.feature_names = []

    def extract_comprehensive_features(self, collapsed_df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """Extract comprehensive features from collapsed UMI data.

        Args:
            collapsed_df: DataFrame with collapsed UMI data

        Returns:
            Tuple of (feature_matrix, feature_names)
        """
        features = []

        # Basic features
        basic_features = [
            'family_size',
            'quality_score',
            'consensus_agreement',
            'allele_fraction'
        ]

        for feature in basic_features:
            if feature in collapsed_df.columns:
                features.append(collapsed_df[feature].values.reshape(-1, 1))

        # Interaction features
        if 'family_size' in collapsed_df.columns and 'quality_score' in collapsed_df.columns:
            family_quality_ratio = collapsed_df['family_size'] / (collapsed_df['quality_score'] + 1)
            features.append(family_quality_ratio.values.reshape(-1, 1))

        if 'consensus_agreement' in collapsed_df.columns and 'allele_fraction' in collapsed_df.columns:
            confidence_af = collapsed_df['consensus_agreement'] * collapsed_df['allele_fraction']
            features.append(confidence_af.values.reshape(-1, 1))

        # Statistical features
        if 'family_size' in collapsed_df.columns:
            # Family size statistics
            family_sizes = collapsed_df['family_size'].values
            features.append(np.log1p(family_sizes).reshape(-1, 1))  # Log transform
            features.append((family_sizes - np.mean(family_sizes)) / (np.std(family_sizes) + 1e-8).reshape(-1, 1))  # Z-score

        # Quality-based features
        if 'quality_score' in collapsed_df.columns:
            quality_scores = collapsed_df['quality_score'].values
            features.append(np.sqrt(quality_scores).reshape(-1, 1))  # Square root transform

        # Combine all features
        if features:
            X = np.hstack(features)
            self.feature_names = basic_features[:len(features)]

            # Add derived feature names
            derived_names = []
            if len(features) > len(basic_features):
                derived_names.extend(['family_quality_ratio', 'confidence_af'])
            if len(features) > len(basic_features) + 2:
                derived_names.extend(['log_family_size', 'zscore_family_size'])
            if len(features) > len(basic_features) + 3:
                derived_names.extend(['sqrt_quality'])

            self.feature_names.extend(derived_names)
        else:
            # Fallback if no features available
            X = np.zeros((len(collapsed_df), 1))
            self.feature_names = ['dummy_feature']

        return pd.DataFrame(X, columns=self.feature_names), self.feature_names

    def select_optimal_features(self, X: pd.DataFrame, y: np.ndarray,
                               method: str = 'mutual_info', k: int = None) -> Tuple[pd.DataFrame, List[str]]:
        """Select optimal features using multiple methods.

        Args:
            X: Feature matrix
            y: Target labels
            method: Feature selection method
            k: Number of features to select (None for automatic)

        Returns:
            Tuple of (selected_features, selected_feature_names)
        """
        if k is None:
            # Automatic selection based on data size
            k = min(max(5, X.shape[1] // 3), X.shape[1])

        if method == 'f_classif':
            selector = SelectKBest(score_func=f_classif, k=k)
        elif method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
        else:
            raise ValueError(f"Unknown feature selection method: {method}")

        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()].tolist()

        return pd.DataFrame(X_selected, columns=selected_features), selected_features


class GradientBoostedVariantCaller:
    """Gradient-boosted models for enhanced variant calling."""

    def __init__(self, config: PipelineConfig, model_type: str = 'xgboost'):
        """Initialize gradient-boosted variant caller.

        Args:
            config: Pipeline configuration
            model_type: Type of gradient boosting model ('xgboost', 'lightgbm', 'sklearn_gbm')
        """
        self.config = config
        self.model_type = model_type
        self.model = None
        self.feature_engineer = EnhancedFeatureEngineer(config)
        self.scaler = StandardScaler()
        self.selected_features = []

    @ml_performance_decorator
    def train_model(self, collapsed_df: pd.DataFrame, rng: np.random.Generator) -> Dict[str, Any]:
        """Train gradient-boosted model for variant calling.

        Args:
            collapsed_df: DataFrame with collapsed UMI data
            rng: Random number generator for reproducibility

        Returns:
            Dictionary with training results and performance metrics
        """
        print(f"  Training {self.model_type} model...")

        # Extract features and labels
        X, feature_names = self.feature_engineer.extract_comprehensive_features(collapsed_df)

        # Use is_variant as target (or generate synthetic labels if not available)
        if 'is_variant' in collapsed_df.columns:
            y = collapsed_df['is_variant'].values
        else:
            # Generate synthetic labels based on allele fraction and quality
            y = self._generate_synthetic_labels(collapsed_df, rng)

        # Handle class imbalance
        pos_ratio = np.mean(y)
        print(f"  Positive class ratio: {pos_ratio:.3f}")

        # Feature selection
        X_selected, selected_features = self.feature_engineer.select_optimal_features(
            X, y, method='mutual_info'
        )
        self.selected_features = selected_features

        print(f"  Selected {len(selected_features)} features: {selected_features[:5]}{'...' if len(selected_features) > 5 else ''}")

        # Scale features
        X_scaled = self.scaler.fit_transform(X_selected)

        # Create model based on type
        if self.model_type == 'xgboost' and XGBOOST_AVAILABLE:
            model = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.config.seed,
                eval_metric='logloss'
            )
        elif self.model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            model = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.config.seed,
                verbosity=-1
            )
        else:
            # Fallback to sklearn GBM
            model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                random_state=self.config.seed
            )

        # Cross-validation for model evaluation
        cv_scores = cross_val_score(
            model, X_scaled, y,
            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=self.config.seed),
            scoring='roc_auc'
        )

        print(f"  CV AUC: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

        # Train model on full data
        model.fit(X_scaled, y)
        self.model = model

        # Get predictions for evaluation
        predictions = model.predict_proba(X_scaled)[:, 1]

        # Calculate optimal threshold
        from .advanced_stats import AdaptiveThresholdOptimizer
        threshold_optimizer = AdaptiveThresholdOptimizer(self.config)
        optimal_threshold = threshold_optimizer.optimize_threshold(predictions, y, metric='f1')

        # Record performance metrics
        tracker = get_ml_performance_tracker()
        model_metrics = {
            'roc_auc': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'n_features': len(selected_features),
            'positive_ratio': pos_ratio
        }

        # Get additional metrics if ground truth is available
        if 'is_variant' in collapsed_df.columns:
            try:
                from sklearn.metrics import roc_auc_score, average_precision_score
                model_metrics['test_roc_auc'] = roc_auc_score(y, predictions)
                model_metrics['test_avg_precision'] = average_precision_score(y, predictions)
            except:
                pass

        tracker.record_model_metrics(f"{self.model_type}_model", model_metrics)

        # Record feature importance
        importance = self.get_feature_importance()
        if importance:
            tracker.record_feature_importance(f"{self.model_type}_model", importance)

        # Record prediction distribution
        tracker.record_prediction_distribution(f"{self.model_type}_model", predictions)

        return {
            'model_type': self.model_type,
            'feature_names': selected_features,
            'cv_scores': cv_scores,
            'predictions': predictions,
            'optimal_threshold': optimal_threshold,
            'positive_ratio': pos_ratio,
            'n_features': len(selected_features),
            'model': model,
            'performance_metrics': model_metrics
        }

    def predict_variants(self, collapsed_df: pd.DataFrame) -> np.ndarray:
        """Predict variants using trained gradient-boosted model.

        Args:
            collapsed_df: DataFrame with collapsed UMI data

        Returns:
            Array of variant probabilities
        """
        if self.model is None:
            raise ValueError("Model must be trained before prediction")

        # Extract features
        X, _ = self.feature_engineer.extract_comprehensive_features(collapsed_df)

        # Feature selection (use same features as training)
        X_selected = X[self.selected_features]

        # Scale features
        X_scaled = self.scaler.transform(X_selected)

        # Get predictions
        return self.model.predict_proba(X_scaled)[:, 1]

    def _generate_synthetic_labels(self, collapsed_df: pd.DataFrame, rng: np.random.Generator) -> np.ndarray:
        """Generate synthetic labels for training when ground truth is not available."""
        # Use allele fraction as primary signal
        af_signal = collapsed_df.get('allele_fraction', pd.Series([0.001] * len(collapsed_df)))

        # Add noise based on quality and consensus
        quality_factor = collapsed_df.get('quality_score', pd.Series([25] * len(collapsed_df))) / 50.0
        consensus_factor = collapsed_df.get('consensus_agreement', pd.Series([0.8] * len(collapsed_df)))

        # Combine signals with more sophisticated modeling
        combined_signal = af_signal * quality_factor * consensus_factor

        # Add interaction terms
        if 'family_size' in collapsed_df.columns:
            family_factor = np.log1p(collapsed_df['family_size']) / 5.0  # Log transform and scale
            combined_signal *= (1 + family_factor)

        # Convert to probabilities with noise
        probabilities = np.clip(combined_signal * 50, 0, 1)  # Scale to reasonable range

        # Add realistic noise based on biological variability
        noise = rng.normal(0, 0.15, len(probabilities))
        noisy_probabilities = np.clip(probabilities + noise, 0, 1)

        # Convert to binary labels using adaptive threshold
        threshold = np.percentile(noisy_probabilities, 80)  # Top 20% as variants
        return (noisy_probabilities > threshold).astype(int)

    def get_feature_importance(self) -> Dict[str, float]:
        """Get feature importance scores from trained model.

        Returns:
            Dictionary mapping feature names to importance scores
        """
        if self.model is None:
            return {}

        if hasattr(self.model, 'feature_importances_'):
            return {
                feat_name: importance
                for feat_name, importance in zip(self.selected_features, self.model.feature_importances_)
            }
        elif hasattr(self.model, 'coef_'):
            # For linear models
            return {
                feat_name: abs(coef)
                for feat_name, coef in zip(self.selected_features, self.model.coef_[0])
            }

        return {}


class EnsembleVariantCaller:
    """Ensemble of multiple ML models for robust variant calling."""

    def __init__(self, config: PipelineConfig):
        """Initialize ensemble variant caller.

        Args:
            config: Pipeline configuration
        """
        self.config = config
        self.models = {}
        self.model_weights = {}
        self.feature_engineer = EnhancedFeatureEngineer(config)
        self.scaler = StandardScaler()
        self.selected_features = []

        # Initialize ensemble components
        self._initialize_ensemble()

    def _initialize_ensemble(self):
        """Initialize ensemble of different model types."""
        if XGBOOST_AVAILABLE:
            self.models['xgboost'] = GradientBoostedVariantCaller(self.config, 'xgboost')
        if LIGHTGBM_AVAILABLE:
            self.models['lightgbm'] = GradientBoostedVariantCaller(self.config, 'lightgbm')

        # Always include sklearn models as fallback
        self.models['gbm'] = GradientBoostedVariantCaller(self.config, 'sklearn_gbm')
        self.models['rf'] = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=self.config.seed,
            n_jobs=1
        )

    @ml_performance_decorator
    def train_ensemble(self, collapsed_df: pd.DataFrame, rng: np.random.Generator) -> Dict[str, Any]:
        """Train ensemble of ML models.

        Args:
            collapsed_df: DataFrame with collapsed UMI data
            rng: Random number generator for reproducibility

        Returns:
            Dictionary with ensemble training results
        """
        print("  Training ensemble of ML models...")

        # Extract features once for all models
        X, feature_names = self.feature_engineer.extract_comprehensive_features(collapsed_df)

        # Generate labels
        if 'is_variant' in collapsed_df.columns:
            y = collapsed_df['is_variant'].values
        else:
            y = self.models['gbm']._generate_synthetic_labels(collapsed_df, rng)

        pos_ratio = np.mean(y)
        print(f"  Positive class ratio: {pos_ratio:.3f}")

        # Feature selection
        X_selected, selected_features = self.feature_engineer.select_optimal_features(X, y)
        self.selected_features = selected_features

        print(f"  Selected {len(selected_features)} features for ensemble")

        # Scale features
        X_scaled = self.scaler.fit_transform(X_selected)

        ensemble_results = {}
        model_performances = {}

        # Train each model and evaluate
        for model_name, model in self.models.items():
            print(f"    Training {model_name}...")

            if hasattr(model, 'train_model'):
                # Gradient-boosted models
                results = model.train_model(collapsed_df, rng)
                cv_score = results['cv_scores'].mean()
            else:
                # Traditional sklearn models
                cv_scores = cross_val_score(
                    model, X_scaled, y,
                    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=self.config.seed),
                    scoring='roc_auc'
                )
                cv_score = cv_scores.mean()

                # Train model
                model.fit(X_scaled, y)

                results = {
                    'cv_scores': cv_scores,
                    'predictions': model.predict_proba(X_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_scaled)
                }

            model_performances[model_name] = cv_score
            ensemble_results[model_name] = results

            print(f"    {model_name} CV AUC: {cv_score:.3f}")

        # Calculate model weights based on performance
        max_performance = max(model_performances.values())
        for model_name, performance in model_performances.items():
            # Weight by relative performance (avoid division by zero)
            self.model_weights[model_name] = performance / max_performance if max_performance > 0 else 0.5

        print(f"  Model weights: {self.model_weights}")

        # Generate ensemble predictions
        ensemble_predictions = np.zeros(len(y))
        for model_name, model in self.models.items():
            weight = self.model_weights[model_name]

            if hasattr(model, 'predict_variants'):
                # Use pre-computed predictions if available
                predictions = ensemble_results[model_name]['predictions']
            else:
                # Re-predict for sklearn models
                predictions = model.predict_proba(X_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_scaled)

            ensemble_predictions += weight * predictions

        # Normalize by total weight
        total_weight = sum(self.model_weights.values())
        if total_weight > 0:
            ensemble_predictions /= total_weight

        # Calculate optimal threshold for ensemble
        from .advanced_stats import AdaptiveThresholdOptimizer
        threshold_optimizer = AdaptiveThresholdOptimizer(self.config)
        optimal_threshold = threshold_optimizer.optimize_threshold(ensemble_predictions, y, metric='f1')

        # Record ensemble performance metrics
        tracker = get_ml_performance_tracker()

        # Calculate ensemble metrics
        ensemble_metrics = {
            'roc_auc': np.mean([perf for perf in model_performances.values()]),
            'ensemble_threshold': optimal_threshold,
            'n_models': len(self.models),
            'positive_ratio': pos_ratio,
            'n_features': len(selected_features)
        }

        # Get additional metrics if ground truth is available
        if 'is_variant' in collapsed_df.columns:
            try:
                from sklearn.metrics import roc_auc_score, average_precision_score
                ensemble_metrics['test_roc_auc'] = roc_auc_score(y, ensemble_predictions)
                ensemble_metrics['test_avg_precision'] = average_precision_score(y, ensemble_predictions)
            except:
                pass

        tracker.record_model_metrics("ensemble_model", ensemble_metrics)

        # Record ensemble feature importance
        ensemble_importance = self.get_feature_importance()
        if ensemble_importance:
            tracker.record_feature_importance("ensemble_model", ensemble_importance)

        # Record ensemble prediction distribution
        tracker.record_prediction_distribution("ensemble_model", ensemble_predictions)

        return {
            'model_performances': model_performances,
            'model_weights': self.model_weights,
            'ensemble_predictions': ensemble_predictions,
            'optimal_threshold': optimal_threshold,
            'positive_ratio': pos_ratio,
            'n_features': len(selected_features),
            'individual_results': ensemble_results,
            'performance_metrics': ensemble_metrics
        }

    def predict_variants(self, collapsed_df: pd.DataFrame) -> np.ndarray:
        """Predict variants using trained ensemble.

        Args:
            collapsed_df: DataFrame with collapsed UMI data

        Returns:
            Array of variant probabilities
        """
        # Extract features
        X, _ = self.feature_engineer.extract_comprehensive_features(collapsed_df)
        X_selected = X[self.selected_features]
        X_scaled = self.scaler.transform(X_selected)

        # Get predictions from each model
        ensemble_predictions = np.zeros(X_scaled.shape[0])

        for model_name, model in self.models.items():
            weight = self.model_weights.get(model_name, 0.0)

            if weight > 0:
                if hasattr(model, 'predict_variants'):
                    predictions = model.predict_variants(collapsed_df)
                else:
                    predictions = model.predict_proba(X_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_scaled)

                ensemble_predictions += weight * predictions

        # Normalize by total weight
        total_weight = sum(self.model_weights.values())
        if total_weight > 0:
            ensemble_predictions /= total_weight

        return ensemble_predictions

    def get_feature_importance(self) -> Dict[str, float]:
        """Get average feature importance across ensemble models.

        Returns:
            Dictionary mapping feature names to average importance scores
        """
        total_importance = {}

        for model_name, model in self.models.items():
            if hasattr(model, 'get_feature_importance'):
                importance = model.get_feature_importance()
                weight = self.model_weights.get(model_name, 0.0)

                for feat_name, imp_score in importance.items():
                    total_importance[feat_name] = total_importance.get(feat_name, 0.0) + weight * imp_score

        # Normalize by total weight
        total_weight = sum(self.model_weights.values())
        if total_weight > 0:
            for feat_name in total_importance:
                total_importance[feat_name] /= total_weight

        return total_importance
