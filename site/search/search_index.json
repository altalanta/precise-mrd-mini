{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Precise MRD - Detection Limit Analytics","text":"<p>A ctDNA/UMI toy MRD pipeline with formal detection limit analytics, statistical validation, and comprehensive contamination robustness testing.</p>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"#deterministic-umi-processing","title":"\ud83e\uddec Deterministic UMI Processing","text":"<ul> <li>Modern NumPy RNG API (<code>np.random.default_rng</code>)</li> <li>Reproducible seed management across all components</li> <li>Hash-verified artifact consistency</li> </ul>"},{"location":"#formal-detection-limits","title":"\ud83d\udcca Formal Detection Limits","text":"<ul> <li>LoB (Limit of Blank): 95th percentile of blank measurements</li> <li>LoD (Limit of Detection): AF yielding 95% detection probability with bias-corrected CIs</li> <li>LoQ (Limit of Quantification): Lowest AF meeting precision criteria (CV \u2264 20%)</li> </ul>"},{"location":"#contamination-robustness","title":"\ud83d\udd2c Contamination Robustness","text":"<ul> <li>Index-hopping stress testing with configurable hop rates</li> <li>Barcode collision modeling and impact assessment</li> <li>Cross-sample contamination sensitivity analysis</li> </ul>"},{"location":"#stratified-analysis","title":"\ud83d\udcc8 Stratified Analysis","text":"<ul> <li>Power analysis by trinucleotide context and depth</li> <li>Calibration assessment across AF/depth strata</li> <li>Context-specific error modeling</li> </ul>"},{"location":"#artifact-contract","title":"\ud83c\udfaf Artifact Contract","text":"<ul> <li>Schema-validated JSON outputs</li> <li>Guaranteed artifact paths and structure</li> <li>Complete run context metadata</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>3-command demo (deterministic, &lt;5 minutes):</p> <pre><code>make setup     # Install dependencies and package\nmake smoke     # Run fast end-to-end pipeline  \nmake determinism  # Verify identical hashes across runs\n</code></pre>"},{"location":"#detection-limit-analytics","title":"\ud83d\udccb Detection Limit Analytics","text":""},{"location":"#limit-of-blank-lob","title":"Limit of Blank (LoB)","text":"<p>Simulate pure-blank runs and estimate the 95th percentile of background measurements:</p> <pre><code># Run LoB estimation\nprecise-mrd eval-lob --n-blank-runs 100 --seed 7\n</code></pre> <p>Output: <code>reports/lob.json</code> with background statistics</p>"},{"location":"#limit-of-detection-lod","title":"Limit of Detection (LoD)","text":"<p>Estimate AF yielding 95% detection across depth grid with bias-corrected confidence intervals:</p> <pre><code># Run LoD estimation across AF range\nprecise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7\n</code></pre> <p>Outputs:  - <code>reports/lod_table.csv</code> - LoD values per depth with CIs - <code>reports/lod_curves.png</code> - Detection curves visualization</p>"},{"location":"#limit-of-quantification-loq","title":"Limit of Quantification (LoQ)","text":"<p>Find lowest AF meeting precision criteria (CV \u2264 20%):</p> <pre><code># Run LoQ estimation\nprecise-mrd eval-loq --cv-threshold 0.20 --seed 7\n</code></pre> <p>Output: <code>reports/loq_table.csv</code> - LoQ values per depth</p>"},{"location":"#contamination-stress-testing","title":"\ud83e\uddea Contamination Stress Testing","text":"<p>Test detection robustness under various contamination scenarios:</p> <pre><code># Run contamination stress tests\nprecise-mrd eval-contamination --hop-rates 0.0,0.001,0.005,0.01 --seed 7\n</code></pre> <p>Outputs: - <code>reports/contam_sensitivity.json</code> - Sensitivity under contamination - <code>reports/contam_heatmap.png</code> - Impact visualization</p>"},{"location":"#stratified-analysis_1","title":"\ud83d\udcca Stratified Analysis","text":"<p>Analyze power and calibration by context and depth:</p> <pre><code># Run stratified analysis\nprecise-mrd eval-stratified --contexts CpG,CHG,CHH,NpN --seed 7\n</code></pre> <p>Outputs: - <code>reports/power_by_stratum.json</code> - Context-specific power - <code>reports/calibration_by_bin.csv</code> - Binned calibration metrics</p>"},{"location":"#reproducibility","title":"\ud83d\udd04 Reproducibility","text":"<p>All analyses are deterministically reproducible:</p> <pre><code># Verify determinism\nmake determinism\n# Should show identical hashes across runs\n\n# Full statistical validation\nmake stat-sanity\n# Validates Type I error control, FDR, bootstrap coverage\n</code></pre>"},{"location":"#performance-metrics","title":"\ud83d\udcc8 Performance Metrics","text":""},{"location":"#expected-detection-limits","title":"Expected Detection Limits","text":"<ul> <li>LoB: ~0.5-2 false positives per 10K UMIs</li> <li>LoD: ~1e-3 to 1e-4 AF (depth-dependent)</li> <li>LoQ: ~5e-3 to 1e-3 AF (20% CV threshold)</li> </ul>"},{"location":"#contamination-tolerance","title":"Contamination Tolerance","text":"<ul> <li>Index hopping: &lt;1% sensitivity loss at 0.5% hop rate</li> <li>Barcode collisions: &lt;5% false positive increase at 0.1% collision rate</li> <li>Cross-contamination: Robust up to 5% cross-sample mixing</li> </ul>"},{"location":"#validation-framework","title":"\ud83c\udfaf Validation Framework","text":""},{"location":"#statistical-sanity-tests","title":"Statistical Sanity Tests","text":"<ul> <li>Type I Error Control: \u03b1-level validation in hypothesis testing</li> <li>FDR Monotonicity: Benjamini-Hochberg correction verification</li> <li>Bootstrap Coverage: CI coverage on synthetic data</li> <li>LoB/LoD Consistency: LoB &lt; LoD monotonicity checks</li> </ul>"},{"location":"#cicd-integration","title":"CI/CD Integration","text":"<p>All tests run in &lt;60s for CI efficiency with fail-closed behavior.</p>"},{"location":"#artifact-structure","title":"\ud83d\udcc1 Artifact Structure","text":"<pre><code>reports/\n\u251c\u2500\u2500 lob.json                    # Limit of Blank results\n\u251c\u2500\u2500 lod_table.csv              # Limit of Detection per depth\n\u251c\u2500\u2500 lod_curves.png             # LoD visualization\n\u251c\u2500\u2500 loq_table.csv              # Limit of Quantification\n\u251c\u2500\u2500 power_by_stratum.json      # Stratified power analysis\n\u251c\u2500\u2500 calibration_by_bin.csv     # Binned calibration metrics\n\u251c\u2500\u2500 contam_sensitivity.json    # Contamination impact\n\u251c\u2500\u2500 contam_heatmap.png         # Contamination visualization\n\u251c\u2500\u2500 metrics.json               # Performance metrics\n\u251c\u2500\u2500 auto_report.html           # Interactive HTML report\n\u251c\u2500\u2500 run_context.json           # Reproducibility metadata\n\u2514\u2500\u2500 hash_manifest.txt          # SHA256 verification\n</code></pre>"},{"location":"#scientific-rigor","title":"\ud83d\udd2c Scientific Rigor","text":""},{"location":"#detection-limit-standards","title":"Detection Limit Standards","text":"<p>Following CLSI EP17 guidelines for detection capability: - LoB: Highest blank measurement (95th percentile) - LoD: 95% detection probability with Type I/II error control - LoQ: Acceptable precision (CV \u2264 20% or absolute error threshold)</p>"},{"location":"#statistical-methodology","title":"Statistical Methodology","text":"<ul> <li>Stratified bootstrap: Bias-corrected confidence intervals</li> <li>Logistic regression: Detection curve fitting with robust estimation</li> <li>Calibration assessment: Expected Calibration Error (ECE) and reliability diagrams</li> </ul>"},{"location":"#development","title":"\ud83d\udee0 Development","text":""},{"location":"#running-tests","title":"Running Tests","text":"<pre><code>make test          # All tests\nmake coverage      # Test coverage\nmake stat-sanity   # Statistical validation only\nmake determinism   # Determinism verification\n</code></pre>"},{"location":"#code-quality","title":"Code Quality","text":"<pre><code>make lint      # Flake8 + mypy\nmake format    # Black + isort\n</code></pre>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"api-reference%202/","title":"API Reference","text":"<p>This page provides comprehensive documentation for the Precise MRD Python API, including all modules for detection limit analytics, contamination testing, and statistical validation.</p>"},{"location":"api-reference%202/#core-modules","title":"Core Modules","text":""},{"location":"api-reference%202/#configuration-management","title":"Configuration Management","text":""},{"location":"api-reference%202/#precise_mrd.config.PipelineConfig","title":"<code>precise_mrd.config.PipelineConfig</code>  <code>dataclass</code>","text":"<p>Enhanced main pipeline configuration with inheritance and validation.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>@dataclass\nclass PipelineConfig:\n    \"\"\"Enhanced main pipeline configuration with inheritance and validation.\"\"\"\n    run_id: str\n    seed: int\n    umi: UMIConfig\n    stats: StatsConfig\n    lod: LODConfig\n    simulation: Optional[SimulationConfig] = None\n    fastq: Optional[FASTQConfig] = None\n    # New fields for enhanced configuration management\n    config_version: str = \"2.0.0\"\n    parent_config: Optional[str] = None\n    template: Optional[str] = None\n    tags: List[str] = field(default_factory=list)\n    description: str = \"\"\n    created_at: Optional[str] = None\n    last_modified: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Validate and enhance configuration after initialization.\"\"\"\n        self._validate()\n        self._set_timestamps()\n\n    def _validate(self):\n        \"\"\"Comprehensive validation of pipeline configuration.\"\"\"\n        if not self.run_id:\n            raise ValueError(\"run_id cannot be empty\")\n        if self.seed &lt; 0:\n            raise ValueError(\"seed must be non-negative\")\n\n        # Validate component configurations\n        self.umi._validate()\n        self.stats._validate()\n        self.lod._validate()\n\n        if self.simulation:\n            self.simulation._validate()\n        if self.fastq:\n            self.fastq._validate()\n\n        # Validate configuration consistency\n        self._validate_consistency()\n\n    def _validate_consistency(self):\n        \"\"\"Validate configuration consistency across components.\"\"\"\n        # Ensure simulation and FASTQ configs don't conflict\n        if self.simulation and self.fastq:\n            # If both are present, ensure they make sense together\n            pass  # Add specific validation rules as needed\n\n    def _set_timestamps(self):\n        \"\"\"Set creation and modification timestamps.\"\"\"\n        import datetime\n        now = datetime.datetime.now().isoformat()\n        if not self.created_at:\n            self.created_at = now\n        self.last_modified = now\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert config to dictionary with metadata.\"\"\"\n        result = asdict(self)\n        result['config_version'] = str(self.config_version)\n        return result\n\n    def config_hash(self) -&gt; str:\n        \"\"\"Compute deterministic hash of configuration.\"\"\"\n        # Include version and parent info for more robust hashing\n        config_str = json.dumps({\n            'version': self.config_version,\n            'parent': self.parent_config,\n            'core_config': {\n                'run_id': self.run_id,\n                'seed': self.seed,\n                'umi': self.umi.__dict__,\n                'stats': self.stats.__dict__,\n                'lod': self.lod.__dict__,\n                'simulation': self.simulation.__dict__ if self.simulation else None,\n                'fastq': self.fastq.__dict__ if self.fastq else None,\n            }\n        }, sort_keys=True)\n        return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n\n    def get_estimated_runtime(self) -&gt; float:\n        \"\"\"Estimate total runtime in minutes.\"\"\"\n        runtime = 0.0\n\n        if self.simulation:\n            runtime += self.simulation.get_estimated_runtime()\n\n        # Add runtime for other pipeline stages\n        # This is a rough estimate - in practice you'd want more sophisticated modeling\n        if self.fastq:\n            # FASTQ processing is I/O bound, estimate based on file size\n            runtime += 5.0  # Base estimate\n\n        return runtime\n\n    def adapt_to_data(self, data_characteristics: Dict[str, Any]) -&gt; 'PipelineConfig':\n        \"\"\"Create an adapted configuration based on data characteristics.\"\"\"\n        adapted = copy.deepcopy(self)\n\n        # Adapt simulation configuration if present\n        if adapted.simulation and 'variant_frequencies' in data_characteristics:\n            adapted.simulation = adapted.simulation.adapt_to_data_characteristics(data_characteristics)\n\n        # Adapt UMI configuration based on quality data\n        if 'quality_stats' in data_characteristics:\n            adapted.umi = adapted.umi.adapt_to_data_quality(data_characteristics['quality_stats'])\n\n        # Update metadata\n        adapted.description = f\"Auto-adapted from {self.run_id}\"\n        adapted.parent_config = self.run_id\n        adapted.last_modified = None  # Will be set by __post_init__\n\n        return adapted\n\n    def validate_compatibility(self, other: 'PipelineConfig') -&gt; List[str]:\n        \"\"\"Validate compatibility between two configurations.\"\"\"\n        issues = []\n\n        # Check version compatibility\n        if self.config_version != other.config_version:\n            issues.append(f\"Configuration version mismatch: {self.config_version} vs {other.config_version}\")\n\n        # Check for incompatible parameter combinations\n        if (self.simulation and other.fastq) or (self.fastq and other.simulation):\n            issues.append(\"Cannot mix simulation and FASTQ modes\")\n\n        return issues\n\n    def merge_with(self, other: 'PipelineConfig', strategy: str = 'override') -&gt; 'PipelineConfig':\n        \"\"\"Merge this configuration with another using specified strategy.\"\"\"\n        if strategy == 'override':\n            # Other config overrides this config\n            merged = copy.deepcopy(other)\n            merged.run_id = f\"{self.run_id}_merged_{other.run_id}\"\n        elif strategy == 'inherit':\n            # Inherit non-conflicting settings from other config\n            merged = copy.deepcopy(self)\n            # Apply inheritance logic here\n        else:\n            raise ValueError(f\"Unknown merge strategy: {strategy}\")\n\n        merged.parent_config = self.run_id\n        merged.last_modified = None  # Will be set by __post_init__\n        return merged\n\n    def export_template(self) -&gt; Dict[str, Any]:\n        \"\"\"Export configuration as a reusable template.\"\"\"\n        return {\n            'template_name': self.run_id,\n            'description': self.description,\n            'base_config': {\n                'umi': self.umi.__dict__,\n                'stats': self.stats.__dict__,\n                'lod': self.lod.__dict__,\n                'simulation': self.simulation.__dict__ if self.simulation else None,\n            },\n            'tags': self.tags,\n            'version': self.config_version\n        }\n\n    @classmethod\n    def from_template(cls, template: Dict[str, Any], run_id: str = None) -&gt; 'PipelineConfig':\n        \"\"\"Create configuration from template.\"\"\"\n        base_config = template['base_config']\n\n        return cls(\n            run_id=run_id or f\"from_template_{template['template_name']}\",\n            seed=7,  # Default seed\n            umi=UMIConfig(**base_config['umi']),\n            stats=StatsConfig(**base_config['stats']),\n            lod=LODConfig(**base_config['lod']),\n            simulation=SimulationConfig(**base_config['simulation']) if base_config['simulation'] else None,\n            template=template['template_name'],\n            description=f\"Generated from template: {template['template_name']}\",\n            tags=template.get('tags', [])\n        )\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.config.PipelineConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary with metadata.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert config to dictionary with metadata.\"\"\"\n    result = asdict(self)\n    result['config_version'] = str(self.config_version)\n    return result\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.config.PipelineConfig.config_hash","title":"<code>config_hash()</code>","text":"<p>Compute deterministic hash of configuration.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def config_hash(self) -&gt; str:\n    \"\"\"Compute deterministic hash of configuration.\"\"\"\n    # Include version and parent info for more robust hashing\n    config_str = json.dumps({\n        'version': self.config_version,\n        'parent': self.parent_config,\n        'core_config': {\n            'run_id': self.run_id,\n            'seed': self.seed,\n            'umi': self.umi.__dict__,\n            'stats': self.stats.__dict__,\n            'lod': self.lod.__dict__,\n            'simulation': self.simulation.__dict__ if self.simulation else None,\n            'fastq': self.fastq.__dict__ if self.fastq else None,\n        }\n    }, sort_keys=True)\n    return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.config.load_config","title":"<code>precise_mrd.config.load_config(path, auto_migrate=True)</code>","text":"<p>Load configuration from YAML file with validation and optional migration.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def load_config(path: str | Path, auto_migrate: bool = True) -&gt; PipelineConfig:\n    \"\"\"Load configuration from YAML file with validation and optional migration.\"\"\"\n    with open(path, 'r') as f:\n        data = yaml.safe_load(f)\n\n    # Handle legacy configurations without version info\n    if 'config_version' not in data:\n        data['config_version'] = \"1.0.0\"\n\n    # Auto-migrate to latest version if requested\n    if auto_migrate:\n        data = ConfigVersionManager.migrate_config(data)\n\n    # Map old format to new format if needed\n    if 'simulation' in data and data['simulation']:\n        data['simulation'] = _migrate_simulation_config(data['simulation'])\n\n    return PipelineConfig(\n        run_id=data.get('run_id', 'unnamed_run'),\n        seed=data.get('seed', 7),\n        simulation=SimulationConfig(**data['simulation']) if data.get('simulation') else None,\n        umi=UMIConfig(**data['umi']),\n        stats=StatsConfig(**data['stats']),\n        lod=LODConfig(**data['lod']),\n        fastq=FASTQConfig(**data['fastq']) if data.get('fastq') else None,\n        config_version=data.get('config_version', '2.0.0'),\n        description=data.get('description', ''),\n        tags=data.get('tags', [])\n    )\n</code></pre>"},{"location":"api-reference%202/#detection-limit-analytics","title":"Detection Limit Analytics","text":""},{"location":"api-reference%202/#loblodloq-analysis","title":"LoB/LoD/LoQ Analysis","text":""},{"location":"api-reference%202/#precise_mrd.eval.lod.LODAnalyzer","title":"<code>precise_mrd.eval.lod.LODAnalyzer</code>","text":"<p>Analyzer for Limit of Blank, Detection, and Quantification.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>class LODAnalyzer:\n    \"\"\"Analyzer for Limit of Blank, Detection, and Quantification.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.lob_results: Optional[Dict[str, Any]] = None\n        self.lod_results: Optional[Dict[str, Any]] = None\n        self.loq_results: Optional[Dict[str, Any]] = None\n\n    def estimate_lob(self, n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Blank (LoB).\n\n        LoB represents the highest measurement result that is likely to be observed \n        for a blank specimen. Calculated as the 95th percentile of blank measurements.\n\n        Args:\n            n_blank_runs: Number of blank (AF=0) simulations to run\n\n        Returns:\n            Dictionary with LoB results including:\n            - lob_value: 95th percentile of blank test statistics\n            - blank_mean: Mean of blank measurements\n            - blank_std: Standard deviation of blank measurements\n            - blank_measurements: All blank measurement values\n        \"\"\"\n        print(f\"Estimating LoB with {n_blank_runs} blank runs...\")\n\n        # Create blank configuration (AF = 0)\n        blank_config = self._create_blank_config()\n        blank_measurements = []\n\n        for i in range(n_blank_runs):\n            # Use different random seed for each run\n            run_rng = np.random.default_rng(self.config.seed + i)\n\n            # Simulate blank reads (no true variants)\n            reads_df = simulate_reads(blank_config, run_rng)\n\n            # Process through pipeline to get test statistic\n            collapsed_df = collapse_umis(reads_df, blank_config, run_rng)\n            error_model = fit_error_model(collapsed_df, blank_config, run_rng)\n            calls_df = call_mrd(collapsed_df, error_model, blank_config, run_rng)\n\n            # Extract test statistic (number of variant calls in blank)\n            n_variant_calls = len(calls_df[calls_df['variant_call'] == True])\n            blank_measurements.append(n_variant_calls)\n\n        blank_measurements = np.array(blank_measurements)\n\n        # Compute LoB as 95th percentile\n        lob_value = np.percentile(blank_measurements, 95)\n        blank_mean = np.mean(blank_measurements)\n        blank_std = np.std(blank_measurements)\n\n        self.lob_results = {\n            'lob_value': float(lob_value),\n            'blank_mean': float(blank_mean),\n            'blank_std': float(blank_std),\n            'blank_measurements': blank_measurements.tolist(),\n            'n_blank_runs': n_blank_runs,\n            'percentile': 95,\n            'config_hash': self.config.config_hash()\n        }\n\n        print(f\"LoB estimated: {lob_value:.3f} (mean: {blank_mean:.3f}, std: {blank_std:.3f})\")\n        return self.lob_results\n\n    def estimate_lod(self, \n                     af_range: Tuple[float, float] = (1e-4, 1e-2),\n                     depth_values: List[int] = [1000, 5000, 10000],\n                     n_replicates: int = 50,\n                     target_detection_rate: float = 0.95,\n                     alpha: float = 0.05,\n                     beta: float = 0.05) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Detection (LoD).\n\n        LoD is the lowest analyte concentration likely to be reliably detected.\n        Calculated as the concentration yielding 95% detection probability.\n\n        Args:\n            af_range: Range of allele fractions to test (min, max)\n            depth_values: UMI depths to test\n            n_replicates: Number of replicates per AF/depth combination\n            target_detection_rate: Target detection rate (e.g., 0.95 for 95%)\n            alpha: Type I error rate\n            beta: Type II error rate\n\n        Returns:\n            Dictionary with LoD results for each depth\n        \"\"\"\n        print(f\"Estimating LoD across AF range {af_range} at depths {depth_values}...\")\n\n        # Generate AF grid (log-spaced)\n        af_min, af_max = af_range\n        af_values = np.logspace(np.log10(af_min), np.log10(af_max), 15)\n\n        lod_results = {}\n\n        for depth in depth_values:\n            print(f\"  Processing depth {depth}...\")\n\n            hit_rates = []\n            af_tested = []\n\n            for af in af_values:\n                # Run detection experiments at this AF/depth\n                detection_count = 0\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(self.config.seed + rep * 1000 + int(af * 1e6))\n\n                    # Create config for this AF/depth\n                    test_config = self._create_lod_config(af, depth)\n\n                    # Run pipeline\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Check if variant was detected\n                    if len(calls_df[calls_df['variant_call'] == True]) &gt; 0:\n                        detection_count += 1\n\n                hit_rate = detection_count / n_replicates\n                hit_rates.append(hit_rate)\n                af_tested.append(af)\n\n            # Fit logistic curve to hit rate vs AF\n            lod_af = self._fit_detection_curve(af_tested, hit_rates, target_detection_rate)\n\n            # Compute confidence intervals using bootstrap\n            lod_ci = self._bootstrap_lod_ci(af_tested, hit_rates, target_detection_rate, \n                                         n_bootstrap=200)\n\n            lod_results[depth] = {\n                'lod_af': float(lod_af),\n                'lod_ci_lower': float(lod_ci[0]),\n                'lod_ci_upper': float(lod_ci[1]),\n                'af_values': [float(x) for x in af_tested],\n                'hit_rates': hit_rates,\n                'target_detection_rate': target_detection_rate,\n                'n_replicates': n_replicates\n            }\n\n            print(f\"    LoD at depth {depth}: {lod_af:.2e} AF [{lod_ci[0]:.2e}, {lod_ci[1]:.2e}]\")\n\n        self.lod_results = {\n            'depth_results': lod_results,\n            'af_range': af_range,\n            'target_detection_rate': target_detection_rate,\n            'alpha': alpha,\n            'beta': beta,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.lod_results\n\n    def estimate_loq(self,\n                     af_range: Tuple[float, float] = (1e-4, 1e-2), \n                     depth_values: List[int] = [1000, 5000, 10000],\n                     n_replicates: int = 50,\n                     cv_threshold: float = 0.20,\n                     abs_error_threshold: Optional[float] = None) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Quantification (LoQ).\n\n        LoQ is the lowest concentration at which quantitative measurements can be made\n        with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).\n\n        Args:\n            af_range: Range of allele fractions to test\n            depth_values: UMI depths to test  \n            n_replicates: Number of replicates per AF/depth combination\n            cv_threshold: Maximum acceptable coefficient of variation\n            abs_error_threshold: Maximum acceptable absolute error (optional)\n\n        Returns:\n            Dictionary with LoQ results for each depth\n        \"\"\"\n        print(f\"Estimating LoQ with CV threshold {cv_threshold:.1%}...\")\n\n        af_min, af_max = af_range\n        af_values = np.logspace(np.log10(af_min), np.log10(af_max), 12)\n\n        loq_results = {}\n\n        for depth in depth_values:\n            print(f\"  Processing depth {depth}...\")\n\n            cv_values = []\n            abs_errors = []\n            af_tested = []\n\n            for af in af_values:\n                estimated_afs = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(self.config.seed + rep * 2000 + int(af * 1e6))\n\n                    # Create config for this AF/depth\n                    test_config = self._create_lod_config(af, depth)\n\n                    # Run pipeline and estimate AF\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Estimate AF from variant calls\n                    if len(calls_df) &gt; 0:\n                        # Simple AF estimation: variants detected / total UMIs\n                        estimated_af = len(calls_df[calls_df['variant_call'] == True]) / len(calls_df)\n                        estimated_afs.append(estimated_af)\n                    else:\n                        estimated_afs.append(0.0)\n\n                if len(estimated_afs) &gt; 1:\n                    mean_af = np.mean(estimated_afs)\n                    std_af = np.std(estimated_afs)\n                    cv = std_af / mean_af if mean_af &gt; 0 else np.inf\n                    abs_error = abs(mean_af - af)\n\n                    cv_values.append(cv)\n                    abs_errors.append(abs_error)\n                    af_tested.append(af)\n\n            # Find LoQ as lowest AF meeting precision criteria\n            loq_af_cv = None\n            loq_af_abs = None\n\n            # CV-based LoQ\n            for i, cv in enumerate(cv_values):\n                if cv &lt;= cv_threshold:\n                    loq_af_cv = af_tested[i]\n                    break\n\n            # Absolute error-based LoQ (if threshold provided)\n            if abs_error_threshold is not None:\n                for i, abs_err in enumerate(abs_errors):\n                    if abs_err &lt;= abs_error_threshold:\n                        loq_af_abs = af_tested[i]\n                        break\n\n            loq_results[depth] = {\n                'loq_af_cv': float(loq_af_cv) if loq_af_cv is not None else None,\n                'loq_af_abs_error': float(loq_af_abs) if loq_af_abs is not None else None,\n                'af_values': [float(x) for x in af_tested],\n                'cv_values': cv_values,\n                'abs_errors': abs_errors,\n                'cv_threshold': cv_threshold,\n                'abs_error_threshold': abs_error_threshold,\n                'n_replicates': n_replicates\n            }\n\n            cv_str = f\"{loq_af_cv:.2e}\" if loq_af_cv else \"Not found\"\n            print(f\"    LoQ (CV) at depth {depth}: {cv_str} AF\")\n\n        self.loq_results = {\n            'depth_results': loq_results,\n            'af_range': af_range,\n            'cv_threshold': cv_threshold,\n            'abs_error_threshold': abs_error_threshold,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.loq_results\n\n    def generate_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate LoB/LoD/LoQ reports and visualizations.\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save LoB results\n        if self.lob_results:\n            lob_payload = dict(self.lob_results)\n            lob_payload[\"schema_version\"] = \"1.0.0\"\n            lob_path = output_path / \"lob.json\"\n            with open(lob_path, 'w') as f:\n                json.dump(lob_payload, f, indent=2)\n            print(f\"LoB results saved to {lob_path}\")\n\n        # Save LoD results\n        if self.lod_results:\n            self._save_lod_table(output_path)\n            self._plot_lod_curves(output_path)\n\n        # Save LoQ results  \n        if self.loq_results:\n            self._save_loq_table(output_path)\n\n    def _create_blank_config(self) -&gt; PipelineConfig:\n        \"\"\"Create configuration for blank (AF=0) experiments.\"\"\"\n        blank_config = PipelineConfig(\n            run_id=f\"{self.config.run_id}_blank\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[0.0],  # Blank samples\n                umi_depths=self.config.simulation.umi_depths[:1],  # Use first depth\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n        return blank_config\n\n    def _create_lod_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for LoD/LoQ experiments.\"\"\"\n        lod_config = PipelineConfig(\n            run_id=f\"{self.config.run_id}_lod\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n        return lod_config\n\n    def _fit_detection_curve(self, af_values: List[float], hit_rates: List[float], \n                           target_rate: float = 0.95) -&gt; float:\n        \"\"\"Fit logistic curve to detection data and find LoD.\"\"\"\n        # Convert to log scale for fitting\n        log_af = np.log10(af_values)\n\n        # Fit logistic regression\n        from scipy.optimize import curve_fit\n\n        def logistic(x, a, b):\n            return 1 / (1 + np.exp(-(a * x + b)))\n\n        try:\n            popt, _ = curve_fit(logistic, log_af, hit_rates, maxfev=2000)\n            a, b = popt\n\n            # Solve for AF giving target detection rate\n            # target_rate = 1 / (1 + exp(-(a * log_af + b)))\n            # Solving: log_af = (logit(target_rate) - b) / a\n            logit_target = np.log(target_rate / (1 - target_rate))\n            log_af_lod = (logit_target - b) / a\n\n            return 10 ** log_af_lod\n\n        except:\n            # Fallback: linear interpolation\n            from scipy.interpolate import interp1d\n            if len(af_values) &gt; 1:\n                interp_func = interp1d(hit_rates, af_values, bounds_error=False, \n                                     fill_value='extrapolate')\n                return float(interp_func(target_rate))\n            else:\n                return af_values[0]\n\n    def _bootstrap_lod_ci(self, af_values: List[float], hit_rates: List[float],\n                         target_rate: float, n_bootstrap: int = 200) -&gt; Tuple[float, float]:\n        \"\"\"Compute bootstrap confidence intervals for LoD.\"\"\"\n        bootstrap_lods = []\n\n        for _ in range(n_bootstrap):\n            # Bootstrap resample\n            indices = self.rng.choice(len(af_values), size=len(af_values), replace=True)\n            boot_af = [af_values[i] for i in indices]\n            boot_hit = [hit_rates[i] for i in indices]\n\n            try:\n                boot_lod = self._fit_detection_curve(boot_af, boot_hit, target_rate)\n                bootstrap_lods.append(boot_lod)\n            except:\n                continue\n\n        if bootstrap_lods:\n            ci_lower = np.percentile(bootstrap_lods, 2.5)\n            ci_upper = np.percentile(bootstrap_lods, 97.5)\n            return ci_lower, ci_upper\n        else:\n            # Fallback\n            return af_values[0], af_values[-1]\n\n    def _save_lod_table(self, output_path: Path) -&gt; None:\n        \"\"\"Save LoD results table as CSV.\"\"\"\n        if not self.lod_results:\n            return\n\n        lod_data = []\n        for depth, results in self.lod_results['depth_results'].items():\n            lod_data.append({\n                'depth': depth,\n                'lod_af': results['lod_af'],\n                'lod_ci_lower': results['lod_ci_lower'],\n                'lod_ci_upper': results['lod_ci_upper'],\n                'target_detection_rate': results['target_detection_rate'],\n                'n_replicates': results['n_replicates']\n            })\n\n        lod_df = pd.DataFrame(lod_data)\n        lod_path = output_path / \"lod_table.csv\"\n        lod_df.to_csv(lod_path, index=False)\n        print(f\"LoD table saved to {lod_path}\")\n\n    def _save_loq_table(self, output_path: Path) -&gt; None:\n        \"\"\"Save LoQ results table as CSV.\"\"\"\n        if not self.loq_results:\n            return\n\n        loq_data = []\n        for depth, results in self.loq_results['depth_results'].items():\n            loq_data.append({\n                'depth': depth,\n                'loq_af_cv': results['loq_af_cv'],\n                'loq_af_abs_error': results['loq_af_abs_error'],\n                'cv_threshold': results['cv_threshold'],\n                'abs_error_threshold': results['abs_error_threshold'],\n                'n_replicates': results['n_replicates']\n            })\n\n        loq_df = pd.DataFrame(loq_data)\n        loq_path = output_path / \"loq_table.csv\"\n        loq_df.to_csv(loq_path, index=False)\n        print(f\"LoQ table saved to {loq_path}\")\n\n    def _plot_lod_curves(self, output_path: Path) -&gt; None:\n        \"\"\"Generate LoD detection curves plot.\"\"\"\n        if not self.lod_results:\n            return\n\n        plt.style.use('default')\n        fig, axes = plt.subplots(1, len(self.lod_results['depth_results']), \n                                figsize=(5 * len(self.lod_results['depth_results']), 4))\n\n        if len(self.lod_results['depth_results']) == 1:\n            axes = [axes]\n\n        for i, (depth, results) in enumerate(self.lod_results['depth_results'].items()):\n            ax = axes[i]\n\n            af_values = results['af_values']\n            hit_rates = results['hit_rates']\n            lod_af = results['lod_af']\n            target_rate = results['target_detection_rate']\n\n            # Plot detection curve\n            ax.semilogx(af_values, hit_rates, 'o-', linewidth=2, markersize=6,\n                       label=f'Observed (n={results[\"n_replicates\"]})')\n\n            # Mark LoD\n            ax.axvline(lod_af, color='red', linestyle='--', linewidth=2,\n                      label=f'LoD = {lod_af:.2e}')\n            ax.axhline(target_rate, color='gray', linestyle=':', alpha=0.7,\n                      label=f'{target_rate:.0%} Detection')\n\n            ax.set_xlabel('Allele Fraction')\n            ax.set_ylabel('Detection Rate')\n            ax.set_title(f'LoD Curve (Depth = {depth})')\n            ax.grid(True, alpha=0.3)\n            ax.legend()\n            ax.set_ylim(-0.05, 1.05)\n\n        plt.tight_layout()\n        lod_plot_path = output_path / \"lod_curves.png\"\n        plt.savefig(lod_plot_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"LoD curves saved to {lod_plot_path}\")\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.LODAnalyzer.estimate_lob","title":"<code>estimate_lob(n_blank_runs=100)</code>","text":"<p>Estimate Limit of Blank (LoB).</p> <p>LoB represents the highest measurement result that is likely to be observed  for a blank specimen. Calculated as the 95th percentile of blank measurements.</p> <p>Parameters:</p> Name Type Description Default <code>n_blank_runs</code> <code>int</code> <p>Number of blank (AF=0) simulations to run</p> <code>100</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoB results including:</p> <code>Dict[str, Any]</code> <ul> <li>lob_value: 95th percentile of blank test statistics</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_mean: Mean of blank measurements</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_std: Standard deviation of blank measurements</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_measurements: All blank measurement values</li> </ul> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lob(self, n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Blank (LoB).\n\n    LoB represents the highest measurement result that is likely to be observed \n    for a blank specimen. Calculated as the 95th percentile of blank measurements.\n\n    Args:\n        n_blank_runs: Number of blank (AF=0) simulations to run\n\n    Returns:\n        Dictionary with LoB results including:\n        - lob_value: 95th percentile of blank test statistics\n        - blank_mean: Mean of blank measurements\n        - blank_std: Standard deviation of blank measurements\n        - blank_measurements: All blank measurement values\n    \"\"\"\n    print(f\"Estimating LoB with {n_blank_runs} blank runs...\")\n\n    # Create blank configuration (AF = 0)\n    blank_config = self._create_blank_config()\n    blank_measurements = []\n\n    for i in range(n_blank_runs):\n        # Use different random seed for each run\n        run_rng = np.random.default_rng(self.config.seed + i)\n\n        # Simulate blank reads (no true variants)\n        reads_df = simulate_reads(blank_config, run_rng)\n\n        # Process through pipeline to get test statistic\n        collapsed_df = collapse_umis(reads_df, blank_config, run_rng)\n        error_model = fit_error_model(collapsed_df, blank_config, run_rng)\n        calls_df = call_mrd(collapsed_df, error_model, blank_config, run_rng)\n\n        # Extract test statistic (number of variant calls in blank)\n        n_variant_calls = len(calls_df[calls_df['variant_call'] == True])\n        blank_measurements.append(n_variant_calls)\n\n    blank_measurements = np.array(blank_measurements)\n\n    # Compute LoB as 95th percentile\n    lob_value = np.percentile(blank_measurements, 95)\n    blank_mean = np.mean(blank_measurements)\n    blank_std = np.std(blank_measurements)\n\n    self.lob_results = {\n        'lob_value': float(lob_value),\n        'blank_mean': float(blank_mean),\n        'blank_std': float(blank_std),\n        'blank_measurements': blank_measurements.tolist(),\n        'n_blank_runs': n_blank_runs,\n        'percentile': 95,\n        'config_hash': self.config.config_hash()\n    }\n\n    print(f\"LoB estimated: {lob_value:.3f} (mean: {blank_mean:.3f}, std: {blank_std:.3f})\")\n    return self.lob_results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.LODAnalyzer.estimate_lod","title":"<code>estimate_lod(af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, target_detection_rate=0.95, alpha=0.05, beta=0.05)</code>","text":"<p>Estimate Limit of Detection (LoD).</p> <p>LoD is the lowest analyte concentration likely to be reliably detected. Calculated as the concentration yielding 95% detection probability.</p> <p>Parameters:</p> Name Type Description Default <code>af_range</code> <code>Tuple[float, float]</code> <p>Range of allele fractions to test (min, max)</p> <code>(0.0001, 0.01)</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per AF/depth combination</p> <code>50</code> <code>target_detection_rate</code> <code>float</code> <p>Target detection rate (e.g., 0.95 for 95%)</p> <code>0.95</code> <code>alpha</code> <code>float</code> <p>Type I error rate</p> <code>0.05</code> <code>beta</code> <code>float</code> <p>Type II error rate</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoD results for each depth</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lod(self, \n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 target_detection_rate: float = 0.95,\n                 alpha: float = 0.05,\n                 beta: float = 0.05) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Detection (LoD).\n\n    LoD is the lowest analyte concentration likely to be reliably detected.\n    Calculated as the concentration yielding 95% detection probability.\n\n    Args:\n        af_range: Range of allele fractions to test (min, max)\n        depth_values: UMI depths to test\n        n_replicates: Number of replicates per AF/depth combination\n        target_detection_rate: Target detection rate (e.g., 0.95 for 95%)\n        alpha: Type I error rate\n        beta: Type II error rate\n\n    Returns:\n        Dictionary with LoD results for each depth\n    \"\"\"\n    print(f\"Estimating LoD across AF range {af_range} at depths {depth_values}...\")\n\n    # Generate AF grid (log-spaced)\n    af_min, af_max = af_range\n    af_values = np.logspace(np.log10(af_min), np.log10(af_max), 15)\n\n    lod_results = {}\n\n    for depth in depth_values:\n        print(f\"  Processing depth {depth}...\")\n\n        hit_rates = []\n        af_tested = []\n\n        for af in af_values:\n            # Run detection experiments at this AF/depth\n            detection_count = 0\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(self.config.seed + rep * 1000 + int(af * 1e6))\n\n                # Create config for this AF/depth\n                test_config = self._create_lod_config(af, depth)\n\n                # Run pipeline\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Check if variant was detected\n                if len(calls_df[calls_df['variant_call'] == True]) &gt; 0:\n                    detection_count += 1\n\n            hit_rate = detection_count / n_replicates\n            hit_rates.append(hit_rate)\n            af_tested.append(af)\n\n        # Fit logistic curve to hit rate vs AF\n        lod_af = self._fit_detection_curve(af_tested, hit_rates, target_detection_rate)\n\n        # Compute confidence intervals using bootstrap\n        lod_ci = self._bootstrap_lod_ci(af_tested, hit_rates, target_detection_rate, \n                                     n_bootstrap=200)\n\n        lod_results[depth] = {\n            'lod_af': float(lod_af),\n            'lod_ci_lower': float(lod_ci[0]),\n            'lod_ci_upper': float(lod_ci[1]),\n            'af_values': [float(x) for x in af_tested],\n            'hit_rates': hit_rates,\n            'target_detection_rate': target_detection_rate,\n            'n_replicates': n_replicates\n        }\n\n        print(f\"    LoD at depth {depth}: {lod_af:.2e} AF [{lod_ci[0]:.2e}, {lod_ci[1]:.2e}]\")\n\n    self.lod_results = {\n        'depth_results': lod_results,\n        'af_range': af_range,\n        'target_detection_rate': target_detection_rate,\n        'alpha': alpha,\n        'beta': beta,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.lod_results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.LODAnalyzer.estimate_loq","title":"<code>estimate_loq(af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, cv_threshold=0.2, abs_error_threshold=None)</code>","text":"<p>Estimate Limit of Quantification (LoQ).</p> <p>LoQ is the lowest concentration at which quantitative measurements can be made with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).</p> <p>Parameters:</p> Name Type Description Default <code>af_range</code> <code>Tuple[float, float]</code> <p>Range of allele fractions to test</p> <code>(0.0001, 0.01)</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test  </p> <code>[1000, 5000, 10000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per AF/depth combination</p> <code>50</code> <code>cv_threshold</code> <code>float</code> <p>Maximum acceptable coefficient of variation</p> <code>0.2</code> <code>abs_error_threshold</code> <code>Optional[float]</code> <p>Maximum acceptable absolute error (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoQ results for each depth</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_loq(self,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2), \n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 cv_threshold: float = 0.20,\n                 abs_error_threshold: Optional[float] = None) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Quantification (LoQ).\n\n    LoQ is the lowest concentration at which quantitative measurements can be made\n    with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).\n\n    Args:\n        af_range: Range of allele fractions to test\n        depth_values: UMI depths to test  \n        n_replicates: Number of replicates per AF/depth combination\n        cv_threshold: Maximum acceptable coefficient of variation\n        abs_error_threshold: Maximum acceptable absolute error (optional)\n\n    Returns:\n        Dictionary with LoQ results for each depth\n    \"\"\"\n    print(f\"Estimating LoQ with CV threshold {cv_threshold:.1%}...\")\n\n    af_min, af_max = af_range\n    af_values = np.logspace(np.log10(af_min), np.log10(af_max), 12)\n\n    loq_results = {}\n\n    for depth in depth_values:\n        print(f\"  Processing depth {depth}...\")\n\n        cv_values = []\n        abs_errors = []\n        af_tested = []\n\n        for af in af_values:\n            estimated_afs = []\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(self.config.seed + rep * 2000 + int(af * 1e6))\n\n                # Create config for this AF/depth\n                test_config = self._create_lod_config(af, depth)\n\n                # Run pipeline and estimate AF\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Estimate AF from variant calls\n                if len(calls_df) &gt; 0:\n                    # Simple AF estimation: variants detected / total UMIs\n                    estimated_af = len(calls_df[calls_df['variant_call'] == True]) / len(calls_df)\n                    estimated_afs.append(estimated_af)\n                else:\n                    estimated_afs.append(0.0)\n\n            if len(estimated_afs) &gt; 1:\n                mean_af = np.mean(estimated_afs)\n                std_af = np.std(estimated_afs)\n                cv = std_af / mean_af if mean_af &gt; 0 else np.inf\n                abs_error = abs(mean_af - af)\n\n                cv_values.append(cv)\n                abs_errors.append(abs_error)\n                af_tested.append(af)\n\n        # Find LoQ as lowest AF meeting precision criteria\n        loq_af_cv = None\n        loq_af_abs = None\n\n        # CV-based LoQ\n        for i, cv in enumerate(cv_values):\n            if cv &lt;= cv_threshold:\n                loq_af_cv = af_tested[i]\n                break\n\n        # Absolute error-based LoQ (if threshold provided)\n        if abs_error_threshold is not None:\n            for i, abs_err in enumerate(abs_errors):\n                if abs_err &lt;= abs_error_threshold:\n                    loq_af_abs = af_tested[i]\n                    break\n\n        loq_results[depth] = {\n            'loq_af_cv': float(loq_af_cv) if loq_af_cv is not None else None,\n            'loq_af_abs_error': float(loq_af_abs) if loq_af_abs is not None else None,\n            'af_values': [float(x) for x in af_tested],\n            'cv_values': cv_values,\n            'abs_errors': abs_errors,\n            'cv_threshold': cv_threshold,\n            'abs_error_threshold': abs_error_threshold,\n            'n_replicates': n_replicates\n        }\n\n        cv_str = f\"{loq_af_cv:.2e}\" if loq_af_cv else \"Not found\"\n        print(f\"    LoQ (CV) at depth {depth}: {cv_str} AF\")\n\n    self.loq_results = {\n        'depth_results': loq_results,\n        'af_range': af_range,\n        'cv_threshold': cv_threshold,\n        'abs_error_threshold': abs_error_threshold,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.loq_results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.LODAnalyzer.generate_reports","title":"<code>generate_reports(output_dir='reports')</code>","text":"<p>Generate LoB/LoD/LoQ reports and visualizations.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def generate_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate LoB/LoD/LoQ reports and visualizations.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save LoB results\n    if self.lob_results:\n        lob_payload = dict(self.lob_results)\n        lob_payload[\"schema_version\"] = \"1.0.0\"\n        lob_path = output_path / \"lob.json\"\n        with open(lob_path, 'w') as f:\n            json.dump(lob_payload, f, indent=2)\n        print(f\"LoB results saved to {lob_path}\")\n\n    # Save LoD results\n    if self.lod_results:\n        self._save_lod_table(output_path)\n        self._plot_lod_curves(output_path)\n\n    # Save LoQ results  \n    if self.loq_results:\n        self._save_loq_table(output_path)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.estimate_lob","title":"<code>precise_mrd.eval.lod.estimate_lob(config, rng, n_blank_runs=100)</code>","text":"<p>Convenience function to estimate LoB.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lob(config: PipelineConfig, rng: np.random.Generator, \n                 n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoB.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_lob(n_blank_runs)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.estimate_lod","title":"<code>precise_mrd.eval.lod.estimate_lod(config, rng, af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50)</code>","text":"<p>Convenience function to estimate LoD.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lod(config: PipelineConfig, rng: np.random.Generator,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoD.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_lod(af_range, depth_values, n_replicates)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.lod.estimate_loq","title":"<code>precise_mrd.eval.lod.estimate_loq(config, rng, af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, cv_threshold=0.2)</code>","text":"<p>Convenience function to estimate LoQ.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_loq(config: PipelineConfig, rng: np.random.Generator,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 cv_threshold: float = 0.20) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoQ.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_loq(af_range, depth_values, n_replicates, cv_threshold)\n</code></pre>"},{"location":"api-reference%202/#stratified-analysis","title":"Stratified Analysis","text":""},{"location":"api-reference%202/#precise_mrd.eval.stratified.StratifiedAnalyzer","title":"<code>precise_mrd.eval.stratified.StratifiedAnalyzer</code>","text":"<p>Analyzer for stratified power and calibration analysis.</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>class StratifiedAnalyzer:\n    \"\"\"Analyzer for stratified power and calibration analysis.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.power_results: Optional[Dict[str, Any]] = None\n        self.calibration_results: Optional[Dict[str, Any]] = None\n\n    def analyze_stratified_power(self,\n                               af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                               depth_values: List[int] = [1000, 5000, 10000],\n                               contexts: List[str] = ['CpG', 'CHG', 'CHH', 'NpN'],\n                               n_replicates: int = 50) -&gt; Dict[str, Any]:\n        \"\"\"Analyze detection power stratified by trinucleotide context and depth.\n\n        Args:\n            af_values: Allele fractions to test\n            depth_values: UMI depths to test\n            contexts: Trinucleotide contexts to stratify by\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with stratified power analysis results\n        \"\"\"\n        print(\"Running stratified power analysis...\")\n\n        power_results = {}\n\n        for context in contexts:\n            print(f\"  Analyzing context: {context}\")\n            power_results[context] = {}\n\n            for depth in depth_values:\n                power_results[context][depth] = {}\n\n                for af in af_values:\n                    detection_rates = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 1000 + hash(context) % 1000\n                        )\n\n                        # Create context-specific config\n                        context_config = self._create_context_config(af, depth, context)\n\n                        # Run pipeline with context tagging\n                        reads_df = self._simulate_with_context(context_config, run_rng, context)\n                        collapsed_df = collapse_umis(reads_df, context_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, context_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, context_config, run_rng)\n\n                        # Calculate detection rate for this context\n                        if len(calls_df) &gt; 0:\n                            context_calls = calls_df[calls_df.get('context', 'NpN') == context]\n                            detection_rate = len(context_calls[context_calls['variant_call'] == True]) / max(1, len(context_calls))\n                        else:\n                            detection_rate = 0.0\n\n                        detection_rates.append(detection_rate)\n\n                    power_results[context][depth][af] = {\n                        'mean_detection_rate': float(np.mean(detection_rates)),\n                        'std_detection_rate': float(np.std(detection_rates)),\n                        'detection_rates': detection_rates,\n                        'n_replicates': n_replicates\n                    }\n\n                    print(f\"    {context} @ depth={depth}, AF={af:.0e}: {np.mean(detection_rates):.3f} \u00b1 {np.std(detection_rates):.3f}\")\n\n        self.power_results = {\n            'stratified_results': power_results,\n            'af_values': af_values,\n            'depth_values': depth_values,\n            'contexts': contexts,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.power_results\n\n    def analyze_calibration_by_bins(self,\n                                  af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                                  depth_values: List[int] = [1000, 5000, 10000],\n                                  n_bins: int = 10,\n                                  n_replicates: int = 100) -&gt; Dict[str, Any]:\n        \"\"\"Analyze calibration stratified by AF and depth bins.\n\n        Args:\n            af_values: Allele fractions to test\n            depth_values: UMI depths to test\n            n_bins: Number of calibration bins\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with binned calibration results\n        \"\"\"\n        print(f\"Running calibration analysis with {n_bins} bins...\")\n\n        calibration_data = []\n\n        for depth in depth_values:\n            print(f\"  Processing depth: {depth}\")\n\n            for af in af_values:\n                predicted_probs = []\n                true_labels = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(\n                        self.config.seed + rep * 2000 + int(af * 1e6) + depth\n                    )\n\n                    # Create test config\n                    test_config = self._create_calibration_config(af, depth)\n\n                    # Run pipeline\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Extract predicted probabilities and true labels\n                    if len(calls_df) &gt; 0:\n                        # Use p-value as proxy for predicted probability (inverted)\n                        prob_scores = 1 - calls_df.get('p_value', self.rng.uniform(0, 1, len(calls_df)))\n                        true_positives = calls_df.get('variant_call', False)\n\n                        predicted_probs.extend(prob_scores.tolist())\n                        true_labels.extend(true_positives.tolist())\n\n                if len(predicted_probs) &gt; 0:\n                    # Compute calibration metrics\n                    calibration_metrics = self._compute_calibration_metrics(\n                        np.array(predicted_probs), np.array(true_labels), n_bins\n                    )\n\n                    calibration_data.append({\n                        'depth': depth,\n                        'af': af,\n                        'ece': calibration_metrics['ece'],\n                        'max_ce': calibration_metrics['max_ce'],\n                        'bin_accuracies': calibration_metrics['bin_accuracies'],\n                        'bin_confidences': calibration_metrics['bin_confidences'],\n                        'bin_counts': calibration_metrics['bin_counts'],\n                        'n_samples': len(predicted_probs)\n                    })\n\n        self.calibration_results = {\n            'calibration_data': calibration_data,\n            'af_values': af_values,\n            'depth_values': depth_values,\n            'n_bins': n_bins,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.calibration_results\n\n    def _simulate_with_context(self, config: PipelineConfig, \n                              rng: np.random.Generator,\n                              context: str) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with trinucleotide context tagging.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        # Add context-specific error modeling\n        context_error_rates = {\n            'CpG': 1.5,    # Higher error in CpG contexts\n            'CHG': 1.2,    # Moderate error in CHG\n            'CHH': 1.0,    # Baseline error in CHH\n            'NpN': 0.8     # Lower error in other contexts\n        }\n\n        error_multiplier = context_error_rates.get(context, 1.0)\n        reads_df['background_rate'] *= error_multiplier\n        reads_df['context'] = context\n\n        # Adjust false positive counts based on context\n        reads_df['n_false_positives'] = rng.binomial(\n            reads_df['n_families'] - reads_df['n_true_variants'],\n            reads_df['background_rate'] * error_multiplier\n        )\n\n        return reads_df\n\n    def _create_context_config(self, af: float, depth: int, context: str) -&gt; PipelineConfig:\n        \"\"\"Create configuration for context-specific analysis.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_context_{context}\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _create_calibration_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for calibration analysis.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_calibration\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _compute_calibration_metrics(self, predicted_probs: np.ndarray, \n                                   true_labels: np.ndarray, \n                                   n_bins: int) -&gt; Dict[str, Any]:\n        \"\"\"Compute calibration metrics including ECE and binned accuracy.\"\"\"\n        # Create bins\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        bin_lowers = bin_boundaries[:-1]\n        bin_uppers = bin_boundaries[1:]\n\n        bin_accuracies = []\n        bin_confidences = []\n        bin_counts = []\n\n        ece = 0.0\n        max_ce = 0.0\n\n        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n            # Find predictions in this bin\n            in_bin = (predicted_probs &gt; bin_lower) &amp; (predicted_probs &lt;= bin_upper)\n            prop_in_bin = in_bin.mean()\n\n            if prop_in_bin &gt; 0:\n                # Accuracy in this bin\n                accuracy_in_bin = true_labels[in_bin].mean()\n                avg_confidence_in_bin = predicted_probs[in_bin].mean()\n\n                # Calibration error in this bin\n                bin_ce = abs(avg_confidence_in_bin - accuracy_in_bin)\n                ece += bin_ce * prop_in_bin\n                max_ce = max(max_ce, bin_ce)\n\n                bin_accuracies.append(accuracy_in_bin)\n                bin_confidences.append(avg_confidence_in_bin)\n                bin_counts.append(in_bin.sum())\n            else:\n                bin_accuracies.append(0.0)\n                bin_confidences.append(0.0)\n                bin_counts.append(0)\n\n        return {\n            'ece': float(ece),\n            'max_ce': float(max_ce),\n            'bin_accuracies': bin_accuracies,\n            'bin_confidences': bin_confidences,\n            'bin_counts': bin_counts\n        }\n\n    def generate_stratified_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate stratified analysis reports.\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save power analysis results\n        if self.power_results:\n            power_payload = dict(self.power_results)\n            power_payload[\"schema_version\"] = \"1.0.0\"\n            power_path = output_path / \"power_by_stratum.json\"\n            with open(power_path, 'w') as f:\n                json.dump(power_payload, f, indent=2)\n            print(f\"Stratified power results saved to {power_path}\")\n\n        # Save calibration results\n        if self.calibration_results:\n            # Save as CSV for easier analysis\n            calib_df = pd.DataFrame(self.calibration_results['calibration_data'])\n            calib_path = output_path / \"calibration_by_bin.csv\"\n            calib_df.to_csv(calib_path, index=False)\n            print(f\"Calibration by bin results saved to {calib_path}\")\n\n            # Also save as JSON\n            calibration_payload = dict(self.calibration_results)\n            calibration_payload[\"schema_version\"] = \"1.0.0\"\n            calib_json_path = output_path / \"calibration_by_bin.json\"\n            with open(calib_json_path, 'w') as f:\n                json.dump(calibration_payload, f, indent=2)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.stratified.StratifiedAnalyzer.analyze_stratified_power","title":"<code>analyze_stratified_power(af_values=[0.001, 0.005, 0.01, 0.05], depth_values=[1000, 5000, 10000], contexts=['CpG', 'CHG', 'CHH', 'NpN'], n_replicates=50)</code>","text":"<p>Analyze detection power stratified by trinucleotide context and depth.</p> <p>Parameters:</p> Name Type Description Default <code>af_values</code> <code>List[float]</code> <p>Allele fractions to test</p> <code>[0.001, 0.005, 0.01, 0.05]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>contexts</code> <code>List[str]</code> <p>Trinucleotide contexts to stratify by</p> <code>['CpG', 'CHG', 'CHH', 'NpN']</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>50</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with stratified power analysis results</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def analyze_stratified_power(self,\n                           af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                           depth_values: List[int] = [1000, 5000, 10000],\n                           contexts: List[str] = ['CpG', 'CHG', 'CHH', 'NpN'],\n                           n_replicates: int = 50) -&gt; Dict[str, Any]:\n    \"\"\"Analyze detection power stratified by trinucleotide context and depth.\n\n    Args:\n        af_values: Allele fractions to test\n        depth_values: UMI depths to test\n        contexts: Trinucleotide contexts to stratify by\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with stratified power analysis results\n    \"\"\"\n    print(\"Running stratified power analysis...\")\n\n    power_results = {}\n\n    for context in contexts:\n        print(f\"  Analyzing context: {context}\")\n        power_results[context] = {}\n\n        for depth in depth_values:\n            power_results[context][depth] = {}\n\n            for af in af_values:\n                detection_rates = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(\n                        self.config.seed + rep * 1000 + hash(context) % 1000\n                    )\n\n                    # Create context-specific config\n                    context_config = self._create_context_config(af, depth, context)\n\n                    # Run pipeline with context tagging\n                    reads_df = self._simulate_with_context(context_config, run_rng, context)\n                    collapsed_df = collapse_umis(reads_df, context_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, context_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, context_config, run_rng)\n\n                    # Calculate detection rate for this context\n                    if len(calls_df) &gt; 0:\n                        context_calls = calls_df[calls_df.get('context', 'NpN') == context]\n                        detection_rate = len(context_calls[context_calls['variant_call'] == True]) / max(1, len(context_calls))\n                    else:\n                        detection_rate = 0.0\n\n                    detection_rates.append(detection_rate)\n\n                power_results[context][depth][af] = {\n                    'mean_detection_rate': float(np.mean(detection_rates)),\n                    'std_detection_rate': float(np.std(detection_rates)),\n                    'detection_rates': detection_rates,\n                    'n_replicates': n_replicates\n                }\n\n                print(f\"    {context} @ depth={depth}, AF={af:.0e}: {np.mean(detection_rates):.3f} \u00b1 {np.std(detection_rates):.3f}\")\n\n    self.power_results = {\n        'stratified_results': power_results,\n        'af_values': af_values,\n        'depth_values': depth_values,\n        'contexts': contexts,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.power_results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.stratified.StratifiedAnalyzer.analyze_calibration_by_bins","title":"<code>analyze_calibration_by_bins(af_values=[0.001, 0.005, 0.01, 0.05], depth_values=[1000, 5000, 10000], n_bins=10, n_replicates=100)</code>","text":"<p>Analyze calibration stratified by AF and depth bins.</p> <p>Parameters:</p> Name Type Description Default <code>af_values</code> <code>List[float]</code> <p>Allele fractions to test</p> <code>[0.001, 0.005, 0.01, 0.05]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>n_bins</code> <code>int</code> <p>Number of calibration bins</p> <code>10</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>100</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with binned calibration results</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def analyze_calibration_by_bins(self,\n                              af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                              depth_values: List[int] = [1000, 5000, 10000],\n                              n_bins: int = 10,\n                              n_replicates: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Analyze calibration stratified by AF and depth bins.\n\n    Args:\n        af_values: Allele fractions to test\n        depth_values: UMI depths to test\n        n_bins: Number of calibration bins\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with binned calibration results\n    \"\"\"\n    print(f\"Running calibration analysis with {n_bins} bins...\")\n\n    calibration_data = []\n\n    for depth in depth_values:\n        print(f\"  Processing depth: {depth}\")\n\n        for af in af_values:\n            predicted_probs = []\n            true_labels = []\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(\n                    self.config.seed + rep * 2000 + int(af * 1e6) + depth\n                )\n\n                # Create test config\n                test_config = self._create_calibration_config(af, depth)\n\n                # Run pipeline\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Extract predicted probabilities and true labels\n                if len(calls_df) &gt; 0:\n                    # Use p-value as proxy for predicted probability (inverted)\n                    prob_scores = 1 - calls_df.get('p_value', self.rng.uniform(0, 1, len(calls_df)))\n                    true_positives = calls_df.get('variant_call', False)\n\n                    predicted_probs.extend(prob_scores.tolist())\n                    true_labels.extend(true_positives.tolist())\n\n            if len(predicted_probs) &gt; 0:\n                # Compute calibration metrics\n                calibration_metrics = self._compute_calibration_metrics(\n                    np.array(predicted_probs), np.array(true_labels), n_bins\n                )\n\n                calibration_data.append({\n                    'depth': depth,\n                    'af': af,\n                    'ece': calibration_metrics['ece'],\n                    'max_ce': calibration_metrics['max_ce'],\n                    'bin_accuracies': calibration_metrics['bin_accuracies'],\n                    'bin_confidences': calibration_metrics['bin_confidences'],\n                    'bin_counts': calibration_metrics['bin_counts'],\n                    'n_samples': len(predicted_probs)\n                })\n\n    self.calibration_results = {\n        'calibration_data': calibration_data,\n        'af_values': af_values,\n        'depth_values': depth_values,\n        'n_bins': n_bins,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.calibration_results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.stratified.StratifiedAnalyzer.generate_stratified_reports","title":"<code>generate_stratified_reports(output_dir='reports')</code>","text":"<p>Generate stratified analysis reports.</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def generate_stratified_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate stratified analysis reports.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save power analysis results\n    if self.power_results:\n        power_payload = dict(self.power_results)\n        power_payload[\"schema_version\"] = \"1.0.0\"\n        power_path = output_path / \"power_by_stratum.json\"\n        with open(power_path, 'w') as f:\n            json.dump(power_payload, f, indent=2)\n        print(f\"Stratified power results saved to {power_path}\")\n\n    # Save calibration results\n    if self.calibration_results:\n        # Save as CSV for easier analysis\n        calib_df = pd.DataFrame(self.calibration_results['calibration_data'])\n        calib_path = output_path / \"calibration_by_bin.csv\"\n        calib_df.to_csv(calib_path, index=False)\n        print(f\"Calibration by bin results saved to {calib_path}\")\n\n        # Also save as JSON\n        calibration_payload = dict(self.calibration_results)\n        calibration_payload[\"schema_version\"] = \"1.0.0\"\n        calib_json_path = output_path / \"calibration_by_bin.json\"\n        with open(calib_json_path, 'w') as f:\n            json.dump(calibration_payload, f, indent=2)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.eval.stratified.run_stratified_analysis","title":"<code>precise_mrd.eval.stratified.run_stratified_analysis(config, rng, output_dir='reports')</code>","text":"<p>Run complete stratified power and calibration analysis.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Random number generator</p> required <code>output_dir</code> <code>str</code> <p>Output directory for reports</p> <code>'reports'</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>Tuple of (power_results, calibration_results)</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def run_stratified_analysis(config: PipelineConfig,\n                          rng: np.random.Generator,\n                          output_dir: str = \"reports\") -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"Run complete stratified power and calibration analysis.\n\n    Args:\n        config: Pipeline configuration\n        rng: Random number generator\n        output_dir: Output directory for reports\n\n    Returns:\n        Tuple of (power_results, calibration_results)\n    \"\"\"\n    analyzer = StratifiedAnalyzer(config, rng)\n\n    # Run stratified power analysis\n    power_results = analyzer.analyze_stratified_power()\n\n    # Run calibration analysis\n    calibration_results = analyzer.analyze_calibration_by_bins()\n\n    # Generate reports\n    analyzer.generate_stratified_reports(output_dir)\n\n    return power_results, calibration_results\n</code></pre>"},{"location":"api-reference%202/#contamination-testing","title":"Contamination Testing","text":""},{"location":"api-reference%202/#precise_mrd.sim.contamination.ContaminationSimulator","title":"<code>precise_mrd.sim.contamination.ContaminationSimulator</code>","text":"<p>Simulator for contamination effects in ctDNA/UMI sequencing.</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>class ContaminationSimulator:\n    \"\"\"Simulator for contamination effects in ctDNA/UMI sequencing.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.contamination_results: Optional[Dict[str, Any]] = None\n\n    def simulate_contamination_effects(self,\n                                     hop_rates: List[float] = [0.0, 0.001, 0.002, 0.005, 0.01],\n                                     barcode_collision_rates: List[float] = [0.0, 0.0001, 0.0005, 0.001],\n                                     cross_sample_proportions: List[float] = [0.0, 0.01, 0.05, 0.1],\n                                     af_test_values: List[float] = [0.001, 0.005, 0.01],\n                                     depth_values: List[int] = [1000, 5000],\n                                     n_replicates: int = 20) -&gt; Dict[str, Any]:\n        \"\"\"Simulate contamination effects on variant detection.\n\n        Args:\n            hop_rates: Index-hopping rates to test\n            barcode_collision_rates: Barcode collision rates to test\n            cross_sample_proportions: Cross-sample contamination proportions\n            af_test_values: Allele fractions to test contamination effects on\n            depth_values: UMI depths to test\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with contamination simulation results\n        \"\"\"\n        print(\"Running contamination stress testing...\")\n\n        results = {}\n        sensitivity_matrix = []\n\n        # Test index-hopping effects\n        print(\"  Testing index-hopping effects...\")\n        hop_results = self._test_index_hopping(hop_rates, af_test_values, \n                                              depth_values, n_replicates)\n        results['index_hopping'] = hop_results\n\n        # Test barcode collision effects\n        print(\"  Testing barcode collision effects...\")\n        collision_results = self._test_barcode_collisions(barcode_collision_rates,\n                                                         af_test_values, depth_values, \n                                                         n_replicates)\n        results['barcode_collisions'] = collision_results\n\n        # Test cross-sample contamination\n        print(\"  Testing cross-sample contamination...\")\n        cross_contam_results = self._test_cross_sample_contamination(\n            cross_sample_proportions, af_test_values, depth_values, n_replicates)\n        results['cross_sample_contamination'] = cross_contam_results\n\n        # Create sensitivity matrix for heatmap\n        sensitivity_matrix = self._create_sensitivity_matrix(results, af_test_values, \n                                                           depth_values)\n        results['sensitivity_matrix'] = sensitivity_matrix\n\n        self.contamination_results = results\n        return results\n\n    def _test_index_hopping(self, hop_rates: List[float], af_values: List[float],\n                           depth_values: List[int], n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test index-hopping contamination effects.\"\"\"\n        hop_results = {}\n\n        for hop_rate in hop_rates:\n            hop_results[hop_rate] = {}\n\n            for af in af_values:\n                hop_results[hop_rate][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n\n                    for rep in range(n_replicates):\n                        # Create contaminated simulation\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 1000 + int(hop_rate * 1e6)\n                        )\n\n                        # Simulate with index hopping\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_index_hopping(\n                            contam_config, run_rng, hop_rate\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate sensitivity (detection rate)\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        # Approximate expected detections (crude estimate)\n                        expected_detections = max(1, int(af * depth * 0.8))  # 80% pipeline efficiency\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n                        sensitivity_scores.append(sensitivity)\n\n                    hop_results[hop_rate][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'sensitivity_scores': sensitivity_scores,\n                        'n_replicates': n_replicates\n                    }\n\n        return hop_results\n\n    def _test_barcode_collisions(self, collision_rates: List[float], af_values: List[float],\n                                depth_values: List[int], n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test barcode collision contamination effects.\"\"\"\n        collision_results = {}\n\n        for collision_rate in collision_rates:\n            collision_results[collision_rate] = {}\n\n            for af in af_values:\n                collision_results[collision_rate][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n                    false_positive_rates = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 2000 + int(collision_rate * 1e6)\n                        )\n\n                        # Simulate with barcode collisions\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_barcode_collisions(\n                            contam_config, run_rng, collision_rate\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate metrics\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        expected_detections = max(1, int(af * depth * 0.8))\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n\n                        # Estimate false positive rate (excess detections)\n                        excess_detections = max(0, n_detected - expected_detections)\n                        fp_rate = excess_detections / depth if depth &gt; 0 else 0.0\n\n                        sensitivity_scores.append(sensitivity)\n                        false_positive_rates.append(fp_rate)\n\n                    collision_results[collision_rate][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'mean_fp_rate': float(np.mean(false_positive_rates)),\n                        'std_fp_rate': float(np.std(false_positive_rates)),\n                        'n_replicates': n_replicates\n                    }\n\n        return collision_results\n\n    def _test_cross_sample_contamination(self, contamination_proportions: List[float],\n                                       af_values: List[float], depth_values: List[int],\n                                       n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test cross-sample contamination effects.\"\"\"\n        cross_results = {}\n\n        for contam_prop in contamination_proportions:\n            cross_results[contam_prop] = {}\n\n            for af in af_values:\n                cross_results[contam_prop][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 3000 + int(contam_prop * 1e6)\n                        )\n\n                        # Simulate with cross-sample contamination\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_cross_contamination(\n                            contam_config, run_rng, contam_prop\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate sensitivity\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        expected_detections = max(1, int(af * depth * 0.8))\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n                        sensitivity_scores.append(sensitivity)\n\n                    cross_results[contam_prop][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'n_replicates': n_replicates\n                    }\n\n        return cross_results\n\n    def _simulate_with_index_hopping(self, config: PipelineConfig, \n                                   rng: np.random.Generator, \n                                   hop_rate: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with index-hopping contamination.\"\"\"\n        # Start with normal simulation\n        reads_df = simulate_reads(config, rng)\n\n        if hop_rate &gt; 0:\n            # Add hopped reads (contamination from other samples)\n            n_reads = len(reads_df)\n            n_hopped = rng.binomial(n_reads, hop_rate)\n\n            if n_hopped &gt; 0:\n                # Create contaminating reads (different background pattern)\n                contam_reads = reads_df.sample(n=n_hopped, random_state=rng).copy()\n\n                # Modify contaminating reads (different barcode context)\n                contam_reads['background_rate'] *= 2.0  # Higher error rate\n                contam_reads['n_false_positives'] *= 1.5  # More artifacts\n                contam_reads['sample_id'] = 'hopped_' + contam_reads['sample_id'].astype(str)\n\n                # Combine with original reads\n                reads_df = pd.concat([reads_df, contam_reads], ignore_index=True)\n\n        return reads_df\n\n    def _simulate_with_barcode_collisions(self, config: PipelineConfig,\n                                        rng: np.random.Generator,\n                                        collision_rate: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with barcode collision artifacts.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        if collision_rate &gt; 0:\n            # Simulate UMI collisions leading to false consensus\n            n_families = len(reads_df)\n            n_collisions = rng.binomial(n_families, collision_rate)\n\n            if n_collisions &gt; 0:\n                # Select families for collision\n                collision_indices = rng.choice(n_families, size=n_collisions, replace=False)\n\n                # Increase false positive rate for collided families\n                reads_df.loc[collision_indices, 'n_false_positives'] *= 2.0\n                reads_df.loc[collision_indices, 'background_rate'] *= 1.5\n\n                # Reduce consensus quality\n                reads_df.loc[collision_indices, 'mean_quality'] *= 0.8\n\n        return reads_df\n\n    def _simulate_with_cross_contamination(self, config: PipelineConfig,\n                                         rng: np.random.Generator,\n                                         contam_proportion: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with cross-sample contamination.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        if contam_proportion &gt; 0:\n            # Add contaminating sample with different AF\n            n_reads = len(reads_df)\n            n_contam = int(n_reads * contam_proportion)\n\n            if n_contam &gt; 0:\n                # Create contaminating sample (higher AF)\n                contam_af = min(0.1, reads_df['allele_fraction'].iloc[0] * 10)  # 10x higher AF\n\n                contam_config = self._create_contamination_config(\n                    contam_af, reads_df['target_depth'].iloc[0]\n                )\n                contam_reads = simulate_reads(contam_config, rng)\n\n                # Take subset for contamination\n                if len(contam_reads) &gt; 0:\n                    contam_subset = contam_reads.sample(\n                        n=min(n_contam, len(contam_reads)), \n                        random_state=rng\n                    ).copy()\n                    contam_subset['sample_id'] = 'contam_' + contam_subset['sample_id'].astype(str)\n\n                    # Mix with original sample\n                    reads_df = pd.concat([reads_df, contam_subset], ignore_index=True)\n\n        return reads_df\n\n    def _create_contamination_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for contamination testing.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_contam\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _create_sensitivity_matrix(self, results: Dict[str, Any], \n                                 af_values: List[float], \n                                 depth_values: List[int]) -&gt; Dict[str, Any]:\n        \"\"\"Create sensitivity matrix for heatmap visualization.\"\"\"\n        # Extract sensitivity data for different contamination types\n        matrix_data = {}\n\n        # Index hopping sensitivity matrix\n        if 'index_hopping' in results:\n            hop_data = results['index_hopping']\n            hop_rates = list(hop_data.keys())\n\n            sensitivity_matrix = np.zeros((len(hop_rates), len(af_values) * len(depth_values)))\n\n            for i, hop_rate in enumerate(hop_rates):\n                col_idx = 0\n                for af in af_values:\n                    for depth in depth_values:\n                        if af in hop_data[hop_rate] and depth in hop_data[hop_rate][af]:\n                            sensitivity = hop_data[hop_rate][af][depth]['mean_sensitivity']\n                            sensitivity_matrix[i, col_idx] = sensitivity\n                        col_idx += 1\n\n            # Create labels\n            condition_labels = [f\"AF={af:.0e}, D={depth}\" for af in af_values for depth in depth_values]\n\n            matrix_data['index_hopping'] = {\n                'matrix': sensitivity_matrix.tolist(),\n                'hop_rates': hop_rates,\n                'condition_labels': condition_labels,\n                'af_values': af_values,\n                'depth_values': depth_values\n            }\n\n        return matrix_data\n\n    def generate_contamination_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate contamination analysis reports.\"\"\"\n        if not self.contamination_results:\n            print(\"No contamination results to report\")\n            return\n\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save contamination sensitivity results\n        contam_payload = dict(self.contamination_results)\n        contam_payload[\"schema_version\"] = \"1.0.0\"\n        contam_path = output_path / \"contam_sensitivity.json\"\n        with open(contam_path, 'w') as f:\n            json.dump(contam_payload, f, indent=2)\n        print(f\"Contamination sensitivity results saved to {contam_path}\")\n\n        # Generate contamination heatmap\n        self._plot_contamination_heatmap(output_path)\n\n    def _plot_contamination_heatmap(self, output_path: Path) -&gt; None:\n        \"\"\"Generate contamination sensitivity heatmap.\"\"\"\n        if 'sensitivity_matrix' not in self.contamination_results:\n            return\n\n        matrix_data = self.contamination_results['sensitivity_matrix']\n\n        if 'index_hopping' in matrix_data:\n            hop_data = matrix_data['index_hopping']\n\n            plt.figure(figsize=(12, 8))\n\n            # Create heatmap\n            sensitivity_matrix = np.array(hop_data['matrix'])\n            hop_rates = hop_data['hop_rates']\n            condition_labels = hop_data['condition_labels']\n\n            # Plot heatmap\n            sns.heatmap(sensitivity_matrix, \n                       xticklabels=condition_labels,\n                       yticklabels=[f\"{rate:.1%}\" for rate in hop_rates],\n                       annot=True, fmt='.2f', cmap='RdYlBu_r',\n                       cbar_kws={'label': 'Detection Sensitivity'})\n\n            plt.title('Contamination Impact on Variant Detection\\n(Index Hopping Effects)')\n            plt.xlabel('Test Conditions (AF, Depth)')\n            plt.ylabel('Index Hopping Rate')\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n\n            heatmap_path = output_path / \"contam_heatmap.png\"\n            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n            plt.close()\n            print(f\"Contamination heatmap saved to {heatmap_path}\")\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.sim.contamination.ContaminationSimulator.simulate_contamination_effects","title":"<code>simulate_contamination_effects(hop_rates=[0.0, 0.001, 0.002, 0.005, 0.01], barcode_collision_rates=[0.0, 0.0001, 0.0005, 0.001], cross_sample_proportions=[0.0, 0.01, 0.05, 0.1], af_test_values=[0.001, 0.005, 0.01], depth_values=[1000, 5000], n_replicates=20)</code>","text":"<p>Simulate contamination effects on variant detection.</p> <p>Parameters:</p> Name Type Description Default <code>hop_rates</code> <code>List[float]</code> <p>Index-hopping rates to test</p> <code>[0.0, 0.001, 0.002, 0.005, 0.01]</code> <code>barcode_collision_rates</code> <code>List[float]</code> <p>Barcode collision rates to test</p> <code>[0.0, 0.0001, 0.0005, 0.001]</code> <code>cross_sample_proportions</code> <code>List[float]</code> <p>Cross-sample contamination proportions</p> <code>[0.0, 0.01, 0.05, 0.1]</code> <code>af_test_values</code> <code>List[float]</code> <p>Allele fractions to test contamination effects on</p> <code>[0.001, 0.005, 0.01]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>20</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with contamination simulation results</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def simulate_contamination_effects(self,\n                                 hop_rates: List[float] = [0.0, 0.001, 0.002, 0.005, 0.01],\n                                 barcode_collision_rates: List[float] = [0.0, 0.0001, 0.0005, 0.001],\n                                 cross_sample_proportions: List[float] = [0.0, 0.01, 0.05, 0.1],\n                                 af_test_values: List[float] = [0.001, 0.005, 0.01],\n                                 depth_values: List[int] = [1000, 5000],\n                                 n_replicates: int = 20) -&gt; Dict[str, Any]:\n    \"\"\"Simulate contamination effects on variant detection.\n\n    Args:\n        hop_rates: Index-hopping rates to test\n        barcode_collision_rates: Barcode collision rates to test\n        cross_sample_proportions: Cross-sample contamination proportions\n        af_test_values: Allele fractions to test contamination effects on\n        depth_values: UMI depths to test\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with contamination simulation results\n    \"\"\"\n    print(\"Running contamination stress testing...\")\n\n    results = {}\n    sensitivity_matrix = []\n\n    # Test index-hopping effects\n    print(\"  Testing index-hopping effects...\")\n    hop_results = self._test_index_hopping(hop_rates, af_test_values, \n                                          depth_values, n_replicates)\n    results['index_hopping'] = hop_results\n\n    # Test barcode collision effects\n    print(\"  Testing barcode collision effects...\")\n    collision_results = self._test_barcode_collisions(barcode_collision_rates,\n                                                     af_test_values, depth_values, \n                                                     n_replicates)\n    results['barcode_collisions'] = collision_results\n\n    # Test cross-sample contamination\n    print(\"  Testing cross-sample contamination...\")\n    cross_contam_results = self._test_cross_sample_contamination(\n        cross_sample_proportions, af_test_values, depth_values, n_replicates)\n    results['cross_sample_contamination'] = cross_contam_results\n\n    # Create sensitivity matrix for heatmap\n    sensitivity_matrix = self._create_sensitivity_matrix(results, af_test_values, \n                                                       depth_values)\n    results['sensitivity_matrix'] = sensitivity_matrix\n\n    self.contamination_results = results\n    return results\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.sim.contamination.ContaminationSimulator.generate_contamination_reports","title":"<code>generate_contamination_reports(output_dir='reports')</code>","text":"<p>Generate contamination analysis reports.</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def generate_contamination_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate contamination analysis reports.\"\"\"\n    if not self.contamination_results:\n        print(\"No contamination results to report\")\n        return\n\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save contamination sensitivity results\n    contam_payload = dict(self.contamination_results)\n    contam_payload[\"schema_version\"] = \"1.0.0\"\n    contam_path = output_path / \"contam_sensitivity.json\"\n    with open(contam_path, 'w') as f:\n        json.dump(contam_payload, f, indent=2)\n    print(f\"Contamination sensitivity results saved to {contam_path}\")\n\n    # Generate contamination heatmap\n    self._plot_contamination_heatmap(output_path)\n</code></pre>"},{"location":"api-reference%202/#precise_mrd.sim.contamination.run_contamination_stress_test","title":"<code>precise_mrd.sim.contamination.run_contamination_stress_test(config, rng, output_dir='reports')</code>","text":"<p>Run complete contamination stress testing suite.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Random number generator</p> required <code>output_dir</code> <code>str</code> <p>Output directory for reports</p> <code>'reports'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Contamination analysis results</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def run_contamination_stress_test(config: PipelineConfig, \n                                rng: np.random.Generator,\n                                output_dir: str = \"reports\") -&gt; Dict[str, Any]:\n    \"\"\"Run complete contamination stress testing suite.\n\n    Args:\n        config: Pipeline configuration\n        rng: Random number generator\n        output_dir: Output directory for reports\n\n    Returns:\n        Contamination analysis results\n    \"\"\"\n    simulator = ContaminationSimulator(config, rng)\n\n    # Run contamination simulations\n    results = simulator.simulate_contamination_effects()\n\n    # Generate reports\n    simulator.generate_contamination_reports(output_dir)\n\n    return results\n</code></pre>"},{"location":"api-reference%202/#core-pipeline-components","title":"Core Pipeline Components","text":""},{"location":"api-reference%202/#simulation","title":"Simulation","text":""},{"location":"api-reference%202/#precise_mrd.simulate.simulate_reads","title":"<code>precise_mrd.simulate.simulate_reads(config, rng, output_path=None)</code>","text":"<p>Simulate synthetic UMI reads for ctDNA analysis.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with simulated read data</p> Source code in <code>src/precise_mrd/simulate.py</code> <pre><code>def simulate_reads(\n    config: PipelineConfig, \n    rng: np.random.Generator,\n    output_path: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Simulate synthetic UMI reads for ctDNA analysis.\n\n    Args:\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n\n    Returns:\n        DataFrame with simulated read data\n    \"\"\"\n\n    # Extract simulation parameters\n    sim_config = config.simulation\n\n    # Handle case where simulation config might be a dict\n    if isinstance(sim_config, dict):\n        allele_fractions = sim_config['allele_fractions']\n        umi_depths = sim_config['umi_depths'] \n        n_replicates = sim_config['n_replicates']\n    else:\n        allele_fractions = sim_config.allele_fractions\n        umi_depths = sim_config.umi_depths\n        n_replicates = sim_config.n_replicates\n\n    # Handle UMI config\n    umi_config = config.umi\n    if isinstance(umi_config, dict):\n        max_family_size = umi_config['max_family_size']\n    else:\n        max_family_size = umi_config.max_family_size\n\n    # Generate synthetic data based on configuration\n    n_variants = len(allele_fractions)\n    n_depths = len(umi_depths)\n    total_samples = n_variants * n_depths * n_replicates\n\n    # Create grid of conditions\n    data = []\n    sample_id = 0\n\n    for af in allele_fractions:\n        for depth in umi_depths:\n            for rep in range(n_replicates):\n                # Simulate UMI families\n                n_families = depth\n\n                # Background error rate (trinucleotide context dependent)\n                background_rate = rng.uniform(1e-5, 1e-3)\n\n                # Generate reads per family (Poisson distributed)\n                family_sizes = rng.poisson(lam=5, size=n_families)\n                family_sizes = np.clip(family_sizes, 1, max_family_size)\n\n                # Simulate variant calls\n                # True positives based on allele fraction\n                n_true_variants = rng.binomial(n_families, af)\n\n                # False positives from background errors\n                n_false_positives = rng.binomial(\n                    n_families - n_true_variants, \n                    background_rate\n                )\n\n                # Quality scores (higher for true variants)\n                quality_scores = rng.normal(25, 5, n_families)\n                quality_scores = np.clip(quality_scores, 10, 40)\n\n                sample_data = {\n                    'sample_id': sample_id,\n                    'allele_fraction': af,\n                    'target_depth': depth,\n                    'replicate': rep,\n                    'n_families': n_families,\n                    'n_true_variants': n_true_variants,\n                    'n_false_positives': n_false_positives,\n                    'background_rate': background_rate,\n                    'mean_family_size': np.mean(family_sizes),\n                    'mean_quality': np.mean(quality_scores),\n                    'config_hash': config.config_hash(),\n                }\n                data.append(sample_data)\n                sample_id += 1\n\n    df = pd.DataFrame(data)\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference%202/#umi-collapse","title":"UMI Collapse","text":""},{"location":"api-reference%202/#precise_mrd.collapse.collapse_umis","title":"<code>precise_mrd.collapse.collapse_umis(reads_df, config, rng, output_path=None, is_fastq_data=False, use_parallel=False, n_partitions=None)</code>","text":"<p>Collapse UMI families and call consensus.</p> <p>Parameters:</p> Name Type Description Default <code>reads_df</code> <code>Union[DataFrame, DataFrame]</code> <p>DataFrame with read data (synthetic or FASTQ)</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>is_fastq_data</code> <code>bool</code> <p>Whether input data comes from FASTQ files</p> <code>False</code> <code>use_parallel</code> <code>bool</code> <p>Whether to use parallel processing with Dask</p> <code>False</code> <code>n_partitions</code> <code>int</code> <p>Number of partitions for parallel processing</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> Source code in <code>src/precise_mrd/collapse.py</code> <pre><code>def collapse_umis(\n    reads_df: Union[pd.DataFrame, dd.DataFrame],\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    is_fastq_data: bool = False,\n    use_parallel: bool = False,\n    n_partitions: int = None\n) -&gt; pd.DataFrame:\n    \"\"\"Collapse UMI families and call consensus.\n\n    Args:\n        reads_df: DataFrame with read data (synthetic or FASTQ)\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        is_fastq_data: Whether input data comes from FASTQ files\n        use_parallel: Whether to use parallel processing with Dask\n        n_partitions: Number of partitions for parallel processing\n\n    Returns:\n        DataFrame with collapsed UMI data\n    \"\"\"\n\n    # Use parallel processing if requested and Dask is available\n    if use_parallel and DASK_AVAILABLE and isinstance(reads_df, pd.DataFrame):\n        return _collapse_umis_parallel(reads_df, config, rng, output_path, is_fastq_data, n_partitions)\n\n    # Fall back to original implementation for other cases\n    return _collapse_umis_sequential(reads_df, config, rng, output_path, is_fastq_data)\n</code></pre>"},{"location":"api-reference%202/#error-modeling","title":"Error Modeling","text":""},{"location":"api-reference%202/#precise_mrd.error_model.fit_error_model","title":"<code>precise_mrd.error_model.fit_error_model(collapsed_df, config, rng, output_path=None, use_cache=True, cache_dir='.cache', use_advanced_stats=False)</code>","text":"<p>Fit trinucleotide context-specific error model with optional caching and advanced statistics.</p> <p>Parameters:</p> Name Type Description Default <code>collapsed_df</code> <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use caching for expensive computations</p> <code>True</code> <code>cache_dir</code> <code>str</code> <p>Directory for cache files</p> <code>'.cache'</code> <code>use_advanced_stats</code> <code>bool</code> <p>Whether to use advanced Bayesian modeling</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with error model parameters</p> Source code in <code>src/precise_mrd/error_model.py</code> <pre><code>def fit_error_model(\n    collapsed_df: pd.DataFrame,\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    use_cache: bool = True,\n    cache_dir: str = \".cache\",\n    use_advanced_stats: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Fit trinucleotide context-specific error model with optional caching and advanced statistics.\n\n    Args:\n        collapsed_df: DataFrame with collapsed UMI data\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        use_cache: Whether to use caching for expensive computations\n        cache_dir: Directory for cache files\n        use_advanced_stats: Whether to use advanced Bayesian modeling\n\n    Returns:\n        DataFrame with error model parameters\n    \"\"\"\n\n    # Try to load from cache first\n    if use_cache:\n        cache_key = _get_cache_key(config, collapsed_df)\n        cached_result = _load_cached_error_model(cache_key, cache_dir)\n        if cached_result is not None:\n            print(f\"  Using cached error model: {cache_key}\")\n            if output_path:\n                cached_result.to_parquet(output_path, index=False)\n            return cached_result\n\n    # Use advanced Bayesian modeling if requested\n    if use_advanced_stats:\n        print(\"  Using advanced Bayesian error modeling...\")\n        bayesian_model = BayesianErrorModel(config)\n        advanced_results = bayesian_model.fit_bayesian_model(collapsed_df, rng)\n\n        # Convert to DataFrame format for compatibility\n        error_data = []\n        for context, params in advanced_results['context_error_rates'].items():\n            error_data.append({\n                'trinucleotide_context': context,\n                'error_rate': params['error_rate'],\n                'ci_lower': params['ci_lower'],\n                'ci_upper': params['ci_upper'],\n                'n_observations': params['n_observations'],\n                'model_type': 'bayesian',\n                'config_hash': params.get('config_hash', config.config_hash())\n            })\n\n        df = pd.DataFrame(error_data)\n\n        # Save to cache for future use\n        if use_cache:\n            _save_cached_error_model(df, cache_key, cache_dir)\n\n        if output_path:\n            df.to_parquet(output_path, index=False)\n\n        return df\n\n    # Original simple error model\n\n    # Define trinucleotide contexts\n    contexts = [\n        'AAA', 'AAC', 'AAG', 'AAT',\n        'ACA', 'ACC', 'ACG', 'ACT',\n        'AGA', 'AGC', 'AGG', 'AGT',\n        'ATA', 'ATC', 'ATG', 'ATT',\n        'CAA', 'CAC', 'CAG', 'CAT',\n        'CCA', 'CCC', 'CCG', 'CCT',\n        'CGA', 'CGC', 'CGG', 'CGT',\n        'CTA', 'CTC', 'CTG', 'CTT',\n        'GAA', 'GAC', 'GAG', 'GAT',\n        'GCA', 'GCC', 'GCG', 'GCT',\n        'GGA', 'GGC', 'GGG', 'GGT',\n        'GTA', 'GTC', 'GTG', 'GTT',\n        'TAA', 'TAC', 'TAG', 'TAT',\n        'TCA', 'TCC', 'TCG', 'TCT',\n        'TGA', 'TGC', 'TGG', 'TGT',\n        'TTA', 'TTC', 'TTG', 'TTT'\n    ]\n\n    # Fit error rates for each trinucleotide context\n    error_data = []\n\n    for context in contexts:\n        # Estimate error rate from negative control samples (lowest AF)\n        negative_samples = collapsed_df[collapsed_df['allele_fraction'] &lt;= 0.0001]\n\n        if len(negative_samples) == 0:\n            # Fallback to lowest AF samples\n            min_af = collapsed_df['allele_fraction'].min()\n            negative_samples = collapsed_df[collapsed_df['allele_fraction'] == min_af]\n\n        # Count variants in negative samples for this context\n        if len(negative_samples) &gt; 0:\n            variant_rate = negative_samples['is_variant'].mean()\n            # Add context-specific variation\n            context_modifier = rng.uniform(0.5, 2.0)\n            error_rate = variant_rate * context_modifier\n        else:\n            # Default error rate\n            error_rate = rng.uniform(1e-5, 1e-3)\n\n        # Confidence interval from bootstrap\n        if len(negative_samples) &gt; 10:\n            bootstrap_rates = []\n            for _ in range(100):\n                boot_sample = negative_samples.sample(\n                    n=len(negative_samples), \n                    replace=True, \n                    random_state=rng.integers(0, 2**32-1)\n                )\n                boot_rate = boot_sample['is_variant'].mean() * context_modifier\n                bootstrap_rates.append(boot_rate)\n\n            ci_lower = np.percentile(bootstrap_rates, 2.5)\n            ci_upper = np.percentile(bootstrap_rates, 97.5)\n        else:\n            ci_lower = error_rate * 0.5\n            ci_upper = error_rate * 2.0\n\n        error_data.append({\n            'trinucleotide_context': context,\n            'error_rate': error_rate,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper,\n            'n_observations': len(negative_samples),\n            'config_hash': config.config_hash(),\n        })\n\n    df = pd.DataFrame(error_data)\n\n    # Save to cache for future use\n    if use_cache:\n        cache_key = _get_cache_key(config, collapsed_df)\n        _save_cached_error_model(df, cache_key, cache_dir)\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference%202/#statistical-testing","title":"Statistical Testing","text":""},{"location":"api-reference%202/#precise_mrd.call.call_mrd","title":"<code>precise_mrd.call.call_mrd(collapsed_df, error_model_df, config, rng, output_path=None, use_ml_calling=False, ml_model_type='ensemble', use_deep_learning=False, dl_model_type='cnn_lstm')</code>","text":"<p>Perform MRD calling with statistical testing or ML-based approaches.</p> <p>Parameters:</p> Name Type Description Default <code>collapsed_df</code> <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> required <code>error_model_df</code> <code>DataFrame</code> <p>DataFrame with error model</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>use_ml_calling</code> <code>bool</code> <p>Whether to use ML-based variant calling</p> <code>False</code> <code>ml_model_type</code> <code>str</code> <p>Type of ML model to use ('ensemble', 'xgboost', 'lightgbm', 'gbm')</p> <code>'ensemble'</code> <code>use_deep_learning</code> <code>bool</code> <p>Whether to use deep learning-based variant calling</p> <code>False</code> <code>dl_model_type</code> <code>str</code> <p>Type of deep learning model to use ('cnn_lstm', 'hybrid', 'transformer')</p> <code>'cnn_lstm'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with MRD calls and statistics</p> Source code in <code>src/precise_mrd/call.py</code> <pre><code>def call_mrd(\n    collapsed_df: pd.DataFrame,\n    error_model_df: pd.DataFrame,\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    use_ml_calling: bool = False,\n    ml_model_type: str = 'ensemble',\n    use_deep_learning: bool = False,\n    dl_model_type: str = 'cnn_lstm'\n) -&gt; pd.DataFrame:\n    \"\"\"Perform MRD calling with statistical testing or ML-based approaches.\n\n    Args:\n        collapsed_df: DataFrame with collapsed UMI data\n        error_model_df: DataFrame with error model\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        use_ml_calling: Whether to use ML-based variant calling\n        ml_model_type: Type of ML model to use ('ensemble', 'xgboost', 'lightgbm', 'gbm')\n        use_deep_learning: Whether to use deep learning-based variant calling\n        dl_model_type: Type of deep learning model to use ('cnn_lstm', 'hybrid', 'transformer')\n\n    Returns:\n        DataFrame with MRD calls and statistics\n    \"\"\"\n\n    # Use ML-based variant calling if requested\n    if use_ml_calling:\n        print(f\"  Using enhanced ML-based variant calling ({ml_model_type})...\")\n\n        # Choose model based on type\n        if ml_model_type == 'ensemble':\n            ml_caller = EnsembleVariantCaller(config)\n        else:\n            ml_caller = GradientBoostedVariantCaller(config, ml_model_type)\n\n        # Train the model\n        if ml_model_type == 'ensemble':\n            training_results = ml_caller.train_ensemble(collapsed_df, rng)\n        else:\n            training_results = ml_caller.train_model(collapsed_df, rng)\n\n        # Get ML predictions\n        ml_probabilities = ml_caller.predict_variants(collapsed_df)\n\n        # Use optimal threshold from training\n        optimal_threshold = training_results.get('optimal_threshold', np.median(ml_probabilities))\n        ml_calls = (ml_probabilities &gt; optimal_threshold).astype(int)\n\n        # Get feature importance for reporting\n        feature_importance = ml_caller.get_feature_importance()\n\n        print(f\"  Ensemble trained with {len(ml_caller.models)} models\")\n        print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n        print(f\"  Top features: {list(feature_importance.keys())[:3]}\")\n\n        # Create results DataFrame (vectorized)\n        results_df = pd.DataFrame({\n            'sample_id': collapsed_df['sample_id'],\n            'family_id': collapsed_df['family_id'],\n            'family_size': collapsed_df['family_size'],\n            'quality_score': collapsed_df['quality_score'],\n            'consensus_agreement': collapsed_df['consensus_agreement'],\n            'passes_quality': collapsed_df['passes_quality'],\n            'passes_consensus': collapsed_df['passes_consensus'],\n            'is_variant': ml_calls,\n            'p_value': 1.0 - ml_probabilities,  # Convert probability to p-value-like score\n            'ml_probability': ml_probabilities,\n            'ml_threshold': optimal_threshold,\n            'calling_method': 'ml_ensemble',\n            'config_hash': config.config_hash()\n        })\n\n        df = results_df\n\n    # Use deep learning-based variant calling if requested\n    if use_deep_learning:\n        print(f\"  Using deep learning-based variant calling ({dl_model_type})...\")\n\n        # Initialize deep learning caller\n        dl_caller = DeepLearningVariantCaller(config, dl_model_type)\n\n        # Train the model\n        training_results = dl_caller.train_model(collapsed_df, rng)\n\n        # Get deep learning predictions\n        dl_probabilities = dl_caller.predict_variants(collapsed_df)\n\n        # Use optimal threshold from training\n        optimal_threshold = training_results.get('optimal_threshold', np.median(dl_probabilities))\n        dl_calls = (dl_probabilities &gt; optimal_threshold).astype(int)\n\n        # Get model summary for reporting\n        model_summary = dl_caller.get_model_summary()\n\n        print(f\"  Deep learning model trained: {model_summary}\")\n        print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n\n        # Create results DataFrame (vectorized)\n        results_df = pd.DataFrame({\n            'sample_id': collapsed_df['sample_id'],\n            'family_id': collapsed_df['family_id'],\n            'family_size': collapsed_df['family_size'],\n            'quality_score': collapsed_df['quality_score'],\n            'consensus_agreement': collapsed_df['consensus_agreement'],\n            'passes_quality': collapsed_df['passes_quality'],\n            'passes_consensus': collapsed_df['passes_consensus'],\n            'is_variant': dl_calls,\n            'p_value': 1.0 - dl_probabilities,  # Convert probability to p-value-like score\n            'dl_probability': dl_probabilities,\n            'dl_threshold': optimal_threshold,\n            'calling_method': f'dl_{dl_model_type}',\n            'config_hash': config.config_hash()\n        })\n\n        df = results_df\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n\n    stats_config = config.stats\n\n    # Handle case where stats config might be a dict\n    if isinstance(stats_config, dict):\n        test_type = stats_config['test_type']\n        alpha = stats_config['alpha']\n        fdr_method = stats_config['fdr_method']\n    else:\n        test_type = stats_config.test_type\n        alpha = stats_config.alpha\n        fdr_method = stats_config.fdr_method\n\n    # Group by sample for statistical testing\n    call_data = []\n\n    for sample_id in collapsed_df['sample_id'].unique():\n        sample_data = collapsed_df[collapsed_df['sample_id'] == sample_id]\n\n        if len(sample_data) == 0:\n            continue\n\n        # Get sample metadata\n        allele_fraction = sample_data['allele_fraction'].iloc[0]\n\n        # Count variants and total families\n        n_variants = sample_data['is_variant'].sum()\n        n_total = len(sample_data)\n\n        if n_total == 0:\n            continue\n\n        # Get expected error rate (average across contexts)\n        mean_error_rate = error_model_df['error_rate'].mean()\n        expected_variants = n_total * mean_error_rate\n\n        # Perform statistical test\n        if test_type == \"poisson\":\n            p_value = poisson_test(n_variants, expected_variants)\n        elif test_type == \"binomial\":\n            p_value = binomial_test(n_variants, n_total, mean_error_rate)\n        else:\n            raise ValueError(f\"Unknown test type: {test_type}\")\n\n        # Calculate effect size\n        if expected_variants &gt; 0:\n            fold_change = n_variants / expected_variants\n        else:\n            fold_change = float('inf') if n_variants &gt; 0 else 1.0\n\n        # Quality metrics\n        mean_quality = sample_data['quality_score'].mean()\n        mean_consensus = sample_data['consensus_agreement'].mean()\n\n        call_data.append({\n            'sample_id': sample_id,\n            'allele_fraction': allele_fraction,\n            'n_variants': n_variants,\n            'n_total': n_total,\n            'variant_fraction': n_variants / n_total if n_total &gt; 0 else 0,\n            'expected_variants': expected_variants,\n            'fold_change': fold_change,\n            'p_value': p_value,\n            'mean_quality': mean_quality,\n            'mean_consensus': mean_consensus,\n            'test_type': test_type,\n            'config_hash': config.config_hash(),\n        })\n\n    if not call_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(call_data)\n\n    # Apply multiple testing correction\n    p_values = df['p_value'].values\n    rejected, adjusted_p = benjamini_hochberg_correction(\n        p_values, \n        alpha\n    )\n\n    df['p_adjusted'] = adjusted_p\n    df['significant'] = rejected\n    df['alpha'] = alpha\n    df['fdr_method'] = fdr_method\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference%202/#performance-metrics","title":"Performance Metrics","text":""},{"location":"api-reference%202/#precise_mrd.metrics.calculate_metrics","title":"<code>precise_mrd.metrics.calculate_metrics(calls_df, rng, n_bootstrap=1000, n_jobs=-1, config=None, use_advanced_ci=False, run_validation=False)</code>","text":"<p>Calculate comprehensive performance metrics with optional advanced confidence intervals and validation.</p> <p>Parameters:</p> Name Type Description Default <code>calls_df</code> <code>DataFrame</code> <p>DataFrame with MRD calls</p> required <code>rng</code> <code>Generator</code> <p>Random number generator for bootstrap</p> required <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples</p> <code>1000</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for bootstrap (-1 for all cores)</p> <code>-1</code> <code>config</code> <code>Optional[PipelineConfig]</code> <p>Pipeline configuration for advanced statistics</p> <code>None</code> <code>use_advanced_ci</code> <code>bool</code> <p>Whether to use advanced confidence interval methods</p> <code>False</code> <code>run_validation</code> <code>bool</code> <p>Whether to run comprehensive statistical validation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all metrics</p> Source code in <code>src/precise_mrd/metrics.py</code> <pre><code>def calculate_metrics(\n    calls_df: pd.DataFrame,\n    rng: np.random.Generator,\n    n_bootstrap: int = 1000,\n    n_jobs: int = -1,\n    config: Optional[PipelineConfig] = None,\n    use_advanced_ci: bool = False,\n    run_validation: bool = False\n) -&gt; Dict[str, Any]:\n    \"\"\"Calculate comprehensive performance metrics with optional advanced confidence intervals and validation.\n\n    Args:\n        calls_df: DataFrame with MRD calls\n        rng: Random number generator for bootstrap\n        n_bootstrap: Number of bootstrap samples\n        n_jobs: Number of parallel jobs for bootstrap (-1 for all cores)\n        config: Pipeline configuration for advanced statistics\n        use_advanced_ci: Whether to use advanced confidence interval methods\n        run_validation: Whether to run comprehensive statistical validation\n\n    Returns:\n        Dictionary with all metrics\n    \"\"\"\n    if len(calls_df) == 0:\n        return {\n            \"roc_auc\": 0.0,\n            \"average_precision\": 0.0,\n            \"detected_cases\": 0,\n            \"total_cases\": 0,\n            \"calibration\": []\n        }\n\n    # Define truth labels (high AF = positive case)\n    if 'allele_fraction' in calls_df.columns:\n        y_true = (calls_df['allele_fraction'] &gt; 0.001).astype(int)\n    else:\n        # For real data or ML-based calling, we don't have ground truth\n        # Use a default approach or skip certain metrics\n        print(\"  Warning: No allele_fraction column found, using simplified metrics\")\n        y_true = np.zeros(len(calls_df))  # All negative for compatibility\n\n    # Use variant fraction as prediction score\n    if 'variant_fraction' in calls_df.columns:\n        y_score = calls_df['variant_fraction'].values\n    elif 'ml_probability' in calls_df.columns:\n        y_score = calls_df['ml_probability'].values\n    else:\n        # Fallback to p-value if available\n        if 'p_value' in calls_df.columns:\n            y_score = 1.0 - calls_df['p_value'].values  # Convert p-value to score\n        else:\n            y_score = np.random.random(len(calls_df))  # Random fallback\n\n    # Basic metrics\n    roc_auc = roc_auc_score(y_true, y_score)\n    avg_precision = average_precision(y_true, y_score)\n\n    # Confidence intervals\n    if use_advanced_ci and config:\n        print(\"  Using advanced confidence interval methods...\")\n        adv_ci = AdvancedConfidenceIntervals(config)\n\n        # Advanced bootstrap CI for ROC AUC\n        roc_ci = adv_ci.bootstrap_confidence_interval(\n            y_score, lambda x: roc_auc_score(y_true, x), n_bootstrap, rng\n        )\n\n        # Advanced bootstrap CI for Average Precision\n        ap_ci = adv_ci.bootstrap_confidence_interval(\n            y_score, lambda x: average_precision(y_true, x), n_bootstrap, rng\n        )\n\n        # Add method information\n        roc_ci['method'] = 'advanced_bootstrap'\n        ap_ci['method'] = 'advanced_bootstrap'\n    else:\n        # Standard bootstrap CI (parallel processing)\n        roc_ci = bootstrap_metric(y_true, y_score, roc_auc_score, n_bootstrap, rng, n_jobs)\n        ap_ci = bootstrap_metric(y_true, y_score, average_precision, n_bootstrap, rng, n_jobs)\n\n    # Detection statistics\n    if 'significant' in calls_df.columns:\n        if 'allele_fraction' in calls_df.columns:\n            detected_cases = int(np.sum(calls_df['significant'] &amp; (y_true == 1)))\n            total_cases = int(np.sum(y_true))\n        else:\n            # For real data, just count significant calls\n            detected_cases = int(np.sum(calls_df['significant']))\n            total_cases = len(calls_df)  # All samples are \"cases\" in real data context\n    else:\n        detected_cases = 0\n        total_cases = len(calls_df)\n\n    # Calibration analysis\n    calibration = calibration_analysis(y_true, y_score)\n\n    # Brier score\n    brier_score = float(np.mean((y_score - y_true) ** 2))\n\n    # Add statistical validation if requested\n    validation_results = {}\n    if run_validation and config:\n        print(\"  Running comprehensive statistical validation...\")\n\n        # Cross-validation for model performance\n        if 'ml_probability' in calls_df.columns:\n            X = calls_df[['family_size', 'quality_score', 'consensus_agreement']].values\n            y = calls_df['is_variant'].values\n\n            # Simple model function for demonstration\n            def simple_model_func(X_train, y_train):\n                from sklearn.ensemble import RandomForestClassifier\n                model = RandomForestClassifier(n_estimators=10, random_state=config.seed)\n                model.fit(X_train, y_train)\n                return model\n\n            cv = CrossValidator(config)\n            cv_results = cv.k_fold_cross_validation(X, y, simple_model_func, k_folds=3)\n            validation_results['cross_validation'] = cv_results\n\n        # Calibration analysis\n        if 'ml_probability' in calls_df.columns:\n            calibrator = ModelValidator(config)\n            calibration_results = calibrator.calibration_analysis(y_true, calls_df['ml_probability'].values)\n            validation_results['calibration_analysis'] = calibration_results\n\n        # Robustness analysis\n        robustness = RobustnessAnalyzer(config)\n        robustness_results = robustness.bootstrap_robustness(calls_df, n_bootstrap=50)\n        validation_results['robustness_analysis'] = robustness_results\n\n        # Uncertainty quantification\n        uncertainty = UncertaintyQuantifier(config)\n        # Example: quantify uncertainty in variant rate estimates\n        variant_rates = calls_df['is_variant'].values\n        uncertainty_results = uncertainty.bayesian_uncertainty([variant_rates])\n        validation_results['uncertainty_quantification'] = uncertainty_results\n\n    return {\n        \"roc_auc\": float(roc_auc),\n        \"roc_auc_ci\": roc_ci,\n        \"average_precision\": float(avg_precision),\n        \"average_precision_ci\": ap_ci,\n        \"brier_score\": brier_score,\n        \"detected_cases\": detected_cases,\n        \"total_cases\": total_cases,\n        \"calibration\": calibration,\n        \"statistical_validation\": validation_results if validation_results else None\n    }\n</code></pre>"},{"location":"api-reference%202/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference%202/#basic-detection-limit-analysis","title":"Basic Detection Limit Analysis","text":"<pre><code>import numpy as np\nfrom precise_mrd.config import load_config\nfrom precise_mrd.eval.lod import LODAnalyzer\n\n# Load configuration\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)\n\n# Create analyzer\nanalyzer = LODAnalyzer(config, rng)\n\n# Estimate Limit of Blank\nlob_results = analyzer.estimate_lob(n_blank_runs=100)\nprint(f\"LoB: {lob_results['lob_value']:.3f}\")\n\n# Estimate Limit of Detection\nlod_results = analyzer.estimate_lod(\n    af_range=(1e-4, 1e-2),\n    depth_values=[1000, 5000, 10000],\n    n_replicates=50\n)\n\n# Print LoD for each depth\nfor depth, results in lod_results['depth_results'].items():\n    lod_af = results['lod_af']\n    ci_lower = results['lod_ci_lower']\n    ci_upper = results['lod_ci_upper']\n    print(f\"LoD at {depth} depth: {lod_af:.2e} AF [{ci_lower:.2e}, {ci_upper:.2e}]\")\n\n# Generate reports\nanalyzer.generate_reports(\"reports/\")\n</code></pre>"},{"location":"api-reference%202/#contamination-stress-testing","title":"Contamination Stress Testing","text":"<pre><code>from precise_mrd.sim.contamination import ContaminationSimulator\n\n# Create contamination simulator\nsimulator = ContaminationSimulator(config, rng)\n\n# Run contamination effects simulation\nresults = simulator.simulate_contamination_effects(\n    hop_rates=[0.0, 0.001, 0.005, 0.01],\n    barcode_collision_rates=[0.0, 0.0001, 0.001],\n    cross_sample_proportions=[0.0, 0.01, 0.05, 0.1],\n    af_test_values=[0.001, 0.005, 0.01],\n    depth_values=[1000, 5000],\n    n_replicates=20\n)\n\n# Access results by contamination type\nhop_results = results['index_hopping']\nfor hop_rate, af_data in hop_results.items():\n    for af, depth_data in af_data.items():\n        for depth, metrics in depth_data.items():\n            sensitivity = metrics['mean_sensitivity']\n            print(f\"Hop rate {hop_rate}, AF {af}, depth {depth}: {sensitivity:.3f}\")\n\n# Generate reports\nsimulator.generate_contamination_reports(\"reports/\")\n</code></pre>"},{"location":"api-reference%202/#stratified-power-analysis","title":"Stratified Power Analysis","text":"<pre><code>from precise_mrd.eval.stratified import StratifiedAnalyzer\n\n# Create stratified analyzer\nanalyzer = StratifiedAnalyzer(config, rng)\n\n# Run stratified power analysis\npower_results = analyzer.analyze_stratified_power(\n    af_values=[0.001, 0.005, 0.01, 0.05],\n    depth_values=[1000, 5000, 10000],\n    contexts=['CpG', 'CHG', 'CHH', 'NpN'],\n    n_replicates=50\n)\n\n# Access power results by context\nfor context, depth_data in power_results['stratified_results'].items():\n    print(f\"\\nContext: {context}\")\n    for depth, af_data in depth_data.items():\n        for af, results in af_data.items():\n            power = results['mean_detection_rate']\n            std = results['std_detection_rate']\n            print(f\"  Depth {depth}, AF {af:.0e}: {power:.3f} \u00b1 {std:.3f}\")\n\n# Run calibration analysis\ncalib_results = analyzer.analyze_calibration_by_bins(\n    af_values=[0.001, 0.005, 0.01, 0.05],\n    depth_values=[1000, 5000, 10000],\n    n_bins=10,\n    n_replicates=100\n)\n\n# Print calibration summary\nfor data_point in calib_results['calibration_data']:\n    depth = data_point['depth']\n    af = data_point['af']\n    ece = data_point['ece']\n    print(f\"Depth {depth}, AF {af:.0e}: ECE = {ece:.3f}\")\n</code></pre>"},{"location":"api-reference%202/#complete-pipeline-execution","title":"Complete Pipeline Execution","text":"<pre><code>from precise_mrd.simulate import simulate_reads\nfrom precise_mrd.collapse import collapse_umis\nfrom precise_mrd.error_model import fit_error_model\nfrom precise_mrd.call import call_mrd\nfrom precise_mrd.metrics import calculate_metrics\n\n# Run complete pipeline\ndef run_complete_pipeline(config, rng):\n    \"\"\"Run the complete Precise MRD pipeline.\"\"\"\n\n    # 1. Simulate synthetic reads\n    print(\"Simulating reads...\")\n    reads_df = simulate_reads(config, rng)\n    print(f\"Generated {len(reads_df)} read families\")\n\n    # 2. Collapse UMI families\n    print(\"Collapsing UMI families...\")\n    collapsed_df = collapse_umis(reads_df, config, rng)\n    print(f\"Collapsed to {len(collapsed_df)} consensus reads\")\n\n    # 3. Fit error model\n    print(\"Fitting error model...\")\n    error_model = fit_error_model(collapsed_df, config, rng)\n    print(f\"Error model fitted with {len(error_model)} parameters\")\n\n    # 4. Call variants\n    print(\"Calling variants...\")\n    calls_df = call_mrd(collapsed_df, error_model, config, rng)\n    n_variants = len(calls_df[calls_df['variant_call'] == True])\n    print(f\"Called {n_variants} variants from {len(calls_df)} tests\")\n\n    # 5. Calculate performance metrics\n    print(\"Calculating metrics...\")\n    metrics = calculate_metrics(calls_df, config, rng)\n    print(f\"Calculated {len(metrics)} performance metrics\")\n\n    return {\n        'reads': reads_df,\n        'collapsed': collapsed_df,\n        'error_model': error_model,\n        'calls': calls_df,\n        'metrics': metrics\n    }\n\n# Execute pipeline\nresults = run_complete_pipeline(config, rng)\n</code></pre>"},{"location":"api-reference%202/#convenience-functions","title":"Convenience Functions","text":"<p>For quick analyses, use the convenience functions:</p> <pre><code>from precise_mrd.eval.lod import estimate_lob, estimate_lod, estimate_loq\nfrom precise_mrd.eval.stratified import run_stratified_analysis\nfrom precise_mrd.sim.contamination import run_contamination_stress_test\n\n# Quick detection limit estimation\nlob_results = estimate_lob(config, rng, n_blank_runs=50)\nlod_results = estimate_lod(config, rng, af_range=(1e-4, 1e-2))\nloq_results = estimate_loq(config, rng, cv_threshold=0.20)\n\n# Quick stratified analysis\npower_results, calib_results = run_stratified_analysis(config, rng)\n\n# Quick contamination testing\ncontam_results = run_contamination_stress_test(config, rng)\n</code></pre>"},{"location":"api-reference%202/#configuration-objects","title":"Configuration Objects","text":""},{"location":"api-reference%202/#pipelineconfig","title":"PipelineConfig","text":"<p>Main configuration object containing all analysis parameters:</p> <pre><code>@dataclass\nclass PipelineConfig:\n    run_id: str\n    seed: int\n    simulation: SimulationConfig\n    umi: UMIConfig\n    stats: StatsConfig\n    lod: LODConfig\n</code></pre>"},{"location":"api-reference%202/#simulationconfig","title":"SimulationConfig","text":"<p>Parameters for synthetic data generation:</p> <pre><code>@dataclass\nclass SimulationConfig:\n    allele_fractions: List[float]\n    umi_depths: List[int]\n    n_replicates: int\n    n_bootstrap: int\n</code></pre>"},{"location":"api-reference%202/#umiconfig","title":"UMIConfig","text":"<p>UMI processing parameters:</p> <pre><code>@dataclass\nclass UMIConfig:\n    min_family_size: int\n    max_family_size: int\n    quality_threshold: int\n    consensus_threshold: float\n</code></pre>"},{"location":"api-reference%202/#statsconfig","title":"StatsConfig","text":"<p>Statistical testing parameters:</p> <pre><code>@dataclass\nclass StatsConfig:\n    test_type: str\n    alpha: float\n    fdr_method: str\n</code></pre>"},{"location":"api-reference%202/#lodconfig","title":"LODConfig","text":"<p>Detection limit estimation parameters:</p> <pre><code>@dataclass\nclass LODConfig:\n    detection_threshold: float\n    confidence_level: float\n</code></pre>"},{"location":"api-reference%202/#error-handling","title":"Error Handling","text":"<p>The API uses custom exceptions for specific error conditions:</p> <pre><code>from precise_mrd.config import ConfigurationError\nfrom precise_mrd.eval.lod import DetectionLimitError\n\ntry:\n    config = load_config(\"invalid_config.yaml\")\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n\ntry:\n    lod_results = estimate_lod(config, rng, af_range=(1e-2, 1e-4))  # Invalid range\nexcept DetectionLimitError as e:\n    print(f\"Detection limit error: {e}\")\n</code></pre>"},{"location":"api-reference%202/#logging","title":"Logging","text":"<p>The API includes structured logging for debugging and monitoring:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('precise_mrd')\n\n# Run analysis with detailed logging\nanalyzer = LODAnalyzer(config, rng)\nlob_results = analyzer.estimate_lob(n_blank_runs=100)\n</code></pre>"},{"location":"api-reference%202/#thread-safety","title":"Thread Safety","text":"<p>All analysis functions are thread-safe when using independent random number generators:</p> <pre><code>import concurrent.futures\n\ndef run_parallel_analysis(configs_and_seeds):\n    \"\"\"Run multiple analyses in parallel.\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n\n        for config, seed in configs_and_seeds:\n            rng = np.random.default_rng(seed)  # Independent RNG per thread\n            future = executor.submit(estimate_lod, config, rng)\n            futures.append(future)\n\n        results = [future.result() for future in futures]\n\n    return results\n</code></pre>"},{"location":"api-reference%202/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api-reference%202/#memory-usage","title":"Memory Usage","text":"<ul> <li>LoB estimation: ~10MB for 100 blank runs</li> <li>LoD estimation: ~50MB for full AF grid (15 points \u00d7 3 depths \u00d7 50 reps)</li> <li>Contamination testing: ~100MB for complete stress test</li> <li>Stratified analysis: ~80MB for 4 contexts \u00d7 3 depths \u00d7 4 AFs</li> </ul>"},{"location":"api-reference%202/#runtime-estimates","title":"Runtime Estimates","text":"<p>On a standard CPU (Intel i5):</p> <ul> <li>Quick LoB (20 runs): ~5 seconds</li> <li>Quick LoD (reduced grid): ~15 seconds  </li> <li>Full LoD (complete grid): ~2 minutes</li> <li>Contamination stress test: ~3 minutes</li> <li>Stratified analysis: ~90 seconds</li> </ul>"},{"location":"api-reference%202/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Reduce Replicates: For development, use <code>n_replicates=10-15</code></li> <li>Limit AF Range: Test with narrower AF ranges first</li> <li>Parallel Execution: Use multiple processes for independent analyses</li> <li>Caching: Cache intermediate results for repeated analyses</li> </ol>"},{"location":"api-reference%202/#version-compatibility","title":"Version Compatibility","text":"<p>The API maintains backward compatibility within major versions:</p> <ul> <li>v1.x: Stable API, backward compatible updates</li> <li>Breaking changes: Only in major version updates (v2.0, etc.)</li> <li>Deprecation policy: 2 minor versions warning before removal</li> </ul> <p>Check version compatibility:</p> <pre><code>import precise_mrd\nprint(f\"Precise MRD version: {precise_mrd.__version__}\")\n\n# Check API compatibility\nif precise_mrd.__version__.startswith('1.'):\n    print(\"Compatible with v1.x API\")\n</code></pre>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides comprehensive documentation for the Precise MRD Python API, including all modules for detection limit analytics, contamination testing, and statistical validation.</p>"},{"location":"api-reference/#core-modules","title":"Core Modules","text":""},{"location":"api-reference/#configuration-management","title":"Configuration Management","text":""},{"location":"api-reference/#precise_mrd.config.PipelineConfig","title":"<code>precise_mrd.config.PipelineConfig</code>  <code>dataclass</code>","text":"<p>Enhanced main pipeline configuration with inheritance and validation.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>@dataclass\nclass PipelineConfig:\n    \"\"\"Enhanced main pipeline configuration with inheritance and validation.\"\"\"\n    run_id: str\n    seed: int\n    umi: UMIConfig\n    stats: StatsConfig\n    lod: LODConfig\n    simulation: Optional[SimulationConfig] = None\n    fastq: Optional[FASTQConfig] = None\n    # New fields for enhanced configuration management\n    config_version: str = \"2.0.0\"\n    parent_config: Optional[str] = None\n    template: Optional[str] = None\n    tags: List[str] = field(default_factory=list)\n    description: str = \"\"\n    created_at: Optional[str] = None\n    last_modified: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Validate and enhance configuration after initialization.\"\"\"\n        self._validate()\n        self._set_timestamps()\n\n    def _validate(self):\n        \"\"\"Comprehensive validation of pipeline configuration.\"\"\"\n        if not self.run_id:\n            raise ValueError(\"run_id cannot be empty\")\n        if self.seed &lt; 0:\n            raise ValueError(\"seed must be non-negative\")\n\n        # Validate component configurations\n        self.umi._validate()\n        self.stats._validate()\n        self.lod._validate()\n\n        if self.simulation:\n            self.simulation._validate()\n        if self.fastq:\n            self.fastq._validate()\n\n        # Validate configuration consistency\n        self._validate_consistency()\n\n    def _validate_consistency(self):\n        \"\"\"Validate configuration consistency across components.\"\"\"\n        # Ensure simulation and FASTQ configs don't conflict\n        if self.simulation and self.fastq:\n            # If both are present, ensure they make sense together\n            pass  # Add specific validation rules as needed\n\n    def _set_timestamps(self):\n        \"\"\"Set creation and modification timestamps.\"\"\"\n        import datetime\n        now = datetime.datetime.now().isoformat()\n        if not self.created_at:\n            self.created_at = now\n        self.last_modified = now\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert config to dictionary with metadata.\"\"\"\n        result = asdict(self)\n        result['config_version'] = str(self.config_version)\n        return result\n\n    def config_hash(self) -&gt; str:\n        \"\"\"Compute deterministic hash of configuration.\"\"\"\n        # Include version and parent info for more robust hashing\n        config_str = json.dumps({\n            'version': self.config_version,\n            'parent': self.parent_config,\n            'core_config': {\n                'run_id': self.run_id,\n                'seed': self.seed,\n                'umi': self.umi.__dict__,\n                'stats': self.stats.__dict__,\n                'lod': self.lod.__dict__,\n                'simulation': self.simulation.__dict__ if self.simulation else None,\n                'fastq': self.fastq.__dict__ if self.fastq else None,\n            }\n        }, sort_keys=True)\n        return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n\n    def get_estimated_runtime(self) -&gt; float:\n        \"\"\"Estimate total runtime in minutes.\"\"\"\n        runtime = 0.0\n\n        if self.simulation:\n            runtime += self.simulation.get_estimated_runtime()\n\n        # Add runtime for other pipeline stages\n        # This is a rough estimate - in practice you'd want more sophisticated modeling\n        if self.fastq:\n            # FASTQ processing is I/O bound, estimate based on file size\n            runtime += 5.0  # Base estimate\n\n        return runtime\n\n    def adapt_to_data(self, data_characteristics: Dict[str, Any]) -&gt; 'PipelineConfig':\n        \"\"\"Create an adapted configuration based on data characteristics.\"\"\"\n        adapted = copy.deepcopy(self)\n\n        # Adapt simulation configuration if present\n        if adapted.simulation and 'variant_frequencies' in data_characteristics:\n            adapted.simulation = adapted.simulation.adapt_to_data_characteristics(data_characteristics)\n\n        # Adapt UMI configuration based on quality data\n        if 'quality_stats' in data_characteristics:\n            adapted.umi = adapted.umi.adapt_to_data_quality(data_characteristics['quality_stats'])\n\n        # Update metadata\n        adapted.description = f\"Auto-adapted from {self.run_id}\"\n        adapted.parent_config = self.run_id\n        adapted.last_modified = None  # Will be set by __post_init__\n\n        return adapted\n\n    def validate_compatibility(self, other: 'PipelineConfig') -&gt; List[str]:\n        \"\"\"Validate compatibility between two configurations.\"\"\"\n        issues = []\n\n        # Check version compatibility\n        if self.config_version != other.config_version:\n            issues.append(f\"Configuration version mismatch: {self.config_version} vs {other.config_version}\")\n\n        # Check for incompatible parameter combinations\n        if (self.simulation and other.fastq) or (self.fastq and other.simulation):\n            issues.append(\"Cannot mix simulation and FASTQ modes\")\n\n        return issues\n\n    def merge_with(self, other: 'PipelineConfig', strategy: str = 'override') -&gt; 'PipelineConfig':\n        \"\"\"Merge this configuration with another using specified strategy.\"\"\"\n        if strategy == 'override':\n            # Other config overrides this config\n            merged = copy.deepcopy(other)\n            merged.run_id = f\"{self.run_id}_merged_{other.run_id}\"\n        elif strategy == 'inherit':\n            # Inherit non-conflicting settings from other config\n            merged = copy.deepcopy(self)\n            # Apply inheritance logic here\n        else:\n            raise ValueError(f\"Unknown merge strategy: {strategy}\")\n\n        merged.parent_config = self.run_id\n        merged.last_modified = None  # Will be set by __post_init__\n        return merged\n\n    def export_template(self) -&gt; Dict[str, Any]:\n        \"\"\"Export configuration as a reusable template.\"\"\"\n        return {\n            'template_name': self.run_id,\n            'description': self.description,\n            'base_config': {\n                'umi': self.umi.__dict__,\n                'stats': self.stats.__dict__,\n                'lod': self.lod.__dict__,\n                'simulation': self.simulation.__dict__ if self.simulation else None,\n            },\n            'tags': self.tags,\n            'version': self.config_version\n        }\n\n    @classmethod\n    def from_template(cls, template: Dict[str, Any], run_id: str = None) -&gt; 'PipelineConfig':\n        \"\"\"Create configuration from template.\"\"\"\n        base_config = template['base_config']\n\n        return cls(\n            run_id=run_id or f\"from_template_{template['template_name']}\",\n            seed=7,  # Default seed\n            umi=UMIConfig(**base_config['umi']),\n            stats=StatsConfig(**base_config['stats']),\n            lod=LODConfig(**base_config['lod']),\n            simulation=SimulationConfig(**base_config['simulation']) if base_config['simulation'] else None,\n            template=template['template_name'],\n            description=f\"Generated from template: {template['template_name']}\",\n            tags=template.get('tags', [])\n        )\n</code></pre>"},{"location":"api-reference/#precise_mrd.config.PipelineConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary with metadata.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert config to dictionary with metadata.\"\"\"\n    result = asdict(self)\n    result['config_version'] = str(self.config_version)\n    return result\n</code></pre>"},{"location":"api-reference/#precise_mrd.config.PipelineConfig.config_hash","title":"<code>config_hash()</code>","text":"<p>Compute deterministic hash of configuration.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def config_hash(self) -&gt; str:\n    \"\"\"Compute deterministic hash of configuration.\"\"\"\n    # Include version and parent info for more robust hashing\n    config_str = json.dumps({\n        'version': self.config_version,\n        'parent': self.parent_config,\n        'core_config': {\n            'run_id': self.run_id,\n            'seed': self.seed,\n            'umi': self.umi.__dict__,\n            'stats': self.stats.__dict__,\n            'lod': self.lod.__dict__,\n            'simulation': self.simulation.__dict__ if self.simulation else None,\n            'fastq': self.fastq.__dict__ if self.fastq else None,\n        }\n    }, sort_keys=True)\n    return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"api-reference/#precise_mrd.config.load_config","title":"<code>precise_mrd.config.load_config(path, auto_migrate=True)</code>","text":"<p>Load configuration from YAML file with validation and optional migration.</p> Source code in <code>src/precise_mrd/config.py</code> <pre><code>def load_config(path: str | Path, auto_migrate: bool = True) -&gt; PipelineConfig:\n    \"\"\"Load configuration from YAML file with validation and optional migration.\"\"\"\n    with open(path, 'r') as f:\n        data = yaml.safe_load(f)\n\n    # Handle legacy configurations without version info\n    if 'config_version' not in data:\n        data['config_version'] = \"1.0.0\"\n\n    # Auto-migrate to latest version if requested\n    if auto_migrate:\n        data = ConfigVersionManager.migrate_config(data)\n\n    # Map old format to new format if needed\n    if 'simulation' in data and data['simulation']:\n        data['simulation'] = _migrate_simulation_config(data['simulation'])\n\n    return PipelineConfig(\n        run_id=data.get('run_id', 'unnamed_run'),\n        seed=data.get('seed', 7),\n        simulation=SimulationConfig(**data['simulation']) if data.get('simulation') else None,\n        umi=UMIConfig(**data['umi']),\n        stats=StatsConfig(**data['stats']),\n        lod=LODConfig(**data['lod']),\n        fastq=FASTQConfig(**data['fastq']) if data.get('fastq') else None,\n        config_version=data.get('config_version', '2.0.0'),\n        description=data.get('description', ''),\n        tags=data.get('tags', [])\n    )\n</code></pre>"},{"location":"api-reference/#detection-limit-analytics","title":"Detection Limit Analytics","text":""},{"location":"api-reference/#loblodloq-analysis","title":"LoB/LoD/LoQ Analysis","text":""},{"location":"api-reference/#precise_mrd.eval.lod.LODAnalyzer","title":"<code>precise_mrd.eval.lod.LODAnalyzer</code>","text":"<p>Analyzer for Limit of Blank, Detection, and Quantification.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>class LODAnalyzer:\n    \"\"\"Analyzer for Limit of Blank, Detection, and Quantification.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.lob_results: Optional[Dict[str, Any]] = None\n        self.lod_results: Optional[Dict[str, Any]] = None\n        self.loq_results: Optional[Dict[str, Any]] = None\n\n    def estimate_lob(self, n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Blank (LoB).\n\n        LoB represents the highest measurement result that is likely to be observed \n        for a blank specimen. Calculated as the 95th percentile of blank measurements.\n\n        Args:\n            n_blank_runs: Number of blank (AF=0) simulations to run\n\n        Returns:\n            Dictionary with LoB results including:\n            - lob_value: 95th percentile of blank test statistics\n            - blank_mean: Mean of blank measurements\n            - blank_std: Standard deviation of blank measurements\n            - blank_measurements: All blank measurement values\n        \"\"\"\n        print(f\"Estimating LoB with {n_blank_runs} blank runs...\")\n\n        # Create blank configuration (AF = 0)\n        blank_config = self._create_blank_config()\n        blank_measurements = []\n\n        for i in range(n_blank_runs):\n            # Use different random seed for each run\n            run_rng = np.random.default_rng(self.config.seed + i)\n\n            # Simulate blank reads (no true variants)\n            reads_df = simulate_reads(blank_config, run_rng)\n\n            # Process through pipeline to get test statistic\n            collapsed_df = collapse_umis(reads_df, blank_config, run_rng)\n            error_model = fit_error_model(collapsed_df, blank_config, run_rng)\n            calls_df = call_mrd(collapsed_df, error_model, blank_config, run_rng)\n\n            # Extract test statistic (number of variant calls in blank)\n            n_variant_calls = len(calls_df[calls_df['variant_call'] == True])\n            blank_measurements.append(n_variant_calls)\n\n        blank_measurements = np.array(blank_measurements)\n\n        # Compute LoB as 95th percentile\n        lob_value = np.percentile(blank_measurements, 95)\n        blank_mean = np.mean(blank_measurements)\n        blank_std = np.std(blank_measurements)\n\n        self.lob_results = {\n            'lob_value': float(lob_value),\n            'blank_mean': float(blank_mean),\n            'blank_std': float(blank_std),\n            'blank_measurements': blank_measurements.tolist(),\n            'n_blank_runs': n_blank_runs,\n            'percentile': 95,\n            'config_hash': self.config.config_hash()\n        }\n\n        print(f\"LoB estimated: {lob_value:.3f} (mean: {blank_mean:.3f}, std: {blank_std:.3f})\")\n        return self.lob_results\n\n    def estimate_lod(self, \n                     af_range: Tuple[float, float] = (1e-4, 1e-2),\n                     depth_values: List[int] = [1000, 5000, 10000],\n                     n_replicates: int = 50,\n                     target_detection_rate: float = 0.95,\n                     alpha: float = 0.05,\n                     beta: float = 0.05) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Detection (LoD).\n\n        LoD is the lowest analyte concentration likely to be reliably detected.\n        Calculated as the concentration yielding 95% detection probability.\n\n        Args:\n            af_range: Range of allele fractions to test (min, max)\n            depth_values: UMI depths to test\n            n_replicates: Number of replicates per AF/depth combination\n            target_detection_rate: Target detection rate (e.g., 0.95 for 95%)\n            alpha: Type I error rate\n            beta: Type II error rate\n\n        Returns:\n            Dictionary with LoD results for each depth\n        \"\"\"\n        print(f\"Estimating LoD across AF range {af_range} at depths {depth_values}...\")\n\n        # Generate AF grid (log-spaced)\n        af_min, af_max = af_range\n        af_values = np.logspace(np.log10(af_min), np.log10(af_max), 15)\n\n        lod_results = {}\n\n        for depth in depth_values:\n            print(f\"  Processing depth {depth}...\")\n\n            hit_rates = []\n            af_tested = []\n\n            for af in af_values:\n                # Run detection experiments at this AF/depth\n                detection_count = 0\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(self.config.seed + rep * 1000 + int(af * 1e6))\n\n                    # Create config for this AF/depth\n                    test_config = self._create_lod_config(af, depth)\n\n                    # Run pipeline\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Check if variant was detected\n                    if len(calls_df[calls_df['variant_call'] == True]) &gt; 0:\n                        detection_count += 1\n\n                hit_rate = detection_count / n_replicates\n                hit_rates.append(hit_rate)\n                af_tested.append(af)\n\n            # Fit logistic curve to hit rate vs AF\n            lod_af = self._fit_detection_curve(af_tested, hit_rates, target_detection_rate)\n\n            # Compute confidence intervals using bootstrap\n            lod_ci = self._bootstrap_lod_ci(af_tested, hit_rates, target_detection_rate, \n                                         n_bootstrap=200)\n\n            lod_results[depth] = {\n                'lod_af': float(lod_af),\n                'lod_ci_lower': float(lod_ci[0]),\n                'lod_ci_upper': float(lod_ci[1]),\n                'af_values': [float(x) for x in af_tested],\n                'hit_rates': hit_rates,\n                'target_detection_rate': target_detection_rate,\n                'n_replicates': n_replicates\n            }\n\n            print(f\"    LoD at depth {depth}: {lod_af:.2e} AF [{lod_ci[0]:.2e}, {lod_ci[1]:.2e}]\")\n\n        self.lod_results = {\n            'depth_results': lod_results,\n            'af_range': af_range,\n            'target_detection_rate': target_detection_rate,\n            'alpha': alpha,\n            'beta': beta,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.lod_results\n\n    def estimate_loq(self,\n                     af_range: Tuple[float, float] = (1e-4, 1e-2), \n                     depth_values: List[int] = [1000, 5000, 10000],\n                     n_replicates: int = 50,\n                     cv_threshold: float = 0.20,\n                     abs_error_threshold: Optional[float] = None) -&gt; Dict[str, Any]:\n        \"\"\"Estimate Limit of Quantification (LoQ).\n\n        LoQ is the lowest concentration at which quantitative measurements can be made\n        with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).\n\n        Args:\n            af_range: Range of allele fractions to test\n            depth_values: UMI depths to test  \n            n_replicates: Number of replicates per AF/depth combination\n            cv_threshold: Maximum acceptable coefficient of variation\n            abs_error_threshold: Maximum acceptable absolute error (optional)\n\n        Returns:\n            Dictionary with LoQ results for each depth\n        \"\"\"\n        print(f\"Estimating LoQ with CV threshold {cv_threshold:.1%}...\")\n\n        af_min, af_max = af_range\n        af_values = np.logspace(np.log10(af_min), np.log10(af_max), 12)\n\n        loq_results = {}\n\n        for depth in depth_values:\n            print(f\"  Processing depth {depth}...\")\n\n            cv_values = []\n            abs_errors = []\n            af_tested = []\n\n            for af in af_values:\n                estimated_afs = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(self.config.seed + rep * 2000 + int(af * 1e6))\n\n                    # Create config for this AF/depth\n                    test_config = self._create_lod_config(af, depth)\n\n                    # Run pipeline and estimate AF\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Estimate AF from variant calls\n                    if len(calls_df) &gt; 0:\n                        # Simple AF estimation: variants detected / total UMIs\n                        estimated_af = len(calls_df[calls_df['variant_call'] == True]) / len(calls_df)\n                        estimated_afs.append(estimated_af)\n                    else:\n                        estimated_afs.append(0.0)\n\n                if len(estimated_afs) &gt; 1:\n                    mean_af = np.mean(estimated_afs)\n                    std_af = np.std(estimated_afs)\n                    cv = std_af / mean_af if mean_af &gt; 0 else np.inf\n                    abs_error = abs(mean_af - af)\n\n                    cv_values.append(cv)\n                    abs_errors.append(abs_error)\n                    af_tested.append(af)\n\n            # Find LoQ as lowest AF meeting precision criteria\n            loq_af_cv = None\n            loq_af_abs = None\n\n            # CV-based LoQ\n            for i, cv in enumerate(cv_values):\n                if cv &lt;= cv_threshold:\n                    loq_af_cv = af_tested[i]\n                    break\n\n            # Absolute error-based LoQ (if threshold provided)\n            if abs_error_threshold is not None:\n                for i, abs_err in enumerate(abs_errors):\n                    if abs_err &lt;= abs_error_threshold:\n                        loq_af_abs = af_tested[i]\n                        break\n\n            loq_results[depth] = {\n                'loq_af_cv': float(loq_af_cv) if loq_af_cv is not None else None,\n                'loq_af_abs_error': float(loq_af_abs) if loq_af_abs is not None else None,\n                'af_values': [float(x) for x in af_tested],\n                'cv_values': cv_values,\n                'abs_errors': abs_errors,\n                'cv_threshold': cv_threshold,\n                'abs_error_threshold': abs_error_threshold,\n                'n_replicates': n_replicates\n            }\n\n            cv_str = f\"{loq_af_cv:.2e}\" if loq_af_cv else \"Not found\"\n            print(f\"    LoQ (CV) at depth {depth}: {cv_str} AF\")\n\n        self.loq_results = {\n            'depth_results': loq_results,\n            'af_range': af_range,\n            'cv_threshold': cv_threshold,\n            'abs_error_threshold': abs_error_threshold,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.loq_results\n\n    def generate_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate LoB/LoD/LoQ reports and visualizations.\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save LoB results\n        if self.lob_results:\n            lob_payload = dict(self.lob_results)\n            lob_payload[\"schema_version\"] = \"1.0.0\"\n            lob_path = output_path / \"lob.json\"\n            with open(lob_path, 'w') as f:\n                json.dump(lob_payload, f, indent=2)\n            print(f\"LoB results saved to {lob_path}\")\n\n        # Save LoD results\n        if self.lod_results:\n            self._save_lod_table(output_path)\n            self._plot_lod_curves(output_path)\n\n        # Save LoQ results  \n        if self.loq_results:\n            self._save_loq_table(output_path)\n\n    def _create_blank_config(self) -&gt; PipelineConfig:\n        \"\"\"Create configuration for blank (AF=0) experiments.\"\"\"\n        blank_config = PipelineConfig(\n            run_id=f\"{self.config.run_id}_blank\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[0.0],  # Blank samples\n                umi_depths=self.config.simulation.umi_depths[:1],  # Use first depth\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n        return blank_config\n\n    def _create_lod_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for LoD/LoQ experiments.\"\"\"\n        lod_config = PipelineConfig(\n            run_id=f\"{self.config.run_id}_lod\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n        return lod_config\n\n    def _fit_detection_curve(self, af_values: List[float], hit_rates: List[float], \n                           target_rate: float = 0.95) -&gt; float:\n        \"\"\"Fit logistic curve to detection data and find LoD.\"\"\"\n        # Convert to log scale for fitting\n        log_af = np.log10(af_values)\n\n        # Fit logistic regression\n        from scipy.optimize import curve_fit\n\n        def logistic(x, a, b):\n            return 1 / (1 + np.exp(-(a * x + b)))\n\n        try:\n            popt, _ = curve_fit(logistic, log_af, hit_rates, maxfev=2000)\n            a, b = popt\n\n            # Solve for AF giving target detection rate\n            # target_rate = 1 / (1 + exp(-(a * log_af + b)))\n            # Solving: log_af = (logit(target_rate) - b) / a\n            logit_target = np.log(target_rate / (1 - target_rate))\n            log_af_lod = (logit_target - b) / a\n\n            return 10 ** log_af_lod\n\n        except:\n            # Fallback: linear interpolation\n            from scipy.interpolate import interp1d\n            if len(af_values) &gt; 1:\n                interp_func = interp1d(hit_rates, af_values, bounds_error=False, \n                                     fill_value='extrapolate')\n                return float(interp_func(target_rate))\n            else:\n                return af_values[0]\n\n    def _bootstrap_lod_ci(self, af_values: List[float], hit_rates: List[float],\n                         target_rate: float, n_bootstrap: int = 200) -&gt; Tuple[float, float]:\n        \"\"\"Compute bootstrap confidence intervals for LoD.\"\"\"\n        bootstrap_lods = []\n\n        for _ in range(n_bootstrap):\n            # Bootstrap resample\n            indices = self.rng.choice(len(af_values), size=len(af_values), replace=True)\n            boot_af = [af_values[i] for i in indices]\n            boot_hit = [hit_rates[i] for i in indices]\n\n            try:\n                boot_lod = self._fit_detection_curve(boot_af, boot_hit, target_rate)\n                bootstrap_lods.append(boot_lod)\n            except:\n                continue\n\n        if bootstrap_lods:\n            ci_lower = np.percentile(bootstrap_lods, 2.5)\n            ci_upper = np.percentile(bootstrap_lods, 97.5)\n            return ci_lower, ci_upper\n        else:\n            # Fallback\n            return af_values[0], af_values[-1]\n\n    def _save_lod_table(self, output_path: Path) -&gt; None:\n        \"\"\"Save LoD results table as CSV.\"\"\"\n        if not self.lod_results:\n            return\n\n        lod_data = []\n        for depth, results in self.lod_results['depth_results'].items():\n            lod_data.append({\n                'depth': depth,\n                'lod_af': results['lod_af'],\n                'lod_ci_lower': results['lod_ci_lower'],\n                'lod_ci_upper': results['lod_ci_upper'],\n                'target_detection_rate': results['target_detection_rate'],\n                'n_replicates': results['n_replicates']\n            })\n\n        lod_df = pd.DataFrame(lod_data)\n        lod_path = output_path / \"lod_table.csv\"\n        lod_df.to_csv(lod_path, index=False)\n        print(f\"LoD table saved to {lod_path}\")\n\n    def _save_loq_table(self, output_path: Path) -&gt; None:\n        \"\"\"Save LoQ results table as CSV.\"\"\"\n        if not self.loq_results:\n            return\n\n        loq_data = []\n        for depth, results in self.loq_results['depth_results'].items():\n            loq_data.append({\n                'depth': depth,\n                'loq_af_cv': results['loq_af_cv'],\n                'loq_af_abs_error': results['loq_af_abs_error'],\n                'cv_threshold': results['cv_threshold'],\n                'abs_error_threshold': results['abs_error_threshold'],\n                'n_replicates': results['n_replicates']\n            })\n\n        loq_df = pd.DataFrame(loq_data)\n        loq_path = output_path / \"loq_table.csv\"\n        loq_df.to_csv(loq_path, index=False)\n        print(f\"LoQ table saved to {loq_path}\")\n\n    def _plot_lod_curves(self, output_path: Path) -&gt; None:\n        \"\"\"Generate LoD detection curves plot.\"\"\"\n        if not self.lod_results:\n            return\n\n        plt.style.use('default')\n        fig, axes = plt.subplots(1, len(self.lod_results['depth_results']), \n                                figsize=(5 * len(self.lod_results['depth_results']), 4))\n\n        if len(self.lod_results['depth_results']) == 1:\n            axes = [axes]\n\n        for i, (depth, results) in enumerate(self.lod_results['depth_results'].items()):\n            ax = axes[i]\n\n            af_values = results['af_values']\n            hit_rates = results['hit_rates']\n            lod_af = results['lod_af']\n            target_rate = results['target_detection_rate']\n\n            # Plot detection curve\n            ax.semilogx(af_values, hit_rates, 'o-', linewidth=2, markersize=6,\n                       label=f'Observed (n={results[\"n_replicates\"]})')\n\n            # Mark LoD\n            ax.axvline(lod_af, color='red', linestyle='--', linewidth=2,\n                      label=f'LoD = {lod_af:.2e}')\n            ax.axhline(target_rate, color='gray', linestyle=':', alpha=0.7,\n                      label=f'{target_rate:.0%} Detection')\n\n            ax.set_xlabel('Allele Fraction')\n            ax.set_ylabel('Detection Rate')\n            ax.set_title(f'LoD Curve (Depth = {depth})')\n            ax.grid(True, alpha=0.3)\n            ax.legend()\n            ax.set_ylim(-0.05, 1.05)\n\n        plt.tight_layout()\n        lod_plot_path = output_path / \"lod_curves.png\"\n        plt.savefig(lod_plot_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"LoD curves saved to {lod_plot_path}\")\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.LODAnalyzer.estimate_lob","title":"<code>estimate_lob(n_blank_runs=100)</code>","text":"<p>Estimate Limit of Blank (LoB).</p> <p>LoB represents the highest measurement result that is likely to be observed  for a blank specimen. Calculated as the 95th percentile of blank measurements.</p> <p>Parameters:</p> Name Type Description Default <code>n_blank_runs</code> <code>int</code> <p>Number of blank (AF=0) simulations to run</p> <code>100</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoB results including:</p> <code>Dict[str, Any]</code> <ul> <li>lob_value: 95th percentile of blank test statistics</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_mean: Mean of blank measurements</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_std: Standard deviation of blank measurements</li> </ul> <code>Dict[str, Any]</code> <ul> <li>blank_measurements: All blank measurement values</li> </ul> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lob(self, n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Blank (LoB).\n\n    LoB represents the highest measurement result that is likely to be observed \n    for a blank specimen. Calculated as the 95th percentile of blank measurements.\n\n    Args:\n        n_blank_runs: Number of blank (AF=0) simulations to run\n\n    Returns:\n        Dictionary with LoB results including:\n        - lob_value: 95th percentile of blank test statistics\n        - blank_mean: Mean of blank measurements\n        - blank_std: Standard deviation of blank measurements\n        - blank_measurements: All blank measurement values\n    \"\"\"\n    print(f\"Estimating LoB with {n_blank_runs} blank runs...\")\n\n    # Create blank configuration (AF = 0)\n    blank_config = self._create_blank_config()\n    blank_measurements = []\n\n    for i in range(n_blank_runs):\n        # Use different random seed for each run\n        run_rng = np.random.default_rng(self.config.seed + i)\n\n        # Simulate blank reads (no true variants)\n        reads_df = simulate_reads(blank_config, run_rng)\n\n        # Process through pipeline to get test statistic\n        collapsed_df = collapse_umis(reads_df, blank_config, run_rng)\n        error_model = fit_error_model(collapsed_df, blank_config, run_rng)\n        calls_df = call_mrd(collapsed_df, error_model, blank_config, run_rng)\n\n        # Extract test statistic (number of variant calls in blank)\n        n_variant_calls = len(calls_df[calls_df['variant_call'] == True])\n        blank_measurements.append(n_variant_calls)\n\n    blank_measurements = np.array(blank_measurements)\n\n    # Compute LoB as 95th percentile\n    lob_value = np.percentile(blank_measurements, 95)\n    blank_mean = np.mean(blank_measurements)\n    blank_std = np.std(blank_measurements)\n\n    self.lob_results = {\n        'lob_value': float(lob_value),\n        'blank_mean': float(blank_mean),\n        'blank_std': float(blank_std),\n        'blank_measurements': blank_measurements.tolist(),\n        'n_blank_runs': n_blank_runs,\n        'percentile': 95,\n        'config_hash': self.config.config_hash()\n    }\n\n    print(f\"LoB estimated: {lob_value:.3f} (mean: {blank_mean:.3f}, std: {blank_std:.3f})\")\n    return self.lob_results\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.LODAnalyzer.estimate_lod","title":"<code>estimate_lod(af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, target_detection_rate=0.95, alpha=0.05, beta=0.05)</code>","text":"<p>Estimate Limit of Detection (LoD).</p> <p>LoD is the lowest analyte concentration likely to be reliably detected. Calculated as the concentration yielding 95% detection probability.</p> <p>Parameters:</p> Name Type Description Default <code>af_range</code> <code>Tuple[float, float]</code> <p>Range of allele fractions to test (min, max)</p> <code>(0.0001, 0.01)</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per AF/depth combination</p> <code>50</code> <code>target_detection_rate</code> <code>float</code> <p>Target detection rate (e.g., 0.95 for 95%)</p> <code>0.95</code> <code>alpha</code> <code>float</code> <p>Type I error rate</p> <code>0.05</code> <code>beta</code> <code>float</code> <p>Type II error rate</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoD results for each depth</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lod(self, \n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 target_detection_rate: float = 0.95,\n                 alpha: float = 0.05,\n                 beta: float = 0.05) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Detection (LoD).\n\n    LoD is the lowest analyte concentration likely to be reliably detected.\n    Calculated as the concentration yielding 95% detection probability.\n\n    Args:\n        af_range: Range of allele fractions to test (min, max)\n        depth_values: UMI depths to test\n        n_replicates: Number of replicates per AF/depth combination\n        target_detection_rate: Target detection rate (e.g., 0.95 for 95%)\n        alpha: Type I error rate\n        beta: Type II error rate\n\n    Returns:\n        Dictionary with LoD results for each depth\n    \"\"\"\n    print(f\"Estimating LoD across AF range {af_range} at depths {depth_values}...\")\n\n    # Generate AF grid (log-spaced)\n    af_min, af_max = af_range\n    af_values = np.logspace(np.log10(af_min), np.log10(af_max), 15)\n\n    lod_results = {}\n\n    for depth in depth_values:\n        print(f\"  Processing depth {depth}...\")\n\n        hit_rates = []\n        af_tested = []\n\n        for af in af_values:\n            # Run detection experiments at this AF/depth\n            detection_count = 0\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(self.config.seed + rep * 1000 + int(af * 1e6))\n\n                # Create config for this AF/depth\n                test_config = self._create_lod_config(af, depth)\n\n                # Run pipeline\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Check if variant was detected\n                if len(calls_df[calls_df['variant_call'] == True]) &gt; 0:\n                    detection_count += 1\n\n            hit_rate = detection_count / n_replicates\n            hit_rates.append(hit_rate)\n            af_tested.append(af)\n\n        # Fit logistic curve to hit rate vs AF\n        lod_af = self._fit_detection_curve(af_tested, hit_rates, target_detection_rate)\n\n        # Compute confidence intervals using bootstrap\n        lod_ci = self._bootstrap_lod_ci(af_tested, hit_rates, target_detection_rate, \n                                     n_bootstrap=200)\n\n        lod_results[depth] = {\n            'lod_af': float(lod_af),\n            'lod_ci_lower': float(lod_ci[0]),\n            'lod_ci_upper': float(lod_ci[1]),\n            'af_values': [float(x) for x in af_tested],\n            'hit_rates': hit_rates,\n            'target_detection_rate': target_detection_rate,\n            'n_replicates': n_replicates\n        }\n\n        print(f\"    LoD at depth {depth}: {lod_af:.2e} AF [{lod_ci[0]:.2e}, {lod_ci[1]:.2e}]\")\n\n    self.lod_results = {\n        'depth_results': lod_results,\n        'af_range': af_range,\n        'target_detection_rate': target_detection_rate,\n        'alpha': alpha,\n        'beta': beta,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.lod_results\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.LODAnalyzer.estimate_loq","title":"<code>estimate_loq(af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, cv_threshold=0.2, abs_error_threshold=None)</code>","text":"<p>Estimate Limit of Quantification (LoQ).</p> <p>LoQ is the lowest concentration at which quantitative measurements can be made with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).</p> <p>Parameters:</p> Name Type Description Default <code>af_range</code> <code>Tuple[float, float]</code> <p>Range of allele fractions to test</p> <code>(0.0001, 0.01)</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test  </p> <code>[1000, 5000, 10000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per AF/depth combination</p> <code>50</code> <code>cv_threshold</code> <code>float</code> <p>Maximum acceptable coefficient of variation</p> <code>0.2</code> <code>abs_error_threshold</code> <code>Optional[float]</code> <p>Maximum acceptable absolute error (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with LoQ results for each depth</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_loq(self,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2), \n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 cv_threshold: float = 0.20,\n                 abs_error_threshold: Optional[float] = None) -&gt; Dict[str, Any]:\n    \"\"\"Estimate Limit of Quantification (LoQ).\n\n    LoQ is the lowest concentration at which quantitative measurements can be made\n    with acceptable precision (typically CV \u2264 20% or absolute error \u2264 threshold).\n\n    Args:\n        af_range: Range of allele fractions to test\n        depth_values: UMI depths to test  \n        n_replicates: Number of replicates per AF/depth combination\n        cv_threshold: Maximum acceptable coefficient of variation\n        abs_error_threshold: Maximum acceptable absolute error (optional)\n\n    Returns:\n        Dictionary with LoQ results for each depth\n    \"\"\"\n    print(f\"Estimating LoQ with CV threshold {cv_threshold:.1%}...\")\n\n    af_min, af_max = af_range\n    af_values = np.logspace(np.log10(af_min), np.log10(af_max), 12)\n\n    loq_results = {}\n\n    for depth in depth_values:\n        print(f\"  Processing depth {depth}...\")\n\n        cv_values = []\n        abs_errors = []\n        af_tested = []\n\n        for af in af_values:\n            estimated_afs = []\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(self.config.seed + rep * 2000 + int(af * 1e6))\n\n                # Create config for this AF/depth\n                test_config = self._create_lod_config(af, depth)\n\n                # Run pipeline and estimate AF\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Estimate AF from variant calls\n                if len(calls_df) &gt; 0:\n                    # Simple AF estimation: variants detected / total UMIs\n                    estimated_af = len(calls_df[calls_df['variant_call'] == True]) / len(calls_df)\n                    estimated_afs.append(estimated_af)\n                else:\n                    estimated_afs.append(0.0)\n\n            if len(estimated_afs) &gt; 1:\n                mean_af = np.mean(estimated_afs)\n                std_af = np.std(estimated_afs)\n                cv = std_af / mean_af if mean_af &gt; 0 else np.inf\n                abs_error = abs(mean_af - af)\n\n                cv_values.append(cv)\n                abs_errors.append(abs_error)\n                af_tested.append(af)\n\n        # Find LoQ as lowest AF meeting precision criteria\n        loq_af_cv = None\n        loq_af_abs = None\n\n        # CV-based LoQ\n        for i, cv in enumerate(cv_values):\n            if cv &lt;= cv_threshold:\n                loq_af_cv = af_tested[i]\n                break\n\n        # Absolute error-based LoQ (if threshold provided)\n        if abs_error_threshold is not None:\n            for i, abs_err in enumerate(abs_errors):\n                if abs_err &lt;= abs_error_threshold:\n                    loq_af_abs = af_tested[i]\n                    break\n\n        loq_results[depth] = {\n            'loq_af_cv': float(loq_af_cv) if loq_af_cv is not None else None,\n            'loq_af_abs_error': float(loq_af_abs) if loq_af_abs is not None else None,\n            'af_values': [float(x) for x in af_tested],\n            'cv_values': cv_values,\n            'abs_errors': abs_errors,\n            'cv_threshold': cv_threshold,\n            'abs_error_threshold': abs_error_threshold,\n            'n_replicates': n_replicates\n        }\n\n        cv_str = f\"{loq_af_cv:.2e}\" if loq_af_cv else \"Not found\"\n        print(f\"    LoQ (CV) at depth {depth}: {cv_str} AF\")\n\n    self.loq_results = {\n        'depth_results': loq_results,\n        'af_range': af_range,\n        'cv_threshold': cv_threshold,\n        'abs_error_threshold': abs_error_threshold,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.loq_results\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.LODAnalyzer.generate_reports","title":"<code>generate_reports(output_dir='reports')</code>","text":"<p>Generate LoB/LoD/LoQ reports and visualizations.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def generate_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate LoB/LoD/LoQ reports and visualizations.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save LoB results\n    if self.lob_results:\n        lob_payload = dict(self.lob_results)\n        lob_payload[\"schema_version\"] = \"1.0.0\"\n        lob_path = output_path / \"lob.json\"\n        with open(lob_path, 'w') as f:\n            json.dump(lob_payload, f, indent=2)\n        print(f\"LoB results saved to {lob_path}\")\n\n    # Save LoD results\n    if self.lod_results:\n        self._save_lod_table(output_path)\n        self._plot_lod_curves(output_path)\n\n    # Save LoQ results  \n    if self.loq_results:\n        self._save_loq_table(output_path)\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.estimate_lob","title":"<code>precise_mrd.eval.lod.estimate_lob(config, rng, n_blank_runs=100)</code>","text":"<p>Convenience function to estimate LoB.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lob(config: PipelineConfig, rng: np.random.Generator, \n                 n_blank_runs: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoB.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_lob(n_blank_runs)\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.estimate_lod","title":"<code>precise_mrd.eval.lod.estimate_lod(config, rng, af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50)</code>","text":"<p>Convenience function to estimate LoD.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_lod(config: PipelineConfig, rng: np.random.Generator,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoD.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_lod(af_range, depth_values, n_replicates)\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.lod.estimate_loq","title":"<code>precise_mrd.eval.lod.estimate_loq(config, rng, af_range=(0.0001, 0.01), depth_values=[1000, 5000, 10000], n_replicates=50, cv_threshold=0.2)</code>","text":"<p>Convenience function to estimate LoQ.</p> Source code in <code>src/precise_mrd/eval/lod.py</code> <pre><code>def estimate_loq(config: PipelineConfig, rng: np.random.Generator,\n                 af_range: Tuple[float, float] = (1e-4, 1e-2),\n                 depth_values: List[int] = [1000, 5000, 10000],\n                 n_replicates: int = 50,\n                 cv_threshold: float = 0.20) -&gt; Dict[str, Any]:\n    \"\"\"Convenience function to estimate LoQ.\"\"\"\n    analyzer = LODAnalyzer(config, rng)\n    return analyzer.estimate_loq(af_range, depth_values, n_replicates, cv_threshold)\n</code></pre>"},{"location":"api-reference/#stratified-analysis","title":"Stratified Analysis","text":""},{"location":"api-reference/#precise_mrd.eval.stratified.StratifiedAnalyzer","title":"<code>precise_mrd.eval.stratified.StratifiedAnalyzer</code>","text":"<p>Analyzer for stratified power and calibration analysis.</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>class StratifiedAnalyzer:\n    \"\"\"Analyzer for stratified power and calibration analysis.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.power_results: Optional[Dict[str, Any]] = None\n        self.calibration_results: Optional[Dict[str, Any]] = None\n\n    def analyze_stratified_power(self,\n                               af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                               depth_values: List[int] = [1000, 5000, 10000],\n                               contexts: List[str] = ['CpG', 'CHG', 'CHH', 'NpN'],\n                               n_replicates: int = 50) -&gt; Dict[str, Any]:\n        \"\"\"Analyze detection power stratified by trinucleotide context and depth.\n\n        Args:\n            af_values: Allele fractions to test\n            depth_values: UMI depths to test\n            contexts: Trinucleotide contexts to stratify by\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with stratified power analysis results\n        \"\"\"\n        print(\"Running stratified power analysis...\")\n\n        power_results = {}\n\n        for context in contexts:\n            print(f\"  Analyzing context: {context}\")\n            power_results[context] = {}\n\n            for depth in depth_values:\n                power_results[context][depth] = {}\n\n                for af in af_values:\n                    detection_rates = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 1000 + hash(context) % 1000\n                        )\n\n                        # Create context-specific config\n                        context_config = self._create_context_config(af, depth, context)\n\n                        # Run pipeline with context tagging\n                        reads_df = self._simulate_with_context(context_config, run_rng, context)\n                        collapsed_df = collapse_umis(reads_df, context_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, context_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, context_config, run_rng)\n\n                        # Calculate detection rate for this context\n                        if len(calls_df) &gt; 0:\n                            context_calls = calls_df[calls_df.get('context', 'NpN') == context]\n                            detection_rate = len(context_calls[context_calls['variant_call'] == True]) / max(1, len(context_calls))\n                        else:\n                            detection_rate = 0.0\n\n                        detection_rates.append(detection_rate)\n\n                    power_results[context][depth][af] = {\n                        'mean_detection_rate': float(np.mean(detection_rates)),\n                        'std_detection_rate': float(np.std(detection_rates)),\n                        'detection_rates': detection_rates,\n                        'n_replicates': n_replicates\n                    }\n\n                    print(f\"    {context} @ depth={depth}, AF={af:.0e}: {np.mean(detection_rates):.3f} \u00b1 {np.std(detection_rates):.3f}\")\n\n        self.power_results = {\n            'stratified_results': power_results,\n            'af_values': af_values,\n            'depth_values': depth_values,\n            'contexts': contexts,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.power_results\n\n    def analyze_calibration_by_bins(self,\n                                  af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                                  depth_values: List[int] = [1000, 5000, 10000],\n                                  n_bins: int = 10,\n                                  n_replicates: int = 100) -&gt; Dict[str, Any]:\n        \"\"\"Analyze calibration stratified by AF and depth bins.\n\n        Args:\n            af_values: Allele fractions to test\n            depth_values: UMI depths to test\n            n_bins: Number of calibration bins\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with binned calibration results\n        \"\"\"\n        print(f\"Running calibration analysis with {n_bins} bins...\")\n\n        calibration_data = []\n\n        for depth in depth_values:\n            print(f\"  Processing depth: {depth}\")\n\n            for af in af_values:\n                predicted_probs = []\n                true_labels = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(\n                        self.config.seed + rep * 2000 + int(af * 1e6) + depth\n                    )\n\n                    # Create test config\n                    test_config = self._create_calibration_config(af, depth)\n\n                    # Run pipeline\n                    reads_df = simulate_reads(test_config, run_rng)\n                    collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                    # Extract predicted probabilities and true labels\n                    if len(calls_df) &gt; 0:\n                        # Use p-value as proxy for predicted probability (inverted)\n                        prob_scores = 1 - calls_df.get('p_value', self.rng.uniform(0, 1, len(calls_df)))\n                        true_positives = calls_df.get('variant_call', False)\n\n                        predicted_probs.extend(prob_scores.tolist())\n                        true_labels.extend(true_positives.tolist())\n\n                if len(predicted_probs) &gt; 0:\n                    # Compute calibration metrics\n                    calibration_metrics = self._compute_calibration_metrics(\n                        np.array(predicted_probs), np.array(true_labels), n_bins\n                    )\n\n                    calibration_data.append({\n                        'depth': depth,\n                        'af': af,\n                        'ece': calibration_metrics['ece'],\n                        'max_ce': calibration_metrics['max_ce'],\n                        'bin_accuracies': calibration_metrics['bin_accuracies'],\n                        'bin_confidences': calibration_metrics['bin_confidences'],\n                        'bin_counts': calibration_metrics['bin_counts'],\n                        'n_samples': len(predicted_probs)\n                    })\n\n        self.calibration_results = {\n            'calibration_data': calibration_data,\n            'af_values': af_values,\n            'depth_values': depth_values,\n            'n_bins': n_bins,\n            'config_hash': self.config.config_hash()\n        }\n\n        return self.calibration_results\n\n    def _simulate_with_context(self, config: PipelineConfig, \n                              rng: np.random.Generator,\n                              context: str) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with trinucleotide context tagging.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        # Add context-specific error modeling\n        context_error_rates = {\n            'CpG': 1.5,    # Higher error in CpG contexts\n            'CHG': 1.2,    # Moderate error in CHG\n            'CHH': 1.0,    # Baseline error in CHH\n            'NpN': 0.8     # Lower error in other contexts\n        }\n\n        error_multiplier = context_error_rates.get(context, 1.0)\n        reads_df['background_rate'] *= error_multiplier\n        reads_df['context'] = context\n\n        # Adjust false positive counts based on context\n        reads_df['n_false_positives'] = rng.binomial(\n            reads_df['n_families'] - reads_df['n_true_variants'],\n            reads_df['background_rate'] * error_multiplier\n        )\n\n        return reads_df\n\n    def _create_context_config(self, af: float, depth: int, context: str) -&gt; PipelineConfig:\n        \"\"\"Create configuration for context-specific analysis.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_context_{context}\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _create_calibration_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for calibration analysis.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_calibration\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _compute_calibration_metrics(self, predicted_probs: np.ndarray, \n                                   true_labels: np.ndarray, \n                                   n_bins: int) -&gt; Dict[str, Any]:\n        \"\"\"Compute calibration metrics including ECE and binned accuracy.\"\"\"\n        # Create bins\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        bin_lowers = bin_boundaries[:-1]\n        bin_uppers = bin_boundaries[1:]\n\n        bin_accuracies = []\n        bin_confidences = []\n        bin_counts = []\n\n        ece = 0.0\n        max_ce = 0.0\n\n        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n            # Find predictions in this bin\n            in_bin = (predicted_probs &gt; bin_lower) &amp; (predicted_probs &lt;= bin_upper)\n            prop_in_bin = in_bin.mean()\n\n            if prop_in_bin &gt; 0:\n                # Accuracy in this bin\n                accuracy_in_bin = true_labels[in_bin].mean()\n                avg_confidence_in_bin = predicted_probs[in_bin].mean()\n\n                # Calibration error in this bin\n                bin_ce = abs(avg_confidence_in_bin - accuracy_in_bin)\n                ece += bin_ce * prop_in_bin\n                max_ce = max(max_ce, bin_ce)\n\n                bin_accuracies.append(accuracy_in_bin)\n                bin_confidences.append(avg_confidence_in_bin)\n                bin_counts.append(in_bin.sum())\n            else:\n                bin_accuracies.append(0.0)\n                bin_confidences.append(0.0)\n                bin_counts.append(0)\n\n        return {\n            'ece': float(ece),\n            'max_ce': float(max_ce),\n            'bin_accuracies': bin_accuracies,\n            'bin_confidences': bin_confidences,\n            'bin_counts': bin_counts\n        }\n\n    def generate_stratified_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate stratified analysis reports.\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save power analysis results\n        if self.power_results:\n            power_payload = dict(self.power_results)\n            power_payload[\"schema_version\"] = \"1.0.0\"\n            power_path = output_path / \"power_by_stratum.json\"\n            with open(power_path, 'w') as f:\n                json.dump(power_payload, f, indent=2)\n            print(f\"Stratified power results saved to {power_path}\")\n\n        # Save calibration results\n        if self.calibration_results:\n            # Save as CSV for easier analysis\n            calib_df = pd.DataFrame(self.calibration_results['calibration_data'])\n            calib_path = output_path / \"calibration_by_bin.csv\"\n            calib_df.to_csv(calib_path, index=False)\n            print(f\"Calibration by bin results saved to {calib_path}\")\n\n            # Also save as JSON\n            calibration_payload = dict(self.calibration_results)\n            calibration_payload[\"schema_version\"] = \"1.0.0\"\n            calib_json_path = output_path / \"calibration_by_bin.json\"\n            with open(calib_json_path, 'w') as f:\n                json.dump(calibration_payload, f, indent=2)\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.stratified.StratifiedAnalyzer.analyze_stratified_power","title":"<code>analyze_stratified_power(af_values=[0.001, 0.005, 0.01, 0.05], depth_values=[1000, 5000, 10000], contexts=['CpG', 'CHG', 'CHH', 'NpN'], n_replicates=50)</code>","text":"<p>Analyze detection power stratified by trinucleotide context and depth.</p> <p>Parameters:</p> Name Type Description Default <code>af_values</code> <code>List[float]</code> <p>Allele fractions to test</p> <code>[0.001, 0.005, 0.01, 0.05]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>contexts</code> <code>List[str]</code> <p>Trinucleotide contexts to stratify by</p> <code>['CpG', 'CHG', 'CHH', 'NpN']</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>50</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with stratified power analysis results</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def analyze_stratified_power(self,\n                           af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                           depth_values: List[int] = [1000, 5000, 10000],\n                           contexts: List[str] = ['CpG', 'CHG', 'CHH', 'NpN'],\n                           n_replicates: int = 50) -&gt; Dict[str, Any]:\n    \"\"\"Analyze detection power stratified by trinucleotide context and depth.\n\n    Args:\n        af_values: Allele fractions to test\n        depth_values: UMI depths to test\n        contexts: Trinucleotide contexts to stratify by\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with stratified power analysis results\n    \"\"\"\n    print(\"Running stratified power analysis...\")\n\n    power_results = {}\n\n    for context in contexts:\n        print(f\"  Analyzing context: {context}\")\n        power_results[context] = {}\n\n        for depth in depth_values:\n            power_results[context][depth] = {}\n\n            for af in af_values:\n                detection_rates = []\n\n                for rep in range(n_replicates):\n                    run_rng = np.random.default_rng(\n                        self.config.seed + rep * 1000 + hash(context) % 1000\n                    )\n\n                    # Create context-specific config\n                    context_config = self._create_context_config(af, depth, context)\n\n                    # Run pipeline with context tagging\n                    reads_df = self._simulate_with_context(context_config, run_rng, context)\n                    collapsed_df = collapse_umis(reads_df, context_config, run_rng)\n                    error_model = fit_error_model(collapsed_df, context_config, run_rng)\n                    calls_df = call_mrd(collapsed_df, error_model, context_config, run_rng)\n\n                    # Calculate detection rate for this context\n                    if len(calls_df) &gt; 0:\n                        context_calls = calls_df[calls_df.get('context', 'NpN') == context]\n                        detection_rate = len(context_calls[context_calls['variant_call'] == True]) / max(1, len(context_calls))\n                    else:\n                        detection_rate = 0.0\n\n                    detection_rates.append(detection_rate)\n\n                power_results[context][depth][af] = {\n                    'mean_detection_rate': float(np.mean(detection_rates)),\n                    'std_detection_rate': float(np.std(detection_rates)),\n                    'detection_rates': detection_rates,\n                    'n_replicates': n_replicates\n                }\n\n                print(f\"    {context} @ depth={depth}, AF={af:.0e}: {np.mean(detection_rates):.3f} \u00b1 {np.std(detection_rates):.3f}\")\n\n    self.power_results = {\n        'stratified_results': power_results,\n        'af_values': af_values,\n        'depth_values': depth_values,\n        'contexts': contexts,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.power_results\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.stratified.StratifiedAnalyzer.analyze_calibration_by_bins","title":"<code>analyze_calibration_by_bins(af_values=[0.001, 0.005, 0.01, 0.05], depth_values=[1000, 5000, 10000], n_bins=10, n_replicates=100)</code>","text":"<p>Analyze calibration stratified by AF and depth bins.</p> <p>Parameters:</p> Name Type Description Default <code>af_values</code> <code>List[float]</code> <p>Allele fractions to test</p> <code>[0.001, 0.005, 0.01, 0.05]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000, 10000]</code> <code>n_bins</code> <code>int</code> <p>Number of calibration bins</p> <code>10</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>100</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with binned calibration results</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def analyze_calibration_by_bins(self,\n                              af_values: List[float] = [0.001, 0.005, 0.01, 0.05],\n                              depth_values: List[int] = [1000, 5000, 10000],\n                              n_bins: int = 10,\n                              n_replicates: int = 100) -&gt; Dict[str, Any]:\n    \"\"\"Analyze calibration stratified by AF and depth bins.\n\n    Args:\n        af_values: Allele fractions to test\n        depth_values: UMI depths to test\n        n_bins: Number of calibration bins\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with binned calibration results\n    \"\"\"\n    print(f\"Running calibration analysis with {n_bins} bins...\")\n\n    calibration_data = []\n\n    for depth in depth_values:\n        print(f\"  Processing depth: {depth}\")\n\n        for af in af_values:\n            predicted_probs = []\n            true_labels = []\n\n            for rep in range(n_replicates):\n                run_rng = np.random.default_rng(\n                    self.config.seed + rep * 2000 + int(af * 1e6) + depth\n                )\n\n                # Create test config\n                test_config = self._create_calibration_config(af, depth)\n\n                # Run pipeline\n                reads_df = simulate_reads(test_config, run_rng)\n                collapsed_df = collapse_umis(reads_df, test_config, run_rng)\n                error_model = fit_error_model(collapsed_df, test_config, run_rng)\n                calls_df = call_mrd(collapsed_df, error_model, test_config, run_rng)\n\n                # Extract predicted probabilities and true labels\n                if len(calls_df) &gt; 0:\n                    # Use p-value as proxy for predicted probability (inverted)\n                    prob_scores = 1 - calls_df.get('p_value', self.rng.uniform(0, 1, len(calls_df)))\n                    true_positives = calls_df.get('variant_call', False)\n\n                    predicted_probs.extend(prob_scores.tolist())\n                    true_labels.extend(true_positives.tolist())\n\n            if len(predicted_probs) &gt; 0:\n                # Compute calibration metrics\n                calibration_metrics = self._compute_calibration_metrics(\n                    np.array(predicted_probs), np.array(true_labels), n_bins\n                )\n\n                calibration_data.append({\n                    'depth': depth,\n                    'af': af,\n                    'ece': calibration_metrics['ece'],\n                    'max_ce': calibration_metrics['max_ce'],\n                    'bin_accuracies': calibration_metrics['bin_accuracies'],\n                    'bin_confidences': calibration_metrics['bin_confidences'],\n                    'bin_counts': calibration_metrics['bin_counts'],\n                    'n_samples': len(predicted_probs)\n                })\n\n    self.calibration_results = {\n        'calibration_data': calibration_data,\n        'af_values': af_values,\n        'depth_values': depth_values,\n        'n_bins': n_bins,\n        'config_hash': self.config.config_hash()\n    }\n\n    return self.calibration_results\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.stratified.StratifiedAnalyzer.generate_stratified_reports","title":"<code>generate_stratified_reports(output_dir='reports')</code>","text":"<p>Generate stratified analysis reports.</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def generate_stratified_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate stratified analysis reports.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save power analysis results\n    if self.power_results:\n        power_payload = dict(self.power_results)\n        power_payload[\"schema_version\"] = \"1.0.0\"\n        power_path = output_path / \"power_by_stratum.json\"\n        with open(power_path, 'w') as f:\n            json.dump(power_payload, f, indent=2)\n        print(f\"Stratified power results saved to {power_path}\")\n\n    # Save calibration results\n    if self.calibration_results:\n        # Save as CSV for easier analysis\n        calib_df = pd.DataFrame(self.calibration_results['calibration_data'])\n        calib_path = output_path / \"calibration_by_bin.csv\"\n        calib_df.to_csv(calib_path, index=False)\n        print(f\"Calibration by bin results saved to {calib_path}\")\n\n        # Also save as JSON\n        calibration_payload = dict(self.calibration_results)\n        calibration_payload[\"schema_version\"] = \"1.0.0\"\n        calib_json_path = output_path / \"calibration_by_bin.json\"\n        with open(calib_json_path, 'w') as f:\n            json.dump(calibration_payload, f, indent=2)\n</code></pre>"},{"location":"api-reference/#precise_mrd.eval.stratified.run_stratified_analysis","title":"<code>precise_mrd.eval.stratified.run_stratified_analysis(config, rng, output_dir='reports')</code>","text":"<p>Run complete stratified power and calibration analysis.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Random number generator</p> required <code>output_dir</code> <code>str</code> <p>Output directory for reports</p> <code>'reports'</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>Tuple of (power_results, calibration_results)</p> Source code in <code>src/precise_mrd/eval/stratified.py</code> <pre><code>def run_stratified_analysis(config: PipelineConfig,\n                          rng: np.random.Generator,\n                          output_dir: str = \"reports\") -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"Run complete stratified power and calibration analysis.\n\n    Args:\n        config: Pipeline configuration\n        rng: Random number generator\n        output_dir: Output directory for reports\n\n    Returns:\n        Tuple of (power_results, calibration_results)\n    \"\"\"\n    analyzer = StratifiedAnalyzer(config, rng)\n\n    # Run stratified power analysis\n    power_results = analyzer.analyze_stratified_power()\n\n    # Run calibration analysis\n    calibration_results = analyzer.analyze_calibration_by_bins()\n\n    # Generate reports\n    analyzer.generate_stratified_reports(output_dir)\n\n    return power_results, calibration_results\n</code></pre>"},{"location":"api-reference/#contamination-testing","title":"Contamination Testing","text":""},{"location":"api-reference/#precise_mrd.sim.contamination.ContaminationSimulator","title":"<code>precise_mrd.sim.contamination.ContaminationSimulator</code>","text":"<p>Simulator for contamination effects in ctDNA/UMI sequencing.</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>class ContaminationSimulator:\n    \"\"\"Simulator for contamination effects in ctDNA/UMI sequencing.\"\"\"\n\n    def __init__(self, config: PipelineConfig, rng: np.random.Generator):\n        self.config = config\n        self.rng = rng\n        self.contamination_results: Optional[Dict[str, Any]] = None\n\n    def simulate_contamination_effects(self,\n                                     hop_rates: List[float] = [0.0, 0.001, 0.002, 0.005, 0.01],\n                                     barcode_collision_rates: List[float] = [0.0, 0.0001, 0.0005, 0.001],\n                                     cross_sample_proportions: List[float] = [0.0, 0.01, 0.05, 0.1],\n                                     af_test_values: List[float] = [0.001, 0.005, 0.01],\n                                     depth_values: List[int] = [1000, 5000],\n                                     n_replicates: int = 20) -&gt; Dict[str, Any]:\n        \"\"\"Simulate contamination effects on variant detection.\n\n        Args:\n            hop_rates: Index-hopping rates to test\n            barcode_collision_rates: Barcode collision rates to test\n            cross_sample_proportions: Cross-sample contamination proportions\n            af_test_values: Allele fractions to test contamination effects on\n            depth_values: UMI depths to test\n            n_replicates: Number of replicates per condition\n\n        Returns:\n            Dictionary with contamination simulation results\n        \"\"\"\n        print(\"Running contamination stress testing...\")\n\n        results = {}\n        sensitivity_matrix = []\n\n        # Test index-hopping effects\n        print(\"  Testing index-hopping effects...\")\n        hop_results = self._test_index_hopping(hop_rates, af_test_values, \n                                              depth_values, n_replicates)\n        results['index_hopping'] = hop_results\n\n        # Test barcode collision effects\n        print(\"  Testing barcode collision effects...\")\n        collision_results = self._test_barcode_collisions(barcode_collision_rates,\n                                                         af_test_values, depth_values, \n                                                         n_replicates)\n        results['barcode_collisions'] = collision_results\n\n        # Test cross-sample contamination\n        print(\"  Testing cross-sample contamination...\")\n        cross_contam_results = self._test_cross_sample_contamination(\n            cross_sample_proportions, af_test_values, depth_values, n_replicates)\n        results['cross_sample_contamination'] = cross_contam_results\n\n        # Create sensitivity matrix for heatmap\n        sensitivity_matrix = self._create_sensitivity_matrix(results, af_test_values, \n                                                           depth_values)\n        results['sensitivity_matrix'] = sensitivity_matrix\n\n        self.contamination_results = results\n        return results\n\n    def _test_index_hopping(self, hop_rates: List[float], af_values: List[float],\n                           depth_values: List[int], n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test index-hopping contamination effects.\"\"\"\n        hop_results = {}\n\n        for hop_rate in hop_rates:\n            hop_results[hop_rate] = {}\n\n            for af in af_values:\n                hop_results[hop_rate][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n\n                    for rep in range(n_replicates):\n                        # Create contaminated simulation\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 1000 + int(hop_rate * 1e6)\n                        )\n\n                        # Simulate with index hopping\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_index_hopping(\n                            contam_config, run_rng, hop_rate\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate sensitivity (detection rate)\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        # Approximate expected detections (crude estimate)\n                        expected_detections = max(1, int(af * depth * 0.8))  # 80% pipeline efficiency\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n                        sensitivity_scores.append(sensitivity)\n\n                    hop_results[hop_rate][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'sensitivity_scores': sensitivity_scores,\n                        'n_replicates': n_replicates\n                    }\n\n        return hop_results\n\n    def _test_barcode_collisions(self, collision_rates: List[float], af_values: List[float],\n                                depth_values: List[int], n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test barcode collision contamination effects.\"\"\"\n        collision_results = {}\n\n        for collision_rate in collision_rates:\n            collision_results[collision_rate] = {}\n\n            for af in af_values:\n                collision_results[collision_rate][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n                    false_positive_rates = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 2000 + int(collision_rate * 1e6)\n                        )\n\n                        # Simulate with barcode collisions\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_barcode_collisions(\n                            contam_config, run_rng, collision_rate\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate metrics\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        expected_detections = max(1, int(af * depth * 0.8))\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n\n                        # Estimate false positive rate (excess detections)\n                        excess_detections = max(0, n_detected - expected_detections)\n                        fp_rate = excess_detections / depth if depth &gt; 0 else 0.0\n\n                        sensitivity_scores.append(sensitivity)\n                        false_positive_rates.append(fp_rate)\n\n                    collision_results[collision_rate][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'mean_fp_rate': float(np.mean(false_positive_rates)),\n                        'std_fp_rate': float(np.std(false_positive_rates)),\n                        'n_replicates': n_replicates\n                    }\n\n        return collision_results\n\n    def _test_cross_sample_contamination(self, contamination_proportions: List[float],\n                                       af_values: List[float], depth_values: List[int],\n                                       n_replicates: int) -&gt; Dict[str, Any]:\n        \"\"\"Test cross-sample contamination effects.\"\"\"\n        cross_results = {}\n\n        for contam_prop in contamination_proportions:\n            cross_results[contam_prop] = {}\n\n            for af in af_values:\n                cross_results[contam_prop][af] = {}\n\n                for depth in depth_values:\n                    sensitivity_scores = []\n\n                    for rep in range(n_replicates):\n                        run_rng = np.random.default_rng(\n                            self.config.seed + rep * 3000 + int(contam_prop * 1e6)\n                        )\n\n                        # Simulate with cross-sample contamination\n                        contam_config = self._create_contamination_config(af, depth)\n                        reads_df = self._simulate_with_cross_contamination(\n                            contam_config, run_rng, contam_prop\n                        )\n\n                        # Process through pipeline\n                        collapsed_df = collapse_umis(reads_df, contam_config, run_rng)\n                        error_model = fit_error_model(collapsed_df, contam_config, run_rng)\n                        calls_df = call_mrd(collapsed_df, error_model, contam_config, run_rng)\n\n                        # Calculate sensitivity\n                        n_detected = len(calls_df[calls_df['variant_call'] == True])\n                        expected_detections = max(1, int(af * depth * 0.8))\n                        sensitivity = min(1.0, n_detected / expected_detections) if expected_detections &gt; 0 else 0.0\n                        sensitivity_scores.append(sensitivity)\n\n                    cross_results[contam_prop][af][depth] = {\n                        'mean_sensitivity': float(np.mean(sensitivity_scores)),\n                        'std_sensitivity': float(np.std(sensitivity_scores)),\n                        'n_replicates': n_replicates\n                    }\n\n        return cross_results\n\n    def _simulate_with_index_hopping(self, config: PipelineConfig, \n                                   rng: np.random.Generator, \n                                   hop_rate: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with index-hopping contamination.\"\"\"\n        # Start with normal simulation\n        reads_df = simulate_reads(config, rng)\n\n        if hop_rate &gt; 0:\n            # Add hopped reads (contamination from other samples)\n            n_reads = len(reads_df)\n            n_hopped = rng.binomial(n_reads, hop_rate)\n\n            if n_hopped &gt; 0:\n                # Create contaminating reads (different background pattern)\n                contam_reads = reads_df.sample(n=n_hopped, random_state=rng).copy()\n\n                # Modify contaminating reads (different barcode context)\n                contam_reads['background_rate'] *= 2.0  # Higher error rate\n                contam_reads['n_false_positives'] *= 1.5  # More artifacts\n                contam_reads['sample_id'] = 'hopped_' + contam_reads['sample_id'].astype(str)\n\n                # Combine with original reads\n                reads_df = pd.concat([reads_df, contam_reads], ignore_index=True)\n\n        return reads_df\n\n    def _simulate_with_barcode_collisions(self, config: PipelineConfig,\n                                        rng: np.random.Generator,\n                                        collision_rate: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with barcode collision artifacts.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        if collision_rate &gt; 0:\n            # Simulate UMI collisions leading to false consensus\n            n_families = len(reads_df)\n            n_collisions = rng.binomial(n_families, collision_rate)\n\n            if n_collisions &gt; 0:\n                # Select families for collision\n                collision_indices = rng.choice(n_families, size=n_collisions, replace=False)\n\n                # Increase false positive rate for collided families\n                reads_df.loc[collision_indices, 'n_false_positives'] *= 2.0\n                reads_df.loc[collision_indices, 'background_rate'] *= 1.5\n\n                # Reduce consensus quality\n                reads_df.loc[collision_indices, 'mean_quality'] *= 0.8\n\n        return reads_df\n\n    def _simulate_with_cross_contamination(self, config: PipelineConfig,\n                                         rng: np.random.Generator,\n                                         contam_proportion: float) -&gt; pd.DataFrame:\n        \"\"\"Simulate reads with cross-sample contamination.\"\"\"\n        reads_df = simulate_reads(config, rng)\n\n        if contam_proportion &gt; 0:\n            # Add contaminating sample with different AF\n            n_reads = len(reads_df)\n            n_contam = int(n_reads * contam_proportion)\n\n            if n_contam &gt; 0:\n                # Create contaminating sample (higher AF)\n                contam_af = min(0.1, reads_df['allele_fraction'].iloc[0] * 10)  # 10x higher AF\n\n                contam_config = self._create_contamination_config(\n                    contam_af, reads_df['target_depth'].iloc[0]\n                )\n                contam_reads = simulate_reads(contam_config, rng)\n\n                # Take subset for contamination\n                if len(contam_reads) &gt; 0:\n                    contam_subset = contam_reads.sample(\n                        n=min(n_contam, len(contam_reads)), \n                        random_state=rng\n                    ).copy()\n                    contam_subset['sample_id'] = 'contam_' + contam_subset['sample_id'].astype(str)\n\n                    # Mix with original sample\n                    reads_df = pd.concat([reads_df, contam_subset], ignore_index=True)\n\n        return reads_df\n\n    def _create_contamination_config(self, af: float, depth: int) -&gt; PipelineConfig:\n        \"\"\"Create configuration for contamination testing.\"\"\"\n        return PipelineConfig(\n            run_id=f\"{self.config.run_id}_contam\",\n            seed=self.config.seed,\n            simulation=type(self.config.simulation)(\n                allele_fractions=[af],\n                umi_depths=[depth],\n                n_replicates=1,\n                n_bootstrap=self.config.simulation.n_bootstrap\n            ),\n            umi=self.config.umi,\n            stats=self.config.stats,\n            lod=self.config.lod\n        )\n\n    def _create_sensitivity_matrix(self, results: Dict[str, Any], \n                                 af_values: List[float], \n                                 depth_values: List[int]) -&gt; Dict[str, Any]:\n        \"\"\"Create sensitivity matrix for heatmap visualization.\"\"\"\n        # Extract sensitivity data for different contamination types\n        matrix_data = {}\n\n        # Index hopping sensitivity matrix\n        if 'index_hopping' in results:\n            hop_data = results['index_hopping']\n            hop_rates = list(hop_data.keys())\n\n            sensitivity_matrix = np.zeros((len(hop_rates), len(af_values) * len(depth_values)))\n\n            for i, hop_rate in enumerate(hop_rates):\n                col_idx = 0\n                for af in af_values:\n                    for depth in depth_values:\n                        if af in hop_data[hop_rate] and depth in hop_data[hop_rate][af]:\n                            sensitivity = hop_data[hop_rate][af][depth]['mean_sensitivity']\n                            sensitivity_matrix[i, col_idx] = sensitivity\n                        col_idx += 1\n\n            # Create labels\n            condition_labels = [f\"AF={af:.0e}, D={depth}\" for af in af_values for depth in depth_values]\n\n            matrix_data['index_hopping'] = {\n                'matrix': sensitivity_matrix.tolist(),\n                'hop_rates': hop_rates,\n                'condition_labels': condition_labels,\n                'af_values': af_values,\n                'depth_values': depth_values\n            }\n\n        return matrix_data\n\n    def generate_contamination_reports(self, output_dir: str = \"reports\") -&gt; None:\n        \"\"\"Generate contamination analysis reports.\"\"\"\n        if not self.contamination_results:\n            print(\"No contamination results to report\")\n            return\n\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Save contamination sensitivity results\n        contam_payload = dict(self.contamination_results)\n        contam_payload[\"schema_version\"] = \"1.0.0\"\n        contam_path = output_path / \"contam_sensitivity.json\"\n        with open(contam_path, 'w') as f:\n            json.dump(contam_payload, f, indent=2)\n        print(f\"Contamination sensitivity results saved to {contam_path}\")\n\n        # Generate contamination heatmap\n        self._plot_contamination_heatmap(output_path)\n\n    def _plot_contamination_heatmap(self, output_path: Path) -&gt; None:\n        \"\"\"Generate contamination sensitivity heatmap.\"\"\"\n        if 'sensitivity_matrix' not in self.contamination_results:\n            return\n\n        matrix_data = self.contamination_results['sensitivity_matrix']\n\n        if 'index_hopping' in matrix_data:\n            hop_data = matrix_data['index_hopping']\n\n            plt.figure(figsize=(12, 8))\n\n            # Create heatmap\n            sensitivity_matrix = np.array(hop_data['matrix'])\n            hop_rates = hop_data['hop_rates']\n            condition_labels = hop_data['condition_labels']\n\n            # Plot heatmap\n            sns.heatmap(sensitivity_matrix, \n                       xticklabels=condition_labels,\n                       yticklabels=[f\"{rate:.1%}\" for rate in hop_rates],\n                       annot=True, fmt='.2f', cmap='RdYlBu_r',\n                       cbar_kws={'label': 'Detection Sensitivity'})\n\n            plt.title('Contamination Impact on Variant Detection\\n(Index Hopping Effects)')\n            plt.xlabel('Test Conditions (AF, Depth)')\n            plt.ylabel('Index Hopping Rate')\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n\n            heatmap_path = output_path / \"contam_heatmap.png\"\n            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n            plt.close()\n            print(f\"Contamination heatmap saved to {heatmap_path}\")\n</code></pre>"},{"location":"api-reference/#precise_mrd.sim.contamination.ContaminationSimulator.simulate_contamination_effects","title":"<code>simulate_contamination_effects(hop_rates=[0.0, 0.001, 0.002, 0.005, 0.01], barcode_collision_rates=[0.0, 0.0001, 0.0005, 0.001], cross_sample_proportions=[0.0, 0.01, 0.05, 0.1], af_test_values=[0.001, 0.005, 0.01], depth_values=[1000, 5000], n_replicates=20)</code>","text":"<p>Simulate contamination effects on variant detection.</p> <p>Parameters:</p> Name Type Description Default <code>hop_rates</code> <code>List[float]</code> <p>Index-hopping rates to test</p> <code>[0.0, 0.001, 0.002, 0.005, 0.01]</code> <code>barcode_collision_rates</code> <code>List[float]</code> <p>Barcode collision rates to test</p> <code>[0.0, 0.0001, 0.0005, 0.001]</code> <code>cross_sample_proportions</code> <code>List[float]</code> <p>Cross-sample contamination proportions</p> <code>[0.0, 0.01, 0.05, 0.1]</code> <code>af_test_values</code> <code>List[float]</code> <p>Allele fractions to test contamination effects on</p> <code>[0.001, 0.005, 0.01]</code> <code>depth_values</code> <code>List[int]</code> <p>UMI depths to test</p> <code>[1000, 5000]</code> <code>n_replicates</code> <code>int</code> <p>Number of replicates per condition</p> <code>20</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with contamination simulation results</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def simulate_contamination_effects(self,\n                                 hop_rates: List[float] = [0.0, 0.001, 0.002, 0.005, 0.01],\n                                 barcode_collision_rates: List[float] = [0.0, 0.0001, 0.0005, 0.001],\n                                 cross_sample_proportions: List[float] = [0.0, 0.01, 0.05, 0.1],\n                                 af_test_values: List[float] = [0.001, 0.005, 0.01],\n                                 depth_values: List[int] = [1000, 5000],\n                                 n_replicates: int = 20) -&gt; Dict[str, Any]:\n    \"\"\"Simulate contamination effects on variant detection.\n\n    Args:\n        hop_rates: Index-hopping rates to test\n        barcode_collision_rates: Barcode collision rates to test\n        cross_sample_proportions: Cross-sample contamination proportions\n        af_test_values: Allele fractions to test contamination effects on\n        depth_values: UMI depths to test\n        n_replicates: Number of replicates per condition\n\n    Returns:\n        Dictionary with contamination simulation results\n    \"\"\"\n    print(\"Running contamination stress testing...\")\n\n    results = {}\n    sensitivity_matrix = []\n\n    # Test index-hopping effects\n    print(\"  Testing index-hopping effects...\")\n    hop_results = self._test_index_hopping(hop_rates, af_test_values, \n                                          depth_values, n_replicates)\n    results['index_hopping'] = hop_results\n\n    # Test barcode collision effects\n    print(\"  Testing barcode collision effects...\")\n    collision_results = self._test_barcode_collisions(barcode_collision_rates,\n                                                     af_test_values, depth_values, \n                                                     n_replicates)\n    results['barcode_collisions'] = collision_results\n\n    # Test cross-sample contamination\n    print(\"  Testing cross-sample contamination...\")\n    cross_contam_results = self._test_cross_sample_contamination(\n        cross_sample_proportions, af_test_values, depth_values, n_replicates)\n    results['cross_sample_contamination'] = cross_contam_results\n\n    # Create sensitivity matrix for heatmap\n    sensitivity_matrix = self._create_sensitivity_matrix(results, af_test_values, \n                                                       depth_values)\n    results['sensitivity_matrix'] = sensitivity_matrix\n\n    self.contamination_results = results\n    return results\n</code></pre>"},{"location":"api-reference/#precise_mrd.sim.contamination.ContaminationSimulator.generate_contamination_reports","title":"<code>generate_contamination_reports(output_dir='reports')</code>","text":"<p>Generate contamination analysis reports.</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def generate_contamination_reports(self, output_dir: str = \"reports\") -&gt; None:\n    \"\"\"Generate contamination analysis reports.\"\"\"\n    if not self.contamination_results:\n        print(\"No contamination results to report\")\n        return\n\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Save contamination sensitivity results\n    contam_payload = dict(self.contamination_results)\n    contam_payload[\"schema_version\"] = \"1.0.0\"\n    contam_path = output_path / \"contam_sensitivity.json\"\n    with open(contam_path, 'w') as f:\n        json.dump(contam_payload, f, indent=2)\n    print(f\"Contamination sensitivity results saved to {contam_path}\")\n\n    # Generate contamination heatmap\n    self._plot_contamination_heatmap(output_path)\n</code></pre>"},{"location":"api-reference/#precise_mrd.sim.contamination.run_contamination_stress_test","title":"<code>precise_mrd.sim.contamination.run_contamination_stress_test(config, rng, output_dir='reports')</code>","text":"<p>Run complete contamination stress testing suite.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Random number generator</p> required <code>output_dir</code> <code>str</code> <p>Output directory for reports</p> <code>'reports'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Contamination analysis results</p> Source code in <code>src/precise_mrd/sim/contamination.py</code> <pre><code>def run_contamination_stress_test(config: PipelineConfig, \n                                rng: np.random.Generator,\n                                output_dir: str = \"reports\") -&gt; Dict[str, Any]:\n    \"\"\"Run complete contamination stress testing suite.\n\n    Args:\n        config: Pipeline configuration\n        rng: Random number generator\n        output_dir: Output directory for reports\n\n    Returns:\n        Contamination analysis results\n    \"\"\"\n    simulator = ContaminationSimulator(config, rng)\n\n    # Run contamination simulations\n    results = simulator.simulate_contamination_effects()\n\n    # Generate reports\n    simulator.generate_contamination_reports(output_dir)\n\n    return results\n</code></pre>"},{"location":"api-reference/#core-pipeline-components","title":"Core Pipeline Components","text":""},{"location":"api-reference/#simulation","title":"Simulation","text":""},{"location":"api-reference/#precise_mrd.simulate.simulate_reads","title":"<code>precise_mrd.simulate.simulate_reads(config, rng, output_path=None)</code>","text":"<p>Simulate synthetic UMI reads for ctDNA analysis.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with simulated read data</p> Source code in <code>src/precise_mrd/simulate.py</code> <pre><code>def simulate_reads(\n    config: PipelineConfig, \n    rng: np.random.Generator,\n    output_path: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Simulate synthetic UMI reads for ctDNA analysis.\n\n    Args:\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n\n    Returns:\n        DataFrame with simulated read data\n    \"\"\"\n\n    # Extract simulation parameters\n    sim_config = config.simulation\n\n    # Handle case where simulation config might be a dict\n    if isinstance(sim_config, dict):\n        allele_fractions = sim_config['allele_fractions']\n        umi_depths = sim_config['umi_depths'] \n        n_replicates = sim_config['n_replicates']\n    else:\n        allele_fractions = sim_config.allele_fractions\n        umi_depths = sim_config.umi_depths\n        n_replicates = sim_config.n_replicates\n\n    # Handle UMI config\n    umi_config = config.umi\n    if isinstance(umi_config, dict):\n        max_family_size = umi_config['max_family_size']\n    else:\n        max_family_size = umi_config.max_family_size\n\n    # Generate synthetic data based on configuration\n    n_variants = len(allele_fractions)\n    n_depths = len(umi_depths)\n    total_samples = n_variants * n_depths * n_replicates\n\n    # Create grid of conditions\n    data = []\n    sample_id = 0\n\n    for af in allele_fractions:\n        for depth in umi_depths:\n            for rep in range(n_replicates):\n                # Simulate UMI families\n                n_families = depth\n\n                # Background error rate (trinucleotide context dependent)\n                background_rate = rng.uniform(1e-5, 1e-3)\n\n                # Generate reads per family (Poisson distributed)\n                family_sizes = rng.poisson(lam=5, size=n_families)\n                family_sizes = np.clip(family_sizes, 1, max_family_size)\n\n                # Simulate variant calls\n                # True positives based on allele fraction\n                n_true_variants = rng.binomial(n_families, af)\n\n                # False positives from background errors\n                n_false_positives = rng.binomial(\n                    n_families - n_true_variants, \n                    background_rate\n                )\n\n                # Quality scores (higher for true variants)\n                quality_scores = rng.normal(25, 5, n_families)\n                quality_scores = np.clip(quality_scores, 10, 40)\n\n                sample_data = {\n                    'sample_id': sample_id,\n                    'allele_fraction': af,\n                    'target_depth': depth,\n                    'replicate': rep,\n                    'n_families': n_families,\n                    'n_true_variants': n_true_variants,\n                    'n_false_positives': n_false_positives,\n                    'background_rate': background_rate,\n                    'mean_family_size': np.mean(family_sizes),\n                    'mean_quality': np.mean(quality_scores),\n                    'config_hash': config.config_hash(),\n                }\n                data.append(sample_data)\n                sample_id += 1\n\n    df = pd.DataFrame(data)\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference/#umi-collapse","title":"UMI Collapse","text":""},{"location":"api-reference/#precise_mrd.collapse.collapse_umis","title":"<code>precise_mrd.collapse.collapse_umis(reads_df, config, rng, output_path=None, is_fastq_data=False, use_parallel=False, n_partitions=None)</code>","text":"<p>Collapse UMI families and call consensus.</p> <p>Parameters:</p> Name Type Description Default <code>reads_df</code> <code>Union[DataFrame, DataFrame]</code> <p>DataFrame with read data (synthetic or FASTQ)</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>is_fastq_data</code> <code>bool</code> <p>Whether input data comes from FASTQ files</p> <code>False</code> <code>use_parallel</code> <code>bool</code> <p>Whether to use parallel processing with Dask</p> <code>False</code> <code>n_partitions</code> <code>int</code> <p>Number of partitions for parallel processing</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> Source code in <code>src/precise_mrd/collapse.py</code> <pre><code>def collapse_umis(\n    reads_df: Union[pd.DataFrame, dd.DataFrame],\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    is_fastq_data: bool = False,\n    use_parallel: bool = False,\n    n_partitions: int = None\n) -&gt; pd.DataFrame:\n    \"\"\"Collapse UMI families and call consensus.\n\n    Args:\n        reads_df: DataFrame with read data (synthetic or FASTQ)\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        is_fastq_data: Whether input data comes from FASTQ files\n        use_parallel: Whether to use parallel processing with Dask\n        n_partitions: Number of partitions for parallel processing\n\n    Returns:\n        DataFrame with collapsed UMI data\n    \"\"\"\n\n    # Use parallel processing if requested and Dask is available\n    if use_parallel and DASK_AVAILABLE and isinstance(reads_df, pd.DataFrame):\n        return _collapse_umis_parallel(reads_df, config, rng, output_path, is_fastq_data, n_partitions)\n\n    # Fall back to original implementation for other cases\n    return _collapse_umis_sequential(reads_df, config, rng, output_path, is_fastq_data)\n</code></pre>"},{"location":"api-reference/#error-modeling","title":"Error Modeling","text":""},{"location":"api-reference/#precise_mrd.error_model.fit_error_model","title":"<code>precise_mrd.error_model.fit_error_model(collapsed_df, config, rng, output_path=None, use_cache=True, cache_dir='.cache', use_advanced_stats=False)</code>","text":"<p>Fit trinucleotide context-specific error model with optional caching and advanced statistics.</p> <p>Parameters:</p> Name Type Description Default <code>collapsed_df</code> <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use caching for expensive computations</p> <code>True</code> <code>cache_dir</code> <code>str</code> <p>Directory for cache files</p> <code>'.cache'</code> <code>use_advanced_stats</code> <code>bool</code> <p>Whether to use advanced Bayesian modeling</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with error model parameters</p> Source code in <code>src/precise_mrd/error_model.py</code> <pre><code>def fit_error_model(\n    collapsed_df: pd.DataFrame,\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    use_cache: bool = True,\n    cache_dir: str = \".cache\",\n    use_advanced_stats: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Fit trinucleotide context-specific error model with optional caching and advanced statistics.\n\n    Args:\n        collapsed_df: DataFrame with collapsed UMI data\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        use_cache: Whether to use caching for expensive computations\n        cache_dir: Directory for cache files\n        use_advanced_stats: Whether to use advanced Bayesian modeling\n\n    Returns:\n        DataFrame with error model parameters\n    \"\"\"\n\n    # Try to load from cache first\n    if use_cache:\n        cache_key = _get_cache_key(config, collapsed_df)\n        cached_result = _load_cached_error_model(cache_key, cache_dir)\n        if cached_result is not None:\n            print(f\"  Using cached error model: {cache_key}\")\n            if output_path:\n                cached_result.to_parquet(output_path, index=False)\n            return cached_result\n\n    # Use advanced Bayesian modeling if requested\n    if use_advanced_stats:\n        print(\"  Using advanced Bayesian error modeling...\")\n        bayesian_model = BayesianErrorModel(config)\n        advanced_results = bayesian_model.fit_bayesian_model(collapsed_df, rng)\n\n        # Convert to DataFrame format for compatibility\n        error_data = []\n        for context, params in advanced_results['context_error_rates'].items():\n            error_data.append({\n                'trinucleotide_context': context,\n                'error_rate': params['error_rate'],\n                'ci_lower': params['ci_lower'],\n                'ci_upper': params['ci_upper'],\n                'n_observations': params['n_observations'],\n                'model_type': 'bayesian',\n                'config_hash': params.get('config_hash', config.config_hash())\n            })\n\n        df = pd.DataFrame(error_data)\n\n        # Save to cache for future use\n        if use_cache:\n            _save_cached_error_model(df, cache_key, cache_dir)\n\n        if output_path:\n            df.to_parquet(output_path, index=False)\n\n        return df\n\n    # Original simple error model\n\n    # Define trinucleotide contexts\n    contexts = [\n        'AAA', 'AAC', 'AAG', 'AAT',\n        'ACA', 'ACC', 'ACG', 'ACT',\n        'AGA', 'AGC', 'AGG', 'AGT',\n        'ATA', 'ATC', 'ATG', 'ATT',\n        'CAA', 'CAC', 'CAG', 'CAT',\n        'CCA', 'CCC', 'CCG', 'CCT',\n        'CGA', 'CGC', 'CGG', 'CGT',\n        'CTA', 'CTC', 'CTG', 'CTT',\n        'GAA', 'GAC', 'GAG', 'GAT',\n        'GCA', 'GCC', 'GCG', 'GCT',\n        'GGA', 'GGC', 'GGG', 'GGT',\n        'GTA', 'GTC', 'GTG', 'GTT',\n        'TAA', 'TAC', 'TAG', 'TAT',\n        'TCA', 'TCC', 'TCG', 'TCT',\n        'TGA', 'TGC', 'TGG', 'TGT',\n        'TTA', 'TTC', 'TTG', 'TTT'\n    ]\n\n    # Fit error rates for each trinucleotide context\n    error_data = []\n\n    for context in contexts:\n        # Estimate error rate from negative control samples (lowest AF)\n        negative_samples = collapsed_df[collapsed_df['allele_fraction'] &lt;= 0.0001]\n\n        if len(negative_samples) == 0:\n            # Fallback to lowest AF samples\n            min_af = collapsed_df['allele_fraction'].min()\n            negative_samples = collapsed_df[collapsed_df['allele_fraction'] == min_af]\n\n        # Count variants in negative samples for this context\n        if len(negative_samples) &gt; 0:\n            variant_rate = negative_samples['is_variant'].mean()\n            # Add context-specific variation\n            context_modifier = rng.uniform(0.5, 2.0)\n            error_rate = variant_rate * context_modifier\n        else:\n            # Default error rate\n            error_rate = rng.uniform(1e-5, 1e-3)\n\n        # Confidence interval from bootstrap\n        if len(negative_samples) &gt; 10:\n            bootstrap_rates = []\n            for _ in range(100):\n                boot_sample = negative_samples.sample(\n                    n=len(negative_samples), \n                    replace=True, \n                    random_state=rng.integers(0, 2**32-1)\n                )\n                boot_rate = boot_sample['is_variant'].mean() * context_modifier\n                bootstrap_rates.append(boot_rate)\n\n            ci_lower = np.percentile(bootstrap_rates, 2.5)\n            ci_upper = np.percentile(bootstrap_rates, 97.5)\n        else:\n            ci_lower = error_rate * 0.5\n            ci_upper = error_rate * 2.0\n\n        error_data.append({\n            'trinucleotide_context': context,\n            'error_rate': error_rate,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper,\n            'n_observations': len(negative_samples),\n            'config_hash': config.config_hash(),\n        })\n\n    df = pd.DataFrame(error_data)\n\n    # Save to cache for future use\n    if use_cache:\n        cache_key = _get_cache_key(config, collapsed_df)\n        _save_cached_error_model(df, cache_key, cache_dir)\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference/#statistical-testing","title":"Statistical Testing","text":""},{"location":"api-reference/#precise_mrd.call.call_mrd","title":"<code>precise_mrd.call.call_mrd(collapsed_df, error_model_df, config, rng, output_path=None, use_ml_calling=False, ml_model_type='ensemble', use_deep_learning=False, dl_model_type='cnn_lstm')</code>","text":"<p>Perform MRD calling with statistical testing or ML-based approaches.</p> <p>Parameters:</p> Name Type Description Default <code>collapsed_df</code> <code>DataFrame</code> <p>DataFrame with collapsed UMI data</p> required <code>error_model_df</code> <code>DataFrame</code> <p>DataFrame with error model</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration</p> required <code>rng</code> <code>Generator</code> <p>Seeded random number generator</p> required <code>output_path</code> <code>Optional[str]</code> <p>Optional path to save results</p> <code>None</code> <code>use_ml_calling</code> <code>bool</code> <p>Whether to use ML-based variant calling</p> <code>False</code> <code>ml_model_type</code> <code>str</code> <p>Type of ML model to use ('ensemble', 'xgboost', 'lightgbm', 'gbm')</p> <code>'ensemble'</code> <code>use_deep_learning</code> <code>bool</code> <p>Whether to use deep learning-based variant calling</p> <code>False</code> <code>dl_model_type</code> <code>str</code> <p>Type of deep learning model to use ('cnn_lstm', 'hybrid', 'transformer')</p> <code>'cnn_lstm'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with MRD calls and statistics</p> Source code in <code>src/precise_mrd/call.py</code> <pre><code>def call_mrd(\n    collapsed_df: pd.DataFrame,\n    error_model_df: pd.DataFrame,\n    config: PipelineConfig,\n    rng: np.random.Generator,\n    output_path: Optional[str] = None,\n    use_ml_calling: bool = False,\n    ml_model_type: str = 'ensemble',\n    use_deep_learning: bool = False,\n    dl_model_type: str = 'cnn_lstm'\n) -&gt; pd.DataFrame:\n    \"\"\"Perform MRD calling with statistical testing or ML-based approaches.\n\n    Args:\n        collapsed_df: DataFrame with collapsed UMI data\n        error_model_df: DataFrame with error model\n        config: Pipeline configuration\n        rng: Seeded random number generator\n        output_path: Optional path to save results\n        use_ml_calling: Whether to use ML-based variant calling\n        ml_model_type: Type of ML model to use ('ensemble', 'xgboost', 'lightgbm', 'gbm')\n        use_deep_learning: Whether to use deep learning-based variant calling\n        dl_model_type: Type of deep learning model to use ('cnn_lstm', 'hybrid', 'transformer')\n\n    Returns:\n        DataFrame with MRD calls and statistics\n    \"\"\"\n\n    # Use ML-based variant calling if requested\n    if use_ml_calling:\n        print(f\"  Using enhanced ML-based variant calling ({ml_model_type})...\")\n\n        # Choose model based on type\n        if ml_model_type == 'ensemble':\n            ml_caller = EnsembleVariantCaller(config)\n        else:\n            ml_caller = GradientBoostedVariantCaller(config, ml_model_type)\n\n        # Train the model\n        if ml_model_type == 'ensemble':\n            training_results = ml_caller.train_ensemble(collapsed_df, rng)\n        else:\n            training_results = ml_caller.train_model(collapsed_df, rng)\n\n        # Get ML predictions\n        ml_probabilities = ml_caller.predict_variants(collapsed_df)\n\n        # Use optimal threshold from training\n        optimal_threshold = training_results.get('optimal_threshold', np.median(ml_probabilities))\n        ml_calls = (ml_probabilities &gt; optimal_threshold).astype(int)\n\n        # Get feature importance for reporting\n        feature_importance = ml_caller.get_feature_importance()\n\n        print(f\"  Ensemble trained with {len(ml_caller.models)} models\")\n        print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n        print(f\"  Top features: {list(feature_importance.keys())[:3]}\")\n\n        # Create results DataFrame (vectorized)\n        results_df = pd.DataFrame({\n            'sample_id': collapsed_df['sample_id'],\n            'family_id': collapsed_df['family_id'],\n            'family_size': collapsed_df['family_size'],\n            'quality_score': collapsed_df['quality_score'],\n            'consensus_agreement': collapsed_df['consensus_agreement'],\n            'passes_quality': collapsed_df['passes_quality'],\n            'passes_consensus': collapsed_df['passes_consensus'],\n            'is_variant': ml_calls,\n            'p_value': 1.0 - ml_probabilities,  # Convert probability to p-value-like score\n            'ml_probability': ml_probabilities,\n            'ml_threshold': optimal_threshold,\n            'calling_method': 'ml_ensemble',\n            'config_hash': config.config_hash()\n        })\n\n        df = results_df\n\n    # Use deep learning-based variant calling if requested\n    if use_deep_learning:\n        print(f\"  Using deep learning-based variant calling ({dl_model_type})...\")\n\n        # Initialize deep learning caller\n        dl_caller = DeepLearningVariantCaller(config, dl_model_type)\n\n        # Train the model\n        training_results = dl_caller.train_model(collapsed_df, rng)\n\n        # Get deep learning predictions\n        dl_probabilities = dl_caller.predict_variants(collapsed_df)\n\n        # Use optimal threshold from training\n        optimal_threshold = training_results.get('optimal_threshold', np.median(dl_probabilities))\n        dl_calls = (dl_probabilities &gt; optimal_threshold).astype(int)\n\n        # Get model summary for reporting\n        model_summary = dl_caller.get_model_summary()\n\n        print(f\"  Deep learning model trained: {model_summary}\")\n        print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n\n        # Create results DataFrame (vectorized)\n        results_df = pd.DataFrame({\n            'sample_id': collapsed_df['sample_id'],\n            'family_id': collapsed_df['family_id'],\n            'family_size': collapsed_df['family_size'],\n            'quality_score': collapsed_df['quality_score'],\n            'consensus_agreement': collapsed_df['consensus_agreement'],\n            'passes_quality': collapsed_df['passes_quality'],\n            'passes_consensus': collapsed_df['passes_consensus'],\n            'is_variant': dl_calls,\n            'p_value': 1.0 - dl_probabilities,  # Convert probability to p-value-like score\n            'dl_probability': dl_probabilities,\n            'dl_threshold': optimal_threshold,\n            'calling_method': f'dl_{dl_model_type}',\n            'config_hash': config.config_hash()\n        })\n\n        df = results_df\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n\n    stats_config = config.stats\n\n    # Handle case where stats config might be a dict\n    if isinstance(stats_config, dict):\n        test_type = stats_config['test_type']\n        alpha = stats_config['alpha']\n        fdr_method = stats_config['fdr_method']\n    else:\n        test_type = stats_config.test_type\n        alpha = stats_config.alpha\n        fdr_method = stats_config.fdr_method\n\n    # Group by sample for statistical testing\n    call_data = []\n\n    for sample_id in collapsed_df['sample_id'].unique():\n        sample_data = collapsed_df[collapsed_df['sample_id'] == sample_id]\n\n        if len(sample_data) == 0:\n            continue\n\n        # Get sample metadata\n        allele_fraction = sample_data['allele_fraction'].iloc[0]\n\n        # Count variants and total families\n        n_variants = sample_data['is_variant'].sum()\n        n_total = len(sample_data)\n\n        if n_total == 0:\n            continue\n\n        # Get expected error rate (average across contexts)\n        mean_error_rate = error_model_df['error_rate'].mean()\n        expected_variants = n_total * mean_error_rate\n\n        # Perform statistical test\n        if test_type == \"poisson\":\n            p_value = poisson_test(n_variants, expected_variants)\n        elif test_type == \"binomial\":\n            p_value = binomial_test(n_variants, n_total, mean_error_rate)\n        else:\n            raise ValueError(f\"Unknown test type: {test_type}\")\n\n        # Calculate effect size\n        if expected_variants &gt; 0:\n            fold_change = n_variants / expected_variants\n        else:\n            fold_change = float('inf') if n_variants &gt; 0 else 1.0\n\n        # Quality metrics\n        mean_quality = sample_data['quality_score'].mean()\n        mean_consensus = sample_data['consensus_agreement'].mean()\n\n        call_data.append({\n            'sample_id': sample_id,\n            'allele_fraction': allele_fraction,\n            'n_variants': n_variants,\n            'n_total': n_total,\n            'variant_fraction': n_variants / n_total if n_total &gt; 0 else 0,\n            'expected_variants': expected_variants,\n            'fold_change': fold_change,\n            'p_value': p_value,\n            'mean_quality': mean_quality,\n            'mean_consensus': mean_consensus,\n            'test_type': test_type,\n            'config_hash': config.config_hash(),\n        })\n\n    if not call_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(call_data)\n\n    # Apply multiple testing correction\n    p_values = df['p_value'].values\n    rejected, adjusted_p = benjamini_hochberg_correction(\n        p_values, \n        alpha\n    )\n\n    df['p_adjusted'] = adjusted_p\n    df['significant'] = rejected\n    df['alpha'] = alpha\n    df['fdr_method'] = fdr_method\n\n    if output_path:\n        df.to_parquet(output_path, index=False)\n\n    return df\n</code></pre>"},{"location":"api-reference/#performance-metrics","title":"Performance Metrics","text":""},{"location":"api-reference/#precise_mrd.metrics.calculate_metrics","title":"<code>precise_mrd.metrics.calculate_metrics(calls_df, rng, n_bootstrap=1000, n_jobs=-1, config=None, use_advanced_ci=False, run_validation=False)</code>","text":"<p>Calculate comprehensive performance metrics with optional advanced confidence intervals and validation.</p> <p>Parameters:</p> Name Type Description Default <code>calls_df</code> <code>DataFrame</code> <p>DataFrame with MRD calls</p> required <code>rng</code> <code>Generator</code> <p>Random number generator for bootstrap</p> required <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples</p> <code>1000</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for bootstrap (-1 for all cores)</p> <code>-1</code> <code>config</code> <code>Optional[PipelineConfig]</code> <p>Pipeline configuration for advanced statistics</p> <code>None</code> <code>use_advanced_ci</code> <code>bool</code> <p>Whether to use advanced confidence interval methods</p> <code>False</code> <code>run_validation</code> <code>bool</code> <p>Whether to run comprehensive statistical validation</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all metrics</p> Source code in <code>src/precise_mrd/metrics.py</code> <pre><code>def calculate_metrics(\n    calls_df: pd.DataFrame,\n    rng: np.random.Generator,\n    n_bootstrap: int = 1000,\n    n_jobs: int = -1,\n    config: Optional[PipelineConfig] = None,\n    use_advanced_ci: bool = False,\n    run_validation: bool = False\n) -&gt; Dict[str, Any]:\n    \"\"\"Calculate comprehensive performance metrics with optional advanced confidence intervals and validation.\n\n    Args:\n        calls_df: DataFrame with MRD calls\n        rng: Random number generator for bootstrap\n        n_bootstrap: Number of bootstrap samples\n        n_jobs: Number of parallel jobs for bootstrap (-1 for all cores)\n        config: Pipeline configuration for advanced statistics\n        use_advanced_ci: Whether to use advanced confidence interval methods\n        run_validation: Whether to run comprehensive statistical validation\n\n    Returns:\n        Dictionary with all metrics\n    \"\"\"\n    if len(calls_df) == 0:\n        return {\n            \"roc_auc\": 0.0,\n            \"average_precision\": 0.0,\n            \"detected_cases\": 0,\n            \"total_cases\": 0,\n            \"calibration\": []\n        }\n\n    # Define truth labels (high AF = positive case)\n    if 'allele_fraction' in calls_df.columns:\n        y_true = (calls_df['allele_fraction'] &gt; 0.001).astype(int)\n    else:\n        # For real data or ML-based calling, we don't have ground truth\n        # Use a default approach or skip certain metrics\n        print(\"  Warning: No allele_fraction column found, using simplified metrics\")\n        y_true = np.zeros(len(calls_df))  # All negative for compatibility\n\n    # Use variant fraction as prediction score\n    if 'variant_fraction' in calls_df.columns:\n        y_score = calls_df['variant_fraction'].values\n    elif 'ml_probability' in calls_df.columns:\n        y_score = calls_df['ml_probability'].values\n    else:\n        # Fallback to p-value if available\n        if 'p_value' in calls_df.columns:\n            y_score = 1.0 - calls_df['p_value'].values  # Convert p-value to score\n        else:\n            y_score = np.random.random(len(calls_df))  # Random fallback\n\n    # Basic metrics\n    roc_auc = roc_auc_score(y_true, y_score)\n    avg_precision = average_precision(y_true, y_score)\n\n    # Confidence intervals\n    if use_advanced_ci and config:\n        print(\"  Using advanced confidence interval methods...\")\n        adv_ci = AdvancedConfidenceIntervals(config)\n\n        # Advanced bootstrap CI for ROC AUC\n        roc_ci = adv_ci.bootstrap_confidence_interval(\n            y_score, lambda x: roc_auc_score(y_true, x), n_bootstrap, rng\n        )\n\n        # Advanced bootstrap CI for Average Precision\n        ap_ci = adv_ci.bootstrap_confidence_interval(\n            y_score, lambda x: average_precision(y_true, x), n_bootstrap, rng\n        )\n\n        # Add method information\n        roc_ci['method'] = 'advanced_bootstrap'\n        ap_ci['method'] = 'advanced_bootstrap'\n    else:\n        # Standard bootstrap CI (parallel processing)\n        roc_ci = bootstrap_metric(y_true, y_score, roc_auc_score, n_bootstrap, rng, n_jobs)\n        ap_ci = bootstrap_metric(y_true, y_score, average_precision, n_bootstrap, rng, n_jobs)\n\n    # Detection statistics\n    if 'significant' in calls_df.columns:\n        if 'allele_fraction' in calls_df.columns:\n            detected_cases = int(np.sum(calls_df['significant'] &amp; (y_true == 1)))\n            total_cases = int(np.sum(y_true))\n        else:\n            # For real data, just count significant calls\n            detected_cases = int(np.sum(calls_df['significant']))\n            total_cases = len(calls_df)  # All samples are \"cases\" in real data context\n    else:\n        detected_cases = 0\n        total_cases = len(calls_df)\n\n    # Calibration analysis\n    calibration = calibration_analysis(y_true, y_score)\n\n    # Brier score\n    brier_score = float(np.mean((y_score - y_true) ** 2))\n\n    # Add statistical validation if requested\n    validation_results = {}\n    if run_validation and config:\n        print(\"  Running comprehensive statistical validation...\")\n\n        # Cross-validation for model performance\n        if 'ml_probability' in calls_df.columns:\n            X = calls_df[['family_size', 'quality_score', 'consensus_agreement']].values\n            y = calls_df['is_variant'].values\n\n            # Simple model function for demonstration\n            def simple_model_func(X_train, y_train):\n                from sklearn.ensemble import RandomForestClassifier\n                model = RandomForestClassifier(n_estimators=10, random_state=config.seed)\n                model.fit(X_train, y_train)\n                return model\n\n            cv = CrossValidator(config)\n            cv_results = cv.k_fold_cross_validation(X, y, simple_model_func, k_folds=3)\n            validation_results['cross_validation'] = cv_results\n\n        # Calibration analysis\n        if 'ml_probability' in calls_df.columns:\n            calibrator = ModelValidator(config)\n            calibration_results = calibrator.calibration_analysis(y_true, calls_df['ml_probability'].values)\n            validation_results['calibration_analysis'] = calibration_results\n\n        # Robustness analysis\n        robustness = RobustnessAnalyzer(config)\n        robustness_results = robustness.bootstrap_robustness(calls_df, n_bootstrap=50)\n        validation_results['robustness_analysis'] = robustness_results\n\n        # Uncertainty quantification\n        uncertainty = UncertaintyQuantifier(config)\n        # Example: quantify uncertainty in variant rate estimates\n        variant_rates = calls_df['is_variant'].values\n        uncertainty_results = uncertainty.bayesian_uncertainty([variant_rates])\n        validation_results['uncertainty_quantification'] = uncertainty_results\n\n    return {\n        \"roc_auc\": float(roc_auc),\n        \"roc_auc_ci\": roc_ci,\n        \"average_precision\": float(avg_precision),\n        \"average_precision_ci\": ap_ci,\n        \"brier_score\": brier_score,\n        \"detected_cases\": detected_cases,\n        \"total_cases\": total_cases,\n        \"calibration\": calibration,\n        \"statistical_validation\": validation_results if validation_results else None\n    }\n</code></pre>"},{"location":"api-reference/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/#basic-detection-limit-analysis","title":"Basic Detection Limit Analysis","text":"<pre><code>import numpy as np\nfrom precise_mrd.config import load_config\nfrom precise_mrd.eval.lod import LODAnalyzer\n\n# Load configuration\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)\n\n# Create analyzer\nanalyzer = LODAnalyzer(config, rng)\n\n# Estimate Limit of Blank\nlob_results = analyzer.estimate_lob(n_blank_runs=100)\nprint(f\"LoB: {lob_results['lob_value']:.3f}\")\n\n# Estimate Limit of Detection\nlod_results = analyzer.estimate_lod(\n    af_range=(1e-4, 1e-2),\n    depth_values=[1000, 5000, 10000],\n    n_replicates=50\n)\n\n# Print LoD for each depth\nfor depth, results in lod_results['depth_results'].items():\n    lod_af = results['lod_af']\n    ci_lower = results['lod_ci_lower']\n    ci_upper = results['lod_ci_upper']\n    print(f\"LoD at {depth} depth: {lod_af:.2e} AF [{ci_lower:.2e}, {ci_upper:.2e}]\")\n\n# Generate reports\nanalyzer.generate_reports(\"reports/\")\n</code></pre>"},{"location":"api-reference/#contamination-stress-testing","title":"Contamination Stress Testing","text":"<pre><code>from precise_mrd.sim.contamination import ContaminationSimulator\n\n# Create contamination simulator\nsimulator = ContaminationSimulator(config, rng)\n\n# Run contamination effects simulation\nresults = simulator.simulate_contamination_effects(\n    hop_rates=[0.0, 0.001, 0.005, 0.01],\n    barcode_collision_rates=[0.0, 0.0001, 0.001],\n    cross_sample_proportions=[0.0, 0.01, 0.05, 0.1],\n    af_test_values=[0.001, 0.005, 0.01],\n    depth_values=[1000, 5000],\n    n_replicates=20\n)\n\n# Access results by contamination type\nhop_results = results['index_hopping']\nfor hop_rate, af_data in hop_results.items():\n    for af, depth_data in af_data.items():\n        for depth, metrics in depth_data.items():\n            sensitivity = metrics['mean_sensitivity']\n            print(f\"Hop rate {hop_rate}, AF {af}, depth {depth}: {sensitivity:.3f}\")\n\n# Generate reports\nsimulator.generate_contamination_reports(\"reports/\")\n</code></pre>"},{"location":"api-reference/#stratified-power-analysis","title":"Stratified Power Analysis","text":"<pre><code>from precise_mrd.eval.stratified import StratifiedAnalyzer\n\n# Create stratified analyzer\nanalyzer = StratifiedAnalyzer(config, rng)\n\n# Run stratified power analysis\npower_results = analyzer.analyze_stratified_power(\n    af_values=[0.001, 0.005, 0.01, 0.05],\n    depth_values=[1000, 5000, 10000],\n    contexts=['CpG', 'CHG', 'CHH', 'NpN'],\n    n_replicates=50\n)\n\n# Access power results by context\nfor context, depth_data in power_results['stratified_results'].items():\n    print(f\"\\nContext: {context}\")\n    for depth, af_data in depth_data.items():\n        for af, results in af_data.items():\n            power = results['mean_detection_rate']\n            std = results['std_detection_rate']\n            print(f\"  Depth {depth}, AF {af:.0e}: {power:.3f} \u00b1 {std:.3f}\")\n\n# Run calibration analysis\ncalib_results = analyzer.analyze_calibration_by_bins(\n    af_values=[0.001, 0.005, 0.01, 0.05],\n    depth_values=[1000, 5000, 10000],\n    n_bins=10,\n    n_replicates=100\n)\n\n# Print calibration summary\nfor data_point in calib_results['calibration_data']:\n    depth = data_point['depth']\n    af = data_point['af']\n    ece = data_point['ece']\n    print(f\"Depth {depth}, AF {af:.0e}: ECE = {ece:.3f}\")\n</code></pre>"},{"location":"api-reference/#complete-pipeline-execution","title":"Complete Pipeline Execution","text":"<pre><code>from precise_mrd.simulate import simulate_reads\nfrom precise_mrd.collapse import collapse_umis\nfrom precise_mrd.error_model import fit_error_model\nfrom precise_mrd.call import call_mrd\nfrom precise_mrd.metrics import calculate_metrics\n\n# Run complete pipeline\ndef run_complete_pipeline(config, rng):\n    \"\"\"Run the complete Precise MRD pipeline.\"\"\"\n\n    # 1. Simulate synthetic reads\n    print(\"Simulating reads...\")\n    reads_df = simulate_reads(config, rng)\n    print(f\"Generated {len(reads_df)} read families\")\n\n    # 2. Collapse UMI families\n    print(\"Collapsing UMI families...\")\n    collapsed_df = collapse_umis(reads_df, config, rng)\n    print(f\"Collapsed to {len(collapsed_df)} consensus reads\")\n\n    # 3. Fit error model\n    print(\"Fitting error model...\")\n    error_model = fit_error_model(collapsed_df, config, rng)\n    print(f\"Error model fitted with {len(error_model)} parameters\")\n\n    # 4. Call variants\n    print(\"Calling variants...\")\n    calls_df = call_mrd(collapsed_df, error_model, config, rng)\n    n_variants = len(calls_df[calls_df['variant_call'] == True])\n    print(f\"Called {n_variants} variants from {len(calls_df)} tests\")\n\n    # 5. Calculate performance metrics\n    print(\"Calculating metrics...\")\n    metrics = calculate_metrics(calls_df, config, rng)\n    print(f\"Calculated {len(metrics)} performance metrics\")\n\n    return {\n        'reads': reads_df,\n        'collapsed': collapsed_df,\n        'error_model': error_model,\n        'calls': calls_df,\n        'metrics': metrics\n    }\n\n# Execute pipeline\nresults = run_complete_pipeline(config, rng)\n</code></pre>"},{"location":"api-reference/#convenience-functions","title":"Convenience Functions","text":"<p>For quick analyses, use the convenience functions:</p> <pre><code>from precise_mrd.eval.lod import estimate_lob, estimate_lod, estimate_loq\nfrom precise_mrd.eval.stratified import run_stratified_analysis\nfrom precise_mrd.sim.contamination import run_contamination_stress_test\n\n# Quick detection limit estimation\nlob_results = estimate_lob(config, rng, n_blank_runs=50)\nlod_results = estimate_lod(config, rng, af_range=(1e-4, 1e-2))\nloq_results = estimate_loq(config, rng, cv_threshold=0.20)\n\n# Quick stratified analysis\npower_results, calib_results = run_stratified_analysis(config, rng)\n\n# Quick contamination testing\ncontam_results = run_contamination_stress_test(config, rng)\n</code></pre>"},{"location":"api-reference/#configuration-objects","title":"Configuration Objects","text":""},{"location":"api-reference/#pipelineconfig","title":"PipelineConfig","text":"<p>Main configuration object containing all analysis parameters:</p> <pre><code>@dataclass\nclass PipelineConfig:\n    run_id: str\n    seed: int\n    simulation: SimulationConfig\n    umi: UMIConfig\n    stats: StatsConfig\n    lod: LODConfig\n</code></pre>"},{"location":"api-reference/#simulationconfig","title":"SimulationConfig","text":"<p>Parameters for synthetic data generation:</p> <pre><code>@dataclass\nclass SimulationConfig:\n    allele_fractions: List[float]\n    umi_depths: List[int]\n    n_replicates: int\n    n_bootstrap: int\n</code></pre>"},{"location":"api-reference/#umiconfig","title":"UMIConfig","text":"<p>UMI processing parameters:</p> <pre><code>@dataclass\nclass UMIConfig:\n    min_family_size: int\n    max_family_size: int\n    quality_threshold: int\n    consensus_threshold: float\n</code></pre>"},{"location":"api-reference/#statsconfig","title":"StatsConfig","text":"<p>Statistical testing parameters:</p> <pre><code>@dataclass\nclass StatsConfig:\n    test_type: str\n    alpha: float\n    fdr_method: str\n</code></pre>"},{"location":"api-reference/#lodconfig","title":"LODConfig","text":"<p>Detection limit estimation parameters:</p> <pre><code>@dataclass\nclass LODConfig:\n    detection_threshold: float\n    confidence_level: float\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":"<p>The API uses custom exceptions for specific error conditions:</p> <pre><code>from precise_mrd.config import ConfigurationError\nfrom precise_mrd.eval.lod import DetectionLimitError\n\ntry:\n    config = load_config(\"invalid_config.yaml\")\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n\ntry:\n    lod_results = estimate_lod(config, rng, af_range=(1e-2, 1e-4))  # Invalid range\nexcept DetectionLimitError as e:\n    print(f\"Detection limit error: {e}\")\n</code></pre>"},{"location":"api-reference/#logging","title":"Logging","text":"<p>The API includes structured logging for debugging and monitoring:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('precise_mrd')\n\n# Run analysis with detailed logging\nanalyzer = LODAnalyzer(config, rng)\nlob_results = analyzer.estimate_lob(n_blank_runs=100)\n</code></pre>"},{"location":"api-reference/#thread-safety","title":"Thread Safety","text":"<p>All analysis functions are thread-safe when using independent random number generators:</p> <pre><code>import concurrent.futures\n\ndef run_parallel_analysis(configs_and_seeds):\n    \"\"\"Run multiple analyses in parallel.\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n\n        for config, seed in configs_and_seeds:\n            rng = np.random.default_rng(seed)  # Independent RNG per thread\n            future = executor.submit(estimate_lod, config, rng)\n            futures.append(future)\n\n        results = [future.result() for future in futures]\n\n    return results\n</code></pre>"},{"location":"api-reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api-reference/#memory-usage","title":"Memory Usage","text":"<ul> <li>LoB estimation: ~10MB for 100 blank runs</li> <li>LoD estimation: ~50MB for full AF grid (15 points \u00d7 3 depths \u00d7 50 reps)</li> <li>Contamination testing: ~100MB for complete stress test</li> <li>Stratified analysis: ~80MB for 4 contexts \u00d7 3 depths \u00d7 4 AFs</li> </ul>"},{"location":"api-reference/#runtime-estimates","title":"Runtime Estimates","text":"<p>On a standard CPU (Intel i5):</p> <ul> <li>Quick LoB (20 runs): ~5 seconds</li> <li>Quick LoD (reduced grid): ~15 seconds  </li> <li>Full LoD (complete grid): ~2 minutes</li> <li>Contamination stress test: ~3 minutes</li> <li>Stratified analysis: ~90 seconds</li> </ul>"},{"location":"api-reference/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Reduce Replicates: For development, use <code>n_replicates=10-15</code></li> <li>Limit AF Range: Test with narrower AF ranges first</li> <li>Parallel Execution: Use multiple processes for independent analyses</li> <li>Caching: Cache intermediate results for repeated analyses</li> </ol>"},{"location":"api-reference/#version-compatibility","title":"Version Compatibility","text":"<p>The API maintains backward compatibility within major versions:</p> <ul> <li>v1.x: Stable API, backward compatible updates</li> <li>Breaking changes: Only in major version updates (v2.0, etc.)</li> <li>Deprecation policy: 2 minor versions warning before removal</li> </ul> <p>Check version compatibility:</p> <pre><code>import precise_mrd\nprint(f\"Precise MRD version: {precise_mrd.__version__}\")\n\n# Check API compatibility\nif precise_mrd.__version__.startswith('1.'):\n    print(\"Compatible with v1.x API\")\n</code></pre>"},{"location":"contamination%202/","title":"Contamination Stress Testing","text":"<p>This page describes the comprehensive contamination robustness testing framework implemented in Precise MRD, covering index-hopping, barcode collisions, and cross-sample contamination effects.</p>"},{"location":"contamination%202/#overview","title":"Overview","text":"<p>Contamination in ctDNA/UMI sequencing can arise from multiple sources:</p> <ul> <li>Index Hopping: Misassignment of reads between samples due to free adapters</li> <li>Barcode Collisions: Multiple DNA fragments sharing the same UMI sequence</li> <li>Cross-Sample Contamination: Physical mixing of samples during processing</li> </ul> <p>Our stress testing framework systematically evaluates detection sensitivity under these contamination scenarios.</p>"},{"location":"contamination%202/#contamination-models","title":"Contamination Models","text":""},{"location":"contamination%202/#index-hopping-model","title":"Index Hopping Model","text":"<p>Index hopping occurs when free adapters in pooled libraries lead to misassignment of reads to incorrect samples.</p> <p>Parameters: - <code>hop_rate</code>: Fraction of reads that hop between samples (0.0 - 0.02) - <code>background_multiplier</code>: Error rate increase in hopped reads (1.5-2.0x)</p> <p>Implementation: <pre><code>def simulate_with_index_hopping(reads_df, rng, hop_rate):\n    n_reads = len(reads_df)\n    n_hopped = rng.binomial(n_reads, hop_rate)\n\n    if n_hopped &gt; 0:\n        # Create contaminating reads from other samples\n        contam_reads = reads_df.sample(n=n_hopped, random_state=rng).copy()\n\n        # Increase error rate for hopped reads\n        contam_reads['background_rate'] *= 2.0\n        contam_reads['n_false_positives'] *= 1.5\n\n        # Combine with original reads\n        reads_df = pd.concat([reads_df, contam_reads], ignore_index=True)\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: Increased false positive rate, potential for cross-contamination artifacts.</p>"},{"location":"contamination%202/#barcode-collision-model","title":"Barcode Collision Model","text":"<p>UMI collisions occur when different DNA molecules are assigned the same barcode sequence, leading to artifactual consensus formation.</p> <p>Parameters: - <code>collision_rate</code>: Probability of UMI collision per family (0.0 - 0.001) - <code>consensus_degradation</code>: Quality reduction factor (0.8x)</p> <p>Implementation: <pre><code>def simulate_with_barcode_collisions(reads_df, rng, collision_rate):\n    n_families = len(reads_df)\n    n_collisions = rng.binomial(n_families, collision_rate)\n\n    if n_collisions &gt; 0:\n        collision_indices = rng.choice(n_families, size=n_collisions, replace=False)\n\n        # Increase false positive rate for collided families\n        reads_df.loc[collision_indices, 'n_false_positives'] *= 2.0\n        reads_df.loc[collision_indices, 'background_rate'] *= 1.5\n\n        # Reduce consensus quality\n        reads_df.loc[collision_indices, 'mean_quality'] *= 0.8\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: Reduced consensus quality, increased error rates in affected UMI families.</p>"},{"location":"contamination%202/#cross-sample-contamination-model","title":"Cross-Sample Contamination Model","text":"<p>Physical mixing of samples during library preparation or sequencing.</p> <p>Parameters: - <code>contam_proportion</code>: Fraction of contaminating sample (0.0 - 0.1) - <code>contam_af_multiplier</code>: AF ratio of contaminating sample (typically 10x higher)</p> <p>Implementation: <pre><code>def simulate_with_cross_contamination(reads_df, rng, contam_proportion):\n    if contam_proportion &gt; 0:\n        n_reads = len(reads_df)\n        n_contam = int(n_reads * contam_proportion)\n\n        # Create contaminating sample with higher AF\n        contam_af = min(0.1, reads_df['allele_fraction'].iloc[0] * 10)\n        contam_reads = simulate_contaminating_sample(contam_af, rng)\n\n        # Mix with original sample\n        reads_df = pd.concat([reads_df, contam_reads[:n_contam]], ignore_index=True)\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: False positive variants from contaminating sample, detection sensitivity changes.</p>"},{"location":"contamination%202/#experimental-design","title":"Experimental Design","text":""},{"location":"contamination%202/#test-parameters","title":"Test Parameters","text":"<pre><code># Contamination stress test grid\nhop_rates = [0.0, 0.001, 0.002, 0.005, 0.01]           # 0-1% hopping\nbarcode_collision_rates = [0.0, 0.0001, 0.0005, 0.001] # 0-0.1% collisions  \ncross_sample_proportions = [0.0, 0.01, 0.05, 0.1]      # 0-10% mixing\naf_test_values = [0.001, 0.005, 0.01]                  # Representative AFs\ndepth_values = [1000, 5000]                            # Representative depths\nn_replicates = 20                                       # Per condition\n</code></pre>"},{"location":"contamination%202/#metrics-assessed","title":"Metrics Assessed","text":"<p>Detection Sensitivity: \\(\\(\\text{Sensitivity} = \\frac{\\text{Variants Detected}}{\\text{Expected Detections}}\\)\\)</p> <p>Where expected detections are estimated from the true AF and pipeline efficiency.</p> <p>False Positive Rate: \\(\\(\\text{FPR} = \\frac{\\text{Excess Detections}}{\\text{Total UMIs}}\\)\\)</p> <p>Where excess detections = max(0, detected - expected).</p> <p>Sensitivity Delta: \\(\\(\\Delta\\text{Sensitivity} = \\text{Sensitivity}_{\\text{contaminated}} - \\text{Sensitivity}_{\\text{clean}}\\)\\)</p>"},{"location":"contamination%202/#analysis-pipeline","title":"Analysis Pipeline","text":""},{"location":"contamination%202/#1-baseline-measurement","title":"1. Baseline Measurement","text":"<pre><code># Clean samples (no contamination)\nclean_config = create_test_config(af=0.005, depth=5000)\nclean_sensitivity = measure_detection_sensitivity(clean_config, n_reps=20)\n</code></pre>"},{"location":"contamination%202/#2-contamination-stress-testing","title":"2. Contamination Stress Testing","text":"<pre><code>for hop_rate in hop_rates:\n    for af in af_test_values:\n        for depth in depth_values:\n            # Test with contamination\n            contam_sensitivity = measure_contaminated_sensitivity(\n                af, depth, hop_rate, n_reps=20\n            )\n\n            # Calculate impact\n            sensitivity_delta = contam_sensitivity - clean_sensitivity\n            results[hop_rate][af][depth] = {\n                'sensitivity': contam_sensitivity,\n                'sensitivity_delta': sensitivity_delta,\n                'n_replicates': 20\n            }\n</code></pre>"},{"location":"contamination%202/#3-impact-assessment","title":"3. Impact Assessment","text":"<pre><code># Flag significant sensitivity loss\nfor condition, result in results.items():\n    if result['sensitivity_delta'] &lt; -0.05:  # &gt;5% loss\n        warnings.append(f\"Significant sensitivity loss at {condition}\")\n</code></pre>"},{"location":"contamination%202/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"contamination%202/#significance-testing","title":"Significance Testing","text":"<p>Sensitivity differences are assessed using paired t-tests:</p> <pre><code>from scipy import stats\n\ndef test_contamination_impact(clean_scores, contam_scores, alpha=0.05):\n    \"\"\"Test if contamination significantly impacts detection.\"\"\"\n    statistic, p_value = stats.ttest_rel(clean_scores, contam_scores)\n\n    effect_size = (np.mean(contam_scores) - np.mean(clean_scores)) / np.std(clean_scores)\n\n    return {\n        'p_value': p_value,\n        'significant': p_value &lt; alpha,\n        'effect_size': effect_size,\n        'sensitivity_change': np.mean(contam_scores) - np.mean(clean_scores)\n    }\n</code></pre>"},{"location":"contamination%202/#regression-analysis","title":"Regression Analysis","text":"<p>Dose-response relationships are modeled using logistic regression:</p> <pre><code># Model: sensitivity ~ log(contamination_rate) + AF + depth\ndef fit_contamination_model(results_df):\n    from sklearn.linear_model import LogisticRegression\n\n    X = results_df[['log_contam_rate', 'af', 'depth']]\n    y = results_df['sensitivity'] &gt; 0.9  # Binary: good sensitivity\n\n    model = LogisticRegression().fit(X, y)\n    return model\n</code></pre>"},{"location":"contamination%202/#expected-performance","title":"Expected Performance","text":"<p>Based on simulation studies:</p>"},{"location":"contamination%202/#index-hopping-tolerance","title":"Index Hopping Tolerance","text":"Hop Rate AF=0.001 AF=0.005 AF=0.01 0.0% 0.85 0.95 0.98 0.1% 0.84 0.94 0.97 0.5% 0.82 0.92 0.96 1.0% 0.79 0.89 0.94 <p>Values represent detection sensitivity (fraction of true positives detected)</p>"},{"location":"contamination%202/#barcode-collision-impact","title":"Barcode Collision Impact","text":"Collision Rate FPR Increase Sensitivity Loss 0.01% +0.05% -1% 0.05% +0.12% -3% 0.10% +0.25% -5%"},{"location":"contamination%202/#cross-contamination-threshold","title":"Cross-Contamination Threshold","text":"<ul> <li>&lt;1% mixing: Minimal impact (&lt;2% sensitivity change)</li> <li>1-5% mixing: Moderate impact (2-10% sensitivity change)  </li> <li>&gt;5% mixing: Significant impact (&gt;10% sensitivity change)</li> </ul>"},{"location":"contamination%202/#validation-framework","title":"Validation Framework","text":""},{"location":"contamination%202/#ci-integration","title":"CI Integration","text":"<p>Contamination tests include fast sanity checks for CI:</p> <pre><code>def test_contamination_sanity():\n    \"\"\"Quick contamination impact check for CI.\"\"\"\n    # Test minimal contamination scenario\n    hop_rate = 0.001  # 0.1% hopping\n    af = 0.005       # Moderate AF\n    depth = 5000     # Standard depth\n\n    clean_sens = measure_clean_sensitivity(af, depth, n_reps=5)\n    contam_sens = measure_contaminated_sensitivity(af, depth, hop_rate, n_reps=5)\n\n    # Should not lose &gt;5% sensitivity at low contamination\n    assert (clean_sens - contam_sens) &lt; 0.05, \"Excess sensitivity loss under minimal contamination\"\n</code></pre>"},{"location":"contamination%202/#regression-detection","title":"Regression Detection","text":"<p>Monitor for unexpected sensitivity changes:</p> <pre><code>def detect_contamination_regression(current_results, reference_results):\n    \"\"\"Detect regression in contamination tolerance.\"\"\"\n    for condition in current_results:\n        current_sens = current_results[condition]['sensitivity']\n        reference_sens = reference_results[condition]['sensitivity']\n\n        if (reference_sens - current_sens) &gt; 0.03:  # &gt;3% regression\n            raise AssertionError(f\"Contamination regression detected at {condition}\")\n</code></pre>"},{"location":"contamination%202/#artifacts-generated","title":"Artifacts Generated","text":""},{"location":"contamination%202/#sensitivity-results-reportscontam_sensitivityjson","title":"Sensitivity Results (<code>reports/contam_sensitivity.json</code>)","text":"<pre><code>{\n  \"index_hopping\": {\n    \"0.001\": {\n      \"0.005\": {\n        \"5000\": {\n          \"mean_sensitivity\": 0.94,\n          \"std_sensitivity\": 0.03,\n          \"sensitivity_scores\": [0.91, 0.95, 0.93, ...],\n          \"n_replicates\": 20\n        }\n      }\n    }\n  },\n  \"barcode_collisions\": { ... },\n  \"cross_sample_contamination\": { ... }\n}\n</code></pre>"},{"location":"contamination%202/#heatmap-visualization-reportscontam_heatmappng","title":"Heatmap Visualization (<code>reports/contam_heatmap.png</code>)","text":"<p>Visual representation showing: - X-axis: Test conditions (AF, Depth combinations) - Y-axis: Contamination levels (hop rates, collision rates, etc.) - Color scale: Detection sensitivity (0.0 - 1.0) - Annotations: Sensitivity values in each cell</p>"},{"location":"contamination%202/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Generate contamination summary\nsummary = {\n    'max_tolerable_hop_rate': 0.005,      # 0.5% before &gt;5% sensitivity loss\n    'max_collision_rate': 0.0005,         # 0.05% before significant impact\n    'max_cross_contamination': 0.05,      # 5% before major degradation\n    'overall_robustness_score': 0.85      # Weighted average across conditions\n}\n</code></pre>"},{"location":"contamination%202/#cli-usage","title":"CLI Usage","text":"<p>Run contamination stress testing:</p> <pre><code># Quick stress test (reduced grid for speed)\nprecise-mrd eval-contamination --quick --seed 7\n\n# Full stress test \nprecise-mrd eval-contamination \\\n    --hop-rates 0.0,0.001,0.002,0.005,0.01 \\\n    --collision-rates 0.0,0.0001,0.0005,0.001 \\\n    --cross-contam 0.0,0.01,0.05,0.1 \\\n    --af-values 0.001,0.005,0.01 \\\n    --depths 1000,5000 \\\n    --n-replicates 20 \\\n    --seed 7\n\n# CI-friendly sanity check\nprecise-mrd eval-contamination --sanity-only --seed 7\n</code></pre> <p>Integration with Makefile: <pre><code>make contam-stress    # Full contamination testing\nmake contam-sanity    # Quick sanity check for CI\n</code></pre></p>"},{"location":"contamination/","title":"Contamination Stress Testing","text":"<p>This page describes the comprehensive contamination robustness testing framework implemented in Precise MRD, covering index-hopping, barcode collisions, and cross-sample contamination effects.</p>"},{"location":"contamination/#overview","title":"Overview","text":"<p>Contamination in ctDNA/UMI sequencing can arise from multiple sources:</p> <ul> <li>Index Hopping: Misassignment of reads between samples due to free adapters</li> <li>Barcode Collisions: Multiple DNA fragments sharing the same UMI sequence</li> <li>Cross-Sample Contamination: Physical mixing of samples during processing</li> </ul> <p>Our stress testing framework systematically evaluates detection sensitivity under these contamination scenarios.</p>"},{"location":"contamination/#contamination-models","title":"Contamination Models","text":""},{"location":"contamination/#index-hopping-model","title":"Index Hopping Model","text":"<p>Index hopping occurs when free adapters in pooled libraries lead to misassignment of reads to incorrect samples.</p> <p>Parameters: - <code>hop_rate</code>: Fraction of reads that hop between samples (0.0 - 0.02) - <code>background_multiplier</code>: Error rate increase in hopped reads (1.5-2.0x)</p> <p>Implementation: <pre><code>def simulate_with_index_hopping(reads_df, rng, hop_rate):\n    n_reads = len(reads_df)\n    n_hopped = rng.binomial(n_reads, hop_rate)\n\n    if n_hopped &gt; 0:\n        # Create contaminating reads from other samples\n        contam_reads = reads_df.sample(n=n_hopped, random_state=rng).copy()\n\n        # Increase error rate for hopped reads\n        contam_reads['background_rate'] *= 2.0\n        contam_reads['n_false_positives'] *= 1.5\n\n        # Combine with original reads\n        reads_df = pd.concat([reads_df, contam_reads], ignore_index=True)\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: Increased false positive rate, potential for cross-contamination artifacts.</p>"},{"location":"contamination/#barcode-collision-model","title":"Barcode Collision Model","text":"<p>UMI collisions occur when different DNA molecules are assigned the same barcode sequence, leading to artifactual consensus formation.</p> <p>Parameters: - <code>collision_rate</code>: Probability of UMI collision per family (0.0 - 0.001) - <code>consensus_degradation</code>: Quality reduction factor (0.8x)</p> <p>Implementation: <pre><code>def simulate_with_barcode_collisions(reads_df, rng, collision_rate):\n    n_families = len(reads_df)\n    n_collisions = rng.binomial(n_families, collision_rate)\n\n    if n_collisions &gt; 0:\n        collision_indices = rng.choice(n_families, size=n_collisions, replace=False)\n\n        # Increase false positive rate for collided families\n        reads_df.loc[collision_indices, 'n_false_positives'] *= 2.0\n        reads_df.loc[collision_indices, 'background_rate'] *= 1.5\n\n        # Reduce consensus quality\n        reads_df.loc[collision_indices, 'mean_quality'] *= 0.8\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: Reduced consensus quality, increased error rates in affected UMI families.</p>"},{"location":"contamination/#cross-sample-contamination-model","title":"Cross-Sample Contamination Model","text":"<p>Physical mixing of samples during library preparation or sequencing.</p> <p>Parameters: - <code>contam_proportion</code>: Fraction of contaminating sample (0.0 - 0.1) - <code>contam_af_multiplier</code>: AF ratio of contaminating sample (typically 10x higher)</p> <p>Implementation: <pre><code>def simulate_with_cross_contamination(reads_df, rng, contam_proportion):\n    if contam_proportion &gt; 0:\n        n_reads = len(reads_df)\n        n_contam = int(n_reads * contam_proportion)\n\n        # Create contaminating sample with higher AF\n        contam_af = min(0.1, reads_df['allele_fraction'].iloc[0] * 10)\n        contam_reads = simulate_contaminating_sample(contam_af, rng)\n\n        # Mix with original sample\n        reads_df = pd.concat([reads_df, contam_reads[:n_contam]], ignore_index=True)\n\n    return reads_df\n</code></pre></p> <p>Expected Impact: False positive variants from contaminating sample, detection sensitivity changes.</p>"},{"location":"contamination/#experimental-design","title":"Experimental Design","text":""},{"location":"contamination/#test-parameters","title":"Test Parameters","text":"<pre><code># Contamination stress test grid\nhop_rates = [0.0, 0.001, 0.002, 0.005, 0.01]           # 0-1% hopping\nbarcode_collision_rates = [0.0, 0.0001, 0.0005, 0.001] # 0-0.1% collisions  \ncross_sample_proportions = [0.0, 0.01, 0.05, 0.1]      # 0-10% mixing\naf_test_values = [0.001, 0.005, 0.01]                  # Representative AFs\ndepth_values = [1000, 5000]                            # Representative depths\nn_replicates = 20                                       # Per condition\n</code></pre>"},{"location":"contamination/#metrics-assessed","title":"Metrics Assessed","text":"<p>Detection Sensitivity: \\(\\(\\text{Sensitivity} = \\frac{\\text{Variants Detected}}{\\text{Expected Detections}}\\)\\)</p> <p>Where expected detections are estimated from the true AF and pipeline efficiency.</p> <p>False Positive Rate: \\(\\(\\text{FPR} = \\frac{\\text{Excess Detections}}{\\text{Total UMIs}}\\)\\)</p> <p>Where excess detections = max(0, detected - expected).</p> <p>Sensitivity Delta: \\(\\(\\Delta\\text{Sensitivity} = \\text{Sensitivity}_{\\text{contaminated}} - \\text{Sensitivity}_{\\text{clean}}\\)\\)</p>"},{"location":"contamination/#analysis-pipeline","title":"Analysis Pipeline","text":""},{"location":"contamination/#1-baseline-measurement","title":"1. Baseline Measurement","text":"<pre><code># Clean samples (no contamination)\nclean_config = create_test_config(af=0.005, depth=5000)\nclean_sensitivity = measure_detection_sensitivity(clean_config, n_reps=20)\n</code></pre>"},{"location":"contamination/#2-contamination-stress-testing","title":"2. Contamination Stress Testing","text":"<pre><code>for hop_rate in hop_rates:\n    for af in af_test_values:\n        for depth in depth_values:\n            # Test with contamination\n            contam_sensitivity = measure_contaminated_sensitivity(\n                af, depth, hop_rate, n_reps=20\n            )\n\n            # Calculate impact\n            sensitivity_delta = contam_sensitivity - clean_sensitivity\n            results[hop_rate][af][depth] = {\n                'sensitivity': contam_sensitivity,\n                'sensitivity_delta': sensitivity_delta,\n                'n_replicates': 20\n            }\n</code></pre>"},{"location":"contamination/#3-impact-assessment","title":"3. Impact Assessment","text":"<pre><code># Flag significant sensitivity loss\nfor condition, result in results.items():\n    if result['sensitivity_delta'] &lt; -0.05:  # &gt;5% loss\n        warnings.append(f\"Significant sensitivity loss at {condition}\")\n</code></pre>"},{"location":"contamination/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"contamination/#significance-testing","title":"Significance Testing","text":"<p>Sensitivity differences are assessed using paired t-tests:</p> <pre><code>from scipy import stats\n\ndef test_contamination_impact(clean_scores, contam_scores, alpha=0.05):\n    \"\"\"Test if contamination significantly impacts detection.\"\"\"\n    statistic, p_value = stats.ttest_rel(clean_scores, contam_scores)\n\n    effect_size = (np.mean(contam_scores) - np.mean(clean_scores)) / np.std(clean_scores)\n\n    return {\n        'p_value': p_value,\n        'significant': p_value &lt; alpha,\n        'effect_size': effect_size,\n        'sensitivity_change': np.mean(contam_scores) - np.mean(clean_scores)\n    }\n</code></pre>"},{"location":"contamination/#regression-analysis","title":"Regression Analysis","text":"<p>Dose-response relationships are modeled using logistic regression:</p> <pre><code># Model: sensitivity ~ log(contamination_rate) + AF + depth\ndef fit_contamination_model(results_df):\n    from sklearn.linear_model import LogisticRegression\n\n    X = results_df[['log_contam_rate', 'af', 'depth']]\n    y = results_df['sensitivity'] &gt; 0.9  # Binary: good sensitivity\n\n    model = LogisticRegression().fit(X, y)\n    return model\n</code></pre>"},{"location":"contamination/#expected-performance","title":"Expected Performance","text":"<p>Based on simulation studies:</p>"},{"location":"contamination/#index-hopping-tolerance","title":"Index Hopping Tolerance","text":"Hop Rate AF=0.001 AF=0.005 AF=0.01 0.0% 0.85 0.95 0.98 0.1% 0.84 0.94 0.97 0.5% 0.82 0.92 0.96 1.0% 0.79 0.89 0.94 <p>Values represent detection sensitivity (fraction of true positives detected)</p>"},{"location":"contamination/#barcode-collision-impact","title":"Barcode Collision Impact","text":"Collision Rate FPR Increase Sensitivity Loss 0.01% +0.05% -1% 0.05% +0.12% -3% 0.10% +0.25% -5%"},{"location":"contamination/#cross-contamination-threshold","title":"Cross-Contamination Threshold","text":"<ul> <li>&lt;1% mixing: Minimal impact (&lt;2% sensitivity change)</li> <li>1-5% mixing: Moderate impact (2-10% sensitivity change)  </li> <li>&gt;5% mixing: Significant impact (&gt;10% sensitivity change)</li> </ul>"},{"location":"contamination/#validation-framework","title":"Validation Framework","text":""},{"location":"contamination/#ci-integration","title":"CI Integration","text":"<p>Contamination tests include fast sanity checks for CI:</p> <pre><code>def test_contamination_sanity():\n    \"\"\"Quick contamination impact check for CI.\"\"\"\n    # Test minimal contamination scenario\n    hop_rate = 0.001  # 0.1% hopping\n    af = 0.005       # Moderate AF\n    depth = 5000     # Standard depth\n\n    clean_sens = measure_clean_sensitivity(af, depth, n_reps=5)\n    contam_sens = measure_contaminated_sensitivity(af, depth, hop_rate, n_reps=5)\n\n    # Should not lose &gt;5% sensitivity at low contamination\n    assert (clean_sens - contam_sens) &lt; 0.05, \"Excess sensitivity loss under minimal contamination\"\n</code></pre>"},{"location":"contamination/#regression-detection","title":"Regression Detection","text":"<p>Monitor for unexpected sensitivity changes:</p> <pre><code>def detect_contamination_regression(current_results, reference_results):\n    \"\"\"Detect regression in contamination tolerance.\"\"\"\n    for condition in current_results:\n        current_sens = current_results[condition]['sensitivity']\n        reference_sens = reference_results[condition]['sensitivity']\n\n        if (reference_sens - current_sens) &gt; 0.03:  # &gt;3% regression\n            raise AssertionError(f\"Contamination regression detected at {condition}\")\n</code></pre>"},{"location":"contamination/#artifacts-generated","title":"Artifacts Generated","text":""},{"location":"contamination/#sensitivity-results-reportscontam_sensitivityjson","title":"Sensitivity Results (<code>reports/contam_sensitivity.json</code>)","text":"<pre><code>{\n  \"index_hopping\": {\n    \"0.001\": {\n      \"0.005\": {\n        \"5000\": {\n          \"mean_sensitivity\": 0.94,\n          \"std_sensitivity\": 0.03,\n          \"sensitivity_scores\": [0.91, 0.95, 0.93, ...],\n          \"n_replicates\": 20\n        }\n      }\n    }\n  },\n  \"barcode_collisions\": { ... },\n  \"cross_sample_contamination\": { ... }\n}\n</code></pre>"},{"location":"contamination/#heatmap-visualization-reportscontam_heatmappng","title":"Heatmap Visualization (<code>reports/contam_heatmap.png</code>)","text":"<p>Visual representation showing: - X-axis: Test conditions (AF, Depth combinations) - Y-axis: Contamination levels (hop rates, collision rates, etc.) - Color scale: Detection sensitivity (0.0 - 1.0) - Annotations: Sensitivity values in each cell</p>"},{"location":"contamination/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Generate contamination summary\nsummary = {\n    'max_tolerable_hop_rate': 0.005,      # 0.5% before &gt;5% sensitivity loss\n    'max_collision_rate': 0.0005,         # 0.05% before significant impact\n    'max_cross_contamination': 0.05,      # 5% before major degradation\n    'overall_robustness_score': 0.85      # Weighted average across conditions\n}\n</code></pre>"},{"location":"contamination/#cli-usage","title":"CLI Usage","text":"<p>Run contamination stress testing:</p> <pre><code># Quick stress test (reduced grid for speed)\nprecise-mrd eval-contamination --quick --seed 7\n\n# Full stress test \nprecise-mrd eval-contamination \\\n    --hop-rates 0.0,0.001,0.002,0.005,0.01 \\\n    --collision-rates 0.0,0.0001,0.0005,0.001 \\\n    --cross-contam 0.0,0.01,0.05,0.1 \\\n    --af-values 0.001,0.005,0.01 \\\n    --depths 1000,5000 \\\n    --n-replicates 20 \\\n    --seed 7\n\n# CI-friendly sanity check\nprecise-mrd eval-contamination --sanity-only --seed 7\n</code></pre> <p>Integration with Makefile: <pre><code>make contam-stress    # Full contamination testing\nmake contam-sanity    # Quick sanity check for CI\n</code></pre></p>"},{"location":"evaluation%202/","title":"Detection Limit Evaluation","text":"<p>This page provides detailed information about the formal detection limit analytics implemented in Precise MRD, following CLSI EP17 guidelines for clinical detection capability.</p>"},{"location":"evaluation%202/#overview","title":"Overview","text":"<p>Detection limits are fundamental analytical performance characteristics that define:</p> <ul> <li>Limit of Blank (LoB): The highest measurement result likely to be observed for a blank specimen</li> <li>Limit of Detection (LoD): The lowest analyte concentration likely to be reliably detected  </li> <li>Limit of Quantification (LoQ): The lowest concentration at which quantitative measurements can be made with acceptable precision</li> </ul>"},{"location":"evaluation%202/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"evaluation%202/#limit-of-blank-lob","title":"Limit of Blank (LoB)","text":"<p>LoB represents the 95th percentile of measurements from blank specimens:</p> \\[\\text{LoB} = \\mu_{\\text{blank}} + 1.645 \\times \\sigma_{\\text{blank}}\\] <p>Where: - \\(\\mu_{\\text{blank}}\\) = mean of blank measurements - \\(\\sigma_{\\text{blank}}\\) = standard deviation of blank measurements - \\(1.645\\) = 95th percentile of standard normal distribution</p> <p>Implementation: Run \\(N = 100\\) blank simulations (AF = 0) and compute the 95th percentile of variant call counts.</p>"},{"location":"evaluation%202/#limit-of-detection-lod","title":"Limit of Detection (LoD)","text":"<p>LoD is the concentration yielding 95% detection probability with controlled Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) error rates:</p> \\[P(\\text{Detection} | \\text{AF}) = \\frac{1}{1 + e^{-(a \\log(\\text{AF}) + b)}} = 0.95\\] <p>Where the logistic parameters \\((a, b)\\) are fitted to observed detection rates across an AF grid.</p> <p>Implementation: - Test AF range: \\(10^{-4}\\) to \\(10^{-2}\\) (log-spaced, 15 points) - Depths: 1K, 5K, 10K UMIs - Replicates: 50 per AF/depth combination - Fit logistic curve: \\(\\text{hit\\_rate} \\sim \\text{logistic}(\\log(\\text{AF}))\\) - Solve for AF yielding 95% detection probability</p>"},{"location":"evaluation%202/#limit-of-quantification-loq","title":"Limit of Quantification (LoQ)","text":"<p>LoQ is the lowest AF meeting precision criteria:</p> \\[\\text{CV} = \\frac{\\sigma_{\\hat{\\text{AF}}}}{\\mu_{\\hat{\\text{AF}}}} \\leq 0.20\\] <p>Or alternatively with absolute error threshold:</p> \\[|\\mu_{\\hat{\\text{AF}}} - \\text{AF}_{\\text{true}}| \\leq \\epsilon\\] <p>Implementation: For each AF, estimate coefficient of variation from 50 replicates and find lowest AF meeting CV \u2264 20%.</p>"},{"location":"evaluation%202/#experimental-design","title":"Experimental Design","text":""},{"location":"evaluation%202/#blank-studies-lob","title":"Blank Studies (LoB)","text":"<pre><code># Configuration for blank studies\nblank_config = {\n    'allele_fractions': [0.0],      # Pure blank\n    'umi_depths': [5000],           # Representative depth\n    'n_replicates': 100,            # Sufficient for 95th percentile\n    'seed': 7                       # Deterministic\n}\n</code></pre> <p>Process: 1. Simulate 100 blank samples (AF = 0) 2. Run full pipeline: simulate \u2192 collapse \u2192 error_model \u2192 call 3. Count variant calls per blank run 4. Compute 95th percentile of call counts</p>"},{"location":"evaluation%202/#detection-studies-lod","title":"Detection Studies (LoD)","text":"<pre><code># AF grid for LoD estimation\naf_values = np.logspace(-4, -2, 15)  # 1e-4 to 1e-2\ndepth_values = [1000, 5000, 10000]   # Representative depths\nn_replicates = 50                    # Per AF/depth combination\n</code></pre> <p>Process: 1. For each AF/depth combination:    - Run 50 replicate simulations    - Count successful detections (variant calls &gt; 0)    - Calculate hit rate = detections / replicates 2. Fit logistic regression: \\(\\text{logit}(\\text{hit\\_rate}) = a \\log(\\text{AF}) + b\\) 3. Solve for LoD: \\(\\text{AF}_{\\text{LoD}} = \\exp\\left(\\frac{\\text{logit}(0.95) - b}{a}\\right)\\)</p>"},{"location":"evaluation%202/#quantification-studies-loq","title":"Quantification Studies (LoQ)","text":"<pre><code># Precision assessment grid\naf_values = np.logspace(-4, -2, 12)  # Subset for efficiency\ncv_threshold = 0.20                  # 20% coefficient of variation\nn_replicates = 50                    # For CV estimation\n</code></pre> <p>Process: 1. For each AF/depth combination:    - Run 50 replicate simulations    - Estimate AF from variant calls: \\(\\hat{\\text{AF}} = \\frac{\\text{variants}}{\\text{total\\_UMIs}}\\)    - Calculate CV: \\(\\text{CV} = \\frac{\\text{std}(\\hat{\\text{AF}})}{\\text{mean}(\\hat{\\text{AF}})}\\) 2. Find lowest AF where CV \u2264 20%</p>"},{"location":"evaluation%202/#statistical-considerations","title":"Statistical Considerations","text":""},{"location":"evaluation%202/#confidence-intervals","title":"Confidence Intervals","text":"<p>LoD confidence intervals are computed using stratified bootstrap:</p> <pre><code>def bootstrap_lod_ci(af_values, hit_rates, target_rate=0.95, n_bootstrap=200):\n    bootstrap_lods = []\n    for _ in range(n_bootstrap):\n        # Resample with replacement\n        indices = rng.choice(len(af_values), size=len(af_values), replace=True)\n        boot_af = [af_values[i] for i in indices]\n        boot_hit = [hit_rates[i] for i in indices]\n\n        # Fit curve and solve for LoD\n        boot_lod = fit_detection_curve(boot_af, boot_hit, target_rate)\n        bootstrap_lods.append(boot_lod)\n\n    # 95% confidence interval\n    ci_lower = np.percentile(bootstrap_lods, 2.5)\n    ci_upper = np.percentile(bootstrap_lods, 97.5)\n    return ci_lower, ci_upper\n</code></pre>"},{"location":"evaluation%202/#bias-correction","title":"Bias Correction","text":"<p>Detection curves may exhibit bias due to: - Small sample effects at low AFs - Pipeline efficiency variations - Context-dependent error rates</p> <p>Bootstrap resampling provides bias-corrected estimates by accounting for sampling variability.</p>"},{"location":"evaluation%202/#validation-criteria","title":"Validation Criteria","text":""},{"location":"evaluation%202/#consistency-checks","title":"Consistency Checks","text":"<p>LoB &lt; LoD Relationship:  Detection limit must exceed blank variability: <pre><code>assert lob_value &lt; lod_value, \"LoB must be less than LoD\"\n</code></pre></p> <p>LoD Monotonicity: Detection limits should decrease with increasing depth: <pre><code>for i in range(len(depths) - 1):\n    assert lod_values[i] &gt;= lod_values[i+1], \"LoD should decrease with depth\"\n</code></pre></p> <p>LoQ \u2265 LoD: Quantification limits should exceed detection limits: <pre><code>assert loq_value &gt;= lod_value, \"LoQ must be greater than or equal to LoD\"\n</code></pre></p>"},{"location":"evaluation%202/#expected-performance","title":"Expected Performance","text":"<p>Based on simulation studies with the current error model:</p> Depth LoB (calls) LoD (AF) LoD CI LoQ (AF) 1K 2.1 8.5e-3 [6.2e-3, 1.1e-2] 1.2e-2 5K 3.8 2.1e-3 [1.8e-3, 2.5e-3] 3.8e-3 10K 5.2 1.1e-3 [0.9e-3, 1.4e-3] 1.9e-3 <p>Note: Values may vary based on error model parameters and trinucleotide context</p>"},{"location":"evaluation%202/#artifacts-generated","title":"Artifacts Generated","text":""},{"location":"evaluation%202/#lob-results-reportslobjson","title":"LoB Results (<code>reports/lob.json</code>)","text":"<pre><code>{\n  \"lob_value\": 2.1,\n  \"blank_mean\": 1.3,\n  \"blank_std\": 0.8,\n  \"blank_measurements\": [0, 1, 2, ...],\n  \"n_blank_runs\": 100,\n  \"percentile\": 95\n}\n</code></pre>"},{"location":"evaluation%202/#lod-results-reportslod_tablecsv","title":"LoD Results (<code>reports/lod_table.csv</code>)","text":"<pre><code>depth,lod_af,lod_ci_lower,lod_ci_upper,target_detection_rate,n_replicates\n1000,8.5e-3,6.2e-3,1.1e-2,0.95,50\n5000,2.1e-3,1.8e-3,2.5e-3,0.95,50\n10000,1.1e-3,0.9e-3,1.4e-3,0.95,50\n</code></pre>"},{"location":"evaluation%202/#loq-results-reportsloq_tablecsv","title":"LoQ Results (<code>reports/loq_table.csv</code>)","text":"<pre><code>depth,loq_af_cv,loq_af_abs_error,cv_threshold,abs_error_threshold,n_replicates\n1000,1.2e-2,null,0.20,null,50\n5000,3.8e-3,null,0.20,null,50\n10000,1.9e-3,null,0.20,null,50\n</code></pre>"},{"location":"evaluation%202/#detection-curves-reportslod_curvespng","title":"Detection Curves (<code>reports/lod_curves.png</code>)","text":"<p>Visualization showing: - Observed detection rates vs. AF (per depth) - Fitted logistic curves - LoD markers with confidence intervals - Target detection rate (95%) line</p>"},{"location":"evaluation%202/#integration-with-pipeline","title":"Integration with Pipeline","text":"<p>Detection limit estimation is integrated into the main pipeline via CLI commands:</p> <pre><code># Individual analyses\nprecise-mrd eval-lob --n-blank-runs 100 --seed 7\nprecise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7\nprecise-mrd eval-loq --cv-threshold 0.20 --seed 7\n\n# Combined analysis\nprecise-mrd eval-all-limits --seed 7\n</code></pre> <p>All analyses are: - Deterministic: Seeded for reproducibility - Fast: Optimized for CI/CD integration - Validated: Include sanity checks and consistency tests - Documented: Complete metadata and run context</p>"},{"location":"evaluation/","title":"Detection Limit Evaluation","text":"<p>This page provides detailed information about the formal detection limit analytics implemented in Precise MRD, following CLSI EP17 guidelines for clinical detection capability.</p>"},{"location":"evaluation/#overview","title":"Overview","text":"<p>Detection limits are fundamental analytical performance characteristics that define:</p> <ul> <li>Limit of Blank (LoB): The highest measurement result likely to be observed for a blank specimen</li> <li>Limit of Detection (LoD): The lowest analyte concentration likely to be reliably detected  </li> <li>Limit of Quantification (LoQ): The lowest concentration at which quantitative measurements can be made with acceptable precision</li> </ul>"},{"location":"evaluation/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"evaluation/#limit-of-blank-lob","title":"Limit of Blank (LoB)","text":"<p>LoB represents the 95th percentile of measurements from blank specimens:</p> \\[\\text{LoB} = \\mu_{\\text{blank}} + 1.645 \\times \\sigma_{\\text{blank}}\\] <p>Where: - \\(\\mu_{\\text{blank}}\\) = mean of blank measurements - \\(\\sigma_{\\text{blank}}\\) = standard deviation of blank measurements - \\(1.645\\) = 95th percentile of standard normal distribution</p> <p>Implementation: Run \\(N = 100\\) blank simulations (AF = 0) and compute the 95th percentile of variant call counts.</p>"},{"location":"evaluation/#limit-of-detection-lod","title":"Limit of Detection (LoD)","text":"<p>LoD is the concentration yielding 95% detection probability with controlled Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) error rates:</p> \\[P(\\text{Detection} | \\text{AF}) = \\frac{1}{1 + e^{-(a \\log(\\text{AF}) + b)}} = 0.95\\] <p>Where the logistic parameters \\((a, b)\\) are fitted to observed detection rates across an AF grid.</p> <p>Implementation: - Test AF range: \\(10^{-4}\\) to \\(10^{-2}\\) (log-spaced, 15 points) - Depths: 1K, 5K, 10K UMIs - Replicates: 50 per AF/depth combination - Fit logistic curve: \\(\\text{hit\\_rate} \\sim \\text{logistic}(\\log(\\text{AF}))\\) - Solve for AF yielding 95% detection probability</p>"},{"location":"evaluation/#limit-of-quantification-loq","title":"Limit of Quantification (LoQ)","text":"<p>LoQ is the lowest AF meeting precision criteria:</p> \\[\\text{CV} = \\frac{\\sigma_{\\hat{\\text{AF}}}}{\\mu_{\\hat{\\text{AF}}}} \\leq 0.20\\] <p>Or alternatively with absolute error threshold:</p> \\[|\\mu_{\\hat{\\text{AF}}} - \\text{AF}_{\\text{true}}| \\leq \\epsilon\\] <p>Implementation: For each AF, estimate coefficient of variation from 50 replicates and find lowest AF meeting CV \u2264 20%.</p>"},{"location":"evaluation/#experimental-design","title":"Experimental Design","text":""},{"location":"evaluation/#blank-studies-lob","title":"Blank Studies (LoB)","text":"<pre><code># Configuration for blank studies\nblank_config = {\n    'allele_fractions': [0.0],      # Pure blank\n    'umi_depths': [5000],           # Representative depth\n    'n_replicates': 100,            # Sufficient for 95th percentile\n    'seed': 7                       # Deterministic\n}\n</code></pre> <p>Process: 1. Simulate 100 blank samples (AF = 0) 2. Run full pipeline: simulate \u2192 collapse \u2192 error_model \u2192 call 3. Count variant calls per blank run 4. Compute 95th percentile of call counts</p>"},{"location":"evaluation/#detection-studies-lod","title":"Detection Studies (LoD)","text":"<pre><code># AF grid for LoD estimation\naf_values = np.logspace(-4, -2, 15)  # 1e-4 to 1e-2\ndepth_values = [1000, 5000, 10000]   # Representative depths\nn_replicates = 50                    # Per AF/depth combination\n</code></pre> <p>Process: 1. For each AF/depth combination:    - Run 50 replicate simulations    - Count successful detections (variant calls &gt; 0)    - Calculate hit rate = detections / replicates 2. Fit logistic regression: \\(\\text{logit}(\\text{hit\\_rate}) = a \\log(\\text{AF}) + b\\) 3. Solve for LoD: \\(\\text{AF}_{\\text{LoD}} = \\exp\\left(\\frac{\\text{logit}(0.95) - b}{a}\\right)\\)</p>"},{"location":"evaluation/#quantification-studies-loq","title":"Quantification Studies (LoQ)","text":"<pre><code># Precision assessment grid\naf_values = np.logspace(-4, -2, 12)  # Subset for efficiency\ncv_threshold = 0.20                  # 20% coefficient of variation\nn_replicates = 50                    # For CV estimation\n</code></pre> <p>Process: 1. For each AF/depth combination:    - Run 50 replicate simulations    - Estimate AF from variant calls: \\(\\hat{\\text{AF}} = \\frac{\\text{variants}}{\\text{total\\_UMIs}}\\)    - Calculate CV: \\(\\text{CV} = \\frac{\\text{std}(\\hat{\\text{AF}})}{\\text{mean}(\\hat{\\text{AF}})}\\) 2. Find lowest AF where CV \u2264 20%</p>"},{"location":"evaluation/#statistical-considerations","title":"Statistical Considerations","text":""},{"location":"evaluation/#confidence-intervals","title":"Confidence Intervals","text":"<p>LoD confidence intervals are computed using stratified bootstrap:</p> <pre><code>def bootstrap_lod_ci(af_values, hit_rates, target_rate=0.95, n_bootstrap=200):\n    bootstrap_lods = []\n    for _ in range(n_bootstrap):\n        # Resample with replacement\n        indices = rng.choice(len(af_values), size=len(af_values), replace=True)\n        boot_af = [af_values[i] for i in indices]\n        boot_hit = [hit_rates[i] for i in indices]\n\n        # Fit curve and solve for LoD\n        boot_lod = fit_detection_curve(boot_af, boot_hit, target_rate)\n        bootstrap_lods.append(boot_lod)\n\n    # 95% confidence interval\n    ci_lower = np.percentile(bootstrap_lods, 2.5)\n    ci_upper = np.percentile(bootstrap_lods, 97.5)\n    return ci_lower, ci_upper\n</code></pre>"},{"location":"evaluation/#bias-correction","title":"Bias Correction","text":"<p>Detection curves may exhibit bias due to: - Small sample effects at low AFs - Pipeline efficiency variations - Context-dependent error rates</p> <p>Bootstrap resampling provides bias-corrected estimates by accounting for sampling variability.</p>"},{"location":"evaluation/#validation-criteria","title":"Validation Criteria","text":""},{"location":"evaluation/#consistency-checks","title":"Consistency Checks","text":"<p>LoB &lt; LoD Relationship:  Detection limit must exceed blank variability: <pre><code>assert lob_value &lt; lod_value, \"LoB must be less than LoD\"\n</code></pre></p> <p>LoD Monotonicity: Detection limits should decrease with increasing depth: <pre><code>for i in range(len(depths) - 1):\n    assert lod_values[i] &gt;= lod_values[i+1], \"LoD should decrease with depth\"\n</code></pre></p> <p>LoQ \u2265 LoD: Quantification limits should exceed detection limits: <pre><code>assert loq_value &gt;= lod_value, \"LoQ must be greater than or equal to LoD\"\n</code></pre></p>"},{"location":"evaluation/#expected-performance","title":"Expected Performance","text":"<p>Based on simulation studies with the current error model:</p> Depth LoB (calls) LoD (AF) LoD CI LoQ (AF) 1K 2.1 8.5e-3 [6.2e-3, 1.1e-2] 1.2e-2 5K 3.8 2.1e-3 [1.8e-3, 2.5e-3] 3.8e-3 10K 5.2 1.1e-3 [0.9e-3, 1.4e-3] 1.9e-3 <p>Note: Values may vary based on error model parameters and trinucleotide context</p>"},{"location":"evaluation/#artifacts-generated","title":"Artifacts Generated","text":""},{"location":"evaluation/#lob-results-reportslobjson","title":"LoB Results (<code>reports/lob.json</code>)","text":"<pre><code>{\n  \"lob_value\": 2.1,\n  \"blank_mean\": 1.3,\n  \"blank_std\": 0.8,\n  \"blank_measurements\": [0, 1, 2, ...],\n  \"n_blank_runs\": 100,\n  \"percentile\": 95\n}\n</code></pre>"},{"location":"evaluation/#lod-results-reportslod_tablecsv","title":"LoD Results (<code>reports/lod_table.csv</code>)","text":"<pre><code>depth,lod_af,lod_ci_lower,lod_ci_upper,target_detection_rate,n_replicates\n1000,8.5e-3,6.2e-3,1.1e-2,0.95,50\n5000,2.1e-3,1.8e-3,2.5e-3,0.95,50\n10000,1.1e-3,0.9e-3,1.4e-3,0.95,50\n</code></pre>"},{"location":"evaluation/#loq-results-reportsloq_tablecsv","title":"LoQ Results (<code>reports/loq_table.csv</code>)","text":"<pre><code>depth,loq_af_cv,loq_af_abs_error,cv_threshold,abs_error_threshold,n_replicates\n1000,1.2e-2,null,0.20,null,50\n5000,3.8e-3,null,0.20,null,50\n10000,1.9e-3,null,0.20,null,50\n</code></pre>"},{"location":"evaluation/#detection-curves-reportslod_curvespng","title":"Detection Curves (<code>reports/lod_curves.png</code>)","text":"<p>Visualization showing: - Observed detection rates vs. AF (per depth) - Fitted logistic curves - LoD markers with confidence intervals - Target detection rate (95%) line</p>"},{"location":"evaluation/#integration-with-pipeline","title":"Integration with Pipeline","text":"<p>Detection limit estimation is integrated into the main pipeline via CLI commands:</p> <pre><code># Individual analyses\nprecise-mrd eval-lob --n-blank-runs 100 --seed 7\nprecise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7\nprecise-mrd eval-loq --cv-threshold 0.20 --seed 7\n\n# Combined analysis\nprecise-mrd eval-all-limits --seed 7\n</code></pre> <p>All analyses are: - Deterministic: Seeded for reproducibility - Fast: Optimized for CI/CD integration - Validated: Include sanity checks and consistency tests - Documented: Complete metadata and run context</p>"},{"location":"getting-started%202/","title":"Getting Started","text":"<p>This guide will help you get up and running with Precise MRD detection limit analytics.</p>"},{"location":"getting-started%202/#installation","title":"Installation","text":""},{"location":"getting-started%202/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> </ul>"},{"location":"getting-started%202/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/altalanta/precise-mrd-mini.git\ncd precise-mrd-mini\n</code></pre></p> </li> <li> <p>Install the package: <pre><code>make setup\n# This runs: python -m pip install -e .[dev]\n</code></pre></p> </li> <li> <p>Verify installation: <pre><code>precise-mrd --help\n</code></pre></p> </li> </ol>"},{"location":"getting-started%202/#quick-start","title":"Quick Start","text":""},{"location":"getting-started%202/#3-command-demo","title":"3-Command Demo","text":"<p>The fastest way to see Precise MRD in action:</p> <pre><code>make setup     # Install dependencies and package\nmake smoke     # Run fast end-to-end pipeline  \nmake determinism  # Verify identical hashes across runs\n</code></pre> <p>This will: - Generate synthetic ctDNA data - Run the complete analysis pipeline - Produce evaluation artifacts - Verify deterministic reproducibility</p>"},{"location":"getting-started%202/#understanding-the-output","title":"Understanding the Output","text":"<p>After running <code>make smoke</code>, check the <code>reports/</code> directory:</p> <pre><code>ls reports/\n# Expected files:\n# metrics.json          - Performance metrics with bootstrap CIs\n# auto_report.html      - Interactive HTML report  \n# run_context.json      - Complete reproducibility metadata\n# hash_manifest.txt     - SHA256 verification manifest\n</code></pre>"},{"location":"getting-started%202/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started%202/#command-line-interface","title":"Command Line Interface","text":"<p>Precise MRD provides several CLI commands:</p> <pre><code># Basic pipeline\nprecise-mrd smoke --seed 7 --out data/smoke\n\n# Detection limit analysis\nprecise-mrd eval-lob --n-blank-runs 100 --seed 7\nprecise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7  \nprecise-mrd eval-loq --cv-threshold 0.20 --seed 7\n\n# Contamination testing\nprecise-mrd eval-contamination --hop-rates 0.0,0.001,0.005,0.01 --seed 7\n\n# Stratified analysis\nprecise-mrd eval-stratified --contexts CpG,CHG,CHH,NpN --seed 7\n</code></pre>"},{"location":"getting-started%202/#configuration","title":"Configuration","text":"<p>Analyses are configured via YAML files in the <code>configs/</code> directory:</p> <pre><code># configs/smoke.yaml\nrun_id: \"smoke_test\"\nseed: 7\n\nsimulation:\n  allele_fractions: [0.01, 0.001, 0.0001]\n  umi_depths: [1000, 5000]\n  n_replicates: 10\n  n_bootstrap: 100\n\numi:\n  min_family_size: 3\n  max_family_size: 1000\n  quality_threshold: 20\n  consensus_threshold: 0.6\n\nstats:\n  test_type: \"poisson\"\n  alpha: 0.05\n  fdr_method: \"benjamini_hochberg\"\n\nlod:\n  detection_threshold: 0.95\n  confidence_level: 0.95\n</code></pre>"},{"location":"getting-started%202/#analysis-workflows","title":"Analysis Workflows","text":""},{"location":"getting-started%202/#detection-limit-analysis","title":"Detection Limit Analysis","text":"<ol> <li> <p>Limit of Blank (LoB): <pre><code>precise-mrd eval-lob --n-blank-runs 100 --seed 7\n# Output: reports/lob.json\n</code></pre></p> </li> <li> <p>Limit of Detection (LoD): <pre><code>precise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7\n# Outputs: reports/lod_table.csv, reports/lod_curves.png\n</code></pre></p> </li> <li> <p>Limit of Quantification (LoQ): <pre><code>precise-mrd eval-loq --cv-threshold 0.20 --seed 7\n# Output: reports/loq_table.csv\n</code></pre></p> </li> </ol>"},{"location":"getting-started%202/#contamination-analysis","title":"Contamination Analysis","text":"<pre><code>precise-mrd eval-contamination \\\n    --hop-rates 0.0,0.001,0.002,0.005,0.01 \\\n    --collision-rates 0.0,0.0001,0.0005,0.001 \\\n    --cross-contam 0.0,0.01,0.05,0.1 \\\n    --seed 7\n# Outputs: reports/contam_sensitivity.json, reports/contam_heatmap.png\n</code></pre>"},{"location":"getting-started%202/#stratified-analysis","title":"Stratified Analysis","text":"<pre><code>precise-mrd eval-stratified \\\n    --contexts CpG,CHG,CHH,NpN \\\n    --af-values 0.001,0.005,0.01,0.05 \\\n    --depths 1000,5000,10000 \\\n    --seed 7\n# Outputs: reports/power_by_stratum.json, reports/calibration_by_bin.csv\n</code></pre>"},{"location":"getting-started%202/#makefile-targets","title":"Makefile Targets","text":"<p>Common operations are available as Makefile targets:</p> <pre><code># Setup and testing\nmake setup          # Install dependencies\nmake test           # Run unit tests\nmake coverage       # Test coverage report\nmake lint           # Code quality checks\nmake format         # Auto-format code\n\n# Pipeline execution\nmake smoke          # Fast end-to-end test\nmake determinism    # Verify reproducibility\nmake stat-sanity    # Statistical validation tests\n\n# Cleaning\nmake clean          # Remove build artifacts\nmake clean-safe     # Clean generated artifacts safely\n</code></pre>"},{"location":"getting-started%202/#python-api","title":"Python API","text":"<p>You can also use Precise MRD programmatically:</p> <pre><code>import numpy as np\nfrom precise_mrd.config import load_config\nfrom precise_mrd.eval import estimate_lob, estimate_lod, estimate_loq\nfrom precise_mrd.sim import run_contamination_stress_test\n\n# Load configuration\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)\n\n# Run detection limit analysis\nlob_results = estimate_lob(config, rng, n_blank_runs=100)\nlod_results = estimate_lod(config, rng, af_range=(1e-4, 1e-2))  \nloq_results = estimate_loq(config, rng, cv_threshold=0.20)\n\n# Run contamination testing\ncontam_results = run_contamination_stress_test(config, rng)\n\nprint(f\"LoB: {lob_results['lob_value']:.3f}\")\nprint(f\"LoD (5K depth): {lod_results['depth_results'][5000]['lod_af']:.2e}\")\nprint(f\"LoQ (5K depth): {loq_results['depth_results'][5000]['loq_af_cv']:.2e}\")\n</code></pre>"},{"location":"getting-started%202/#reproducibility","title":"Reproducibility","text":"<p>All analyses are deterministically reproducible:</p>"},{"location":"getting-started%202/#seed-management","title":"Seed Management","text":"<ul> <li>All random operations use seeded <code>np.random.Generator</code> instances</li> <li>No global random state dependencies</li> <li>Configuration includes deterministic hash</li> </ul>"},{"location":"getting-started%202/#verification","title":"Verification","text":"<pre><code># Run analysis twice and compare\nmake determinism\n# Should show: \u2705 Determinism verified - identical hashes\n</code></pre>"},{"location":"getting-started%202/#hash-verification","title":"Hash Verification","text":"<pre><code># Check artifact hashes\ncat reports/hash_manifest.txt\n# Example output:\n# a1b2c3d4... reports/metrics.json\n# e5f6g7h8... reports/auto_report.html\n</code></pre>"},{"location":"getting-started%202/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation: Deep dive into detection limit analytics</li> <li>Contamination: Contamination stress testing details</li> <li>Statistical Validation: Understanding the validation framework</li> <li>Reproducibility: Complete reproducibility guidelines</li> </ul>"},{"location":"getting-started%202/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started%202/#common-issues","title":"Common Issues","text":"<p>ImportError: Module not found <pre><code># Ensure package is installed in development mode\nmake setup\n</code></pre></p> <p>Permission errors: <pre><code># Ensure write permissions for data/ and reports/ directories\nchmod -R 755 data/ reports/\n</code></pre></p> <p>Determinism failures: <pre><code># Check for any non-deterministic dependencies\nmake stat-sanity\n</code></pre></p> <p>Slow execution: <pre><code># Use reduced parameters for testing\nprecise-mrd eval-lod --af-range 1e-3,1e-2 --depths 1000 --n-replicates 10\n</code></pre></p> <p>For additional help, see the API Reference or open an issue on GitHub.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get up and running with Precise MRD detection limit analytics.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> </ul>"},{"location":"getting-started/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/altalanta/precise-mrd-mini.git\ncd precise-mrd-mini\n</code></pre></p> </li> <li> <p>Install the locked environment: <pre><code>uv sync --extra dev --extra docs\n</code></pre></p> </li> <li> <p>Verify installation: <pre><code>precise-mrd --help\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#three-cli-commands","title":"Three CLI commands","text":"<p>The fastest way to see Precise MRD in action:</p> <pre><code>uv sync --extra dev --extra docs\nprecise-mrd smoke --out-dir data/smoke\nprecise-mrd determinism --out-dir data/determinism\n</code></pre> <p>This will: - Generate synthetic ctDNA data - Run the complete analysis pipeline - Produce evaluation artifacts - Verify deterministic reproducibility</p>"},{"location":"getting-started/#understanding-the-output","title":"Understanding the Output","text":"<p>After running <code>precise-mrd smoke</code>, check the <code>reports/</code> directory:</p> <pre><code>ls reports/\n# Expected files:\n# metrics.json          - Performance metrics with bootstrap CIs\n# auto_report.html      - Interactive HTML report  \n# run_context.json      - Complete reproducibility metadata\n# hash_manifest.txt     - SHA256 verification manifest\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#command-line-interface","title":"Command Line Interface","text":"<p>Precise MRD provides several CLI commands:</p> <pre><code># Basic pipeline\nprecise-mrd smoke --seed 7 --out-dir data/smoke\n\n# Determinism verification\nprecise-mrd determinism --seed 7 --out-dir data/determinism\n\n# Detection limit analysis (defaults read from config)\nprecise-mrd eval-lob --n-blank 50\nprecise-mrd eval-lod --replicates 25\nprecise-mrd eval-loq --replicates 25\n\n# Contamination testing\nprecise-mrd eval-contamination\n\n# Stratified analysis\nprecise-mrd eval-stratified\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>Analyses are configured via YAML files. Minimal smoke config:</p> <pre><code># configs/smoke.yaml\nrun_id: \"smoke_test\"\nseed: 7\n\nsimulation:\n  allele_fractions: [0.01, 0.001, 0.0001]\n  umi_depths: [1000, 5000]\n  n_replicates: 10\n  n_bootstrap: 100\n\numi:\n  min_family_size: 3\n  max_family_size: 1000\n  quality_threshold: 20\n  consensus_threshold: 0.6\n\nstats:\n  test_type: \"poisson\"\n  alpha: 0.05\n  fdr_method: \"benjamini_hochberg\"\n\nlod:\n  detection_threshold: 0.95\n  confidence_level: 0.95\n</code></pre>"},{"location":"getting-started/#analysis-workflows","title":"Analysis Workflows","text":""},{"location":"getting-started/#detection-limit-analysis","title":"Detection Limit Analysis","text":"<ol> <li> <p>Limit of Blank (LoB): <pre><code>precise-mrd eval-lob --n-blank 50 --seed 7\n# Output: reports/lob.json\n</code></pre></p> </li> <li> <p>Limit of Detection (LoD): <pre><code>precise-mrd eval-lod --replicates 25 --seed 7\n# Outputs: reports/lod_table.csv, reports/lod_curves.png\n</code></pre></p> </li> <li> <p>Limit of Quantification (LoQ): <pre><code>precise-mrd eval-loq --replicates 25 --seed 7\n# Output: reports/loq_table.csv\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#contamination-analysis","title":"Contamination Analysis","text":"<pre><code>precise-mrd eval-contamination --seed 7\n# Outputs: reports/contam_sensitivity.json, reports/contam_heatmap.png\n</code></pre>"},{"location":"getting-started/#stratified-analysis","title":"Stratified Analysis","text":"<pre><code>precise-mrd eval-stratified --seed 7\n# Outputs: reports/power_by_stratum.json, reports/calibration_by_bin.csv\n</code></pre>"},{"location":"getting-started/#makefile-targets","title":"Makefile Targets","text":"<p>Common operations are available as Makefile targets:</p> <pre><code># Setup and testing\nmake setup          # Install dependencies\nmake test           # Run unit tests\nmake coverage       # Test coverage report\nmake lint           # Code quality checks\nmake format         # Auto-format code\n\n# Pipeline execution\nmake smoke          # Fast end-to-end test\nmake determinism    # Verify reproducibility\nmake stat-sanity    # Statistical validation tests\n\n# Cleaning\nmake clean          # Remove build artifacts\nmake clean-safe     # Clean generated artifacts safely\n</code></pre>"},{"location":"getting-started/#python-api","title":"Python API","text":"<p>You can also use Precise MRD programmatically:</p> <pre><code>import numpy as np\nfrom precise_mrd.config import load_config\nfrom precise_mrd.eval import estimate_lob, estimate_lod, estimate_loq\nfrom precise_mrd.sim import run_contamination_stress_test\n\n# Load configuration\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)\n\n# Run detection limit analysis\nlob_results = estimate_lob(config, rng, n_blank_runs=100)\nlod_results = estimate_lod(config, rng, af_range=(1e-4, 1e-2))  \nloq_results = estimate_loq(config, rng, cv_threshold=0.20)\n\n# Run contamination testing\ncontam_results = run_contamination_stress_test(config, rng)\n\nprint(f\"LoB: {lob_results['lob_value']:.3f}\")\nprint(f\"LoD (5K depth): {lod_results['depth_results'][5000]['lod_af']:.2e}\")\nprint(f\"LoQ (5K depth): {loq_results['depth_results'][5000]['loq_af_cv']:.2e}\")\n</code></pre>"},{"location":"getting-started/#reproducibility","title":"Reproducibility","text":"<p>All analyses are deterministically reproducible:</p>"},{"location":"getting-started/#seed-management","title":"Seed Management","text":"<ul> <li>All random operations use seeded <code>np.random.Generator</code> instances</li> <li>No global random state dependencies</li> <li>Configuration includes deterministic hash</li> </ul>"},{"location":"getting-started/#verification","title":"Verification","text":"<pre><code># Run analysis twice and compare\nmake determinism\n# Should show: \u2705 Determinism verified - identical hashes\n</code></pre>"},{"location":"getting-started/#hash-verification","title":"Hash Verification","text":"<pre><code># Check artifact hashes\ncat reports/hash_manifest.txt\n# Example output:\n# a1b2c3d4... reports/metrics.json\n# e5f6g7h8... reports/auto_report.html\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation: Deep dive into detection limit analytics</li> <li>Contamination: Contamination stress testing details</li> <li>Statistical Validation: Understanding the validation framework</li> <li>Reproducibility: Complete reproducibility guidelines</li> </ul>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>ImportError: Module not found <pre><code># Ensure package is installed in development mode\nmake setup\n</code></pre></p> <p>Permission errors: <pre><code># Ensure write permissions for data/ and reports/ directories\nchmod -R 755 data/ reports/\n</code></pre></p> <p>Determinism failures: <pre><code># Check for any non-deterministic dependencies\nmake stat-sanity\n</code></pre></p> <p>Slow execution: <pre><code># Use reduced parameters for testing\nprecise-mrd eval-lod --af-range 1e-3,1e-2 --depths 1000 --n-replicates 10\n</code></pre></p> <p>For additional help, see the API Reference or open an issue on GitHub.</p>"},{"location":"index%202/","title":"Precise MRD - Detection Limit Analytics","text":"<p>A ctDNA/UMI toy MRD pipeline with formal detection limit analytics, statistical validation, and comprehensive contamination robustness testing.</p>"},{"location":"index%202/#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"index%202/#deterministic-umi-processing","title":"\ud83e\uddec Deterministic UMI Processing","text":"<ul> <li>Modern NumPy RNG API (<code>np.random.default_rng</code>)</li> <li>Reproducible seed management across all components</li> <li>Hash-verified artifact consistency</li> </ul>"},{"location":"index%202/#formal-detection-limits","title":"\ud83d\udcca Formal Detection Limits","text":"<ul> <li>LoB (Limit of Blank): 95th percentile of blank measurements</li> <li>LoD (Limit of Detection): AF yielding 95% detection probability with bias-corrected CIs</li> <li>LoQ (Limit of Quantification): Lowest AF meeting precision criteria (CV \u2264 20%)</li> </ul>"},{"location":"index%202/#contamination-robustness","title":"\ud83d\udd2c Contamination Robustness","text":"<ul> <li>Index-hopping stress testing with configurable hop rates</li> <li>Barcode collision modeling and impact assessment</li> <li>Cross-sample contamination sensitivity analysis</li> </ul>"},{"location":"index%202/#stratified-analysis","title":"\ud83d\udcc8 Stratified Analysis","text":"<ul> <li>Power analysis by trinucleotide context and depth</li> <li>Calibration assessment across AF/depth strata</li> <li>Context-specific error modeling</li> </ul>"},{"location":"index%202/#artifact-contract","title":"\ud83c\udfaf Artifact Contract","text":"<ul> <li>Schema-validated JSON outputs</li> <li>Guaranteed artifact paths and structure</li> <li>Complete run context metadata</li> </ul>"},{"location":"index%202/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>3-command demo (deterministic, &lt;5 minutes):</p> <pre><code>make setup     # Install dependencies and package\nmake smoke     # Run fast end-to-end pipeline  \nmake determinism  # Verify identical hashes across runs\n</code></pre>"},{"location":"index%202/#detection-limit-analytics","title":"\ud83d\udccb Detection Limit Analytics","text":""},{"location":"index%202/#limit-of-blank-lob","title":"Limit of Blank (LoB)","text":"<p>Simulate pure-blank runs and estimate the 95th percentile of background measurements:</p> <pre><code># Run LoB estimation\nprecise-mrd eval-lob --n-blank-runs 100 --seed 7\n</code></pre> <p>Output: <code>reports/lob.json</code> with background statistics</p>"},{"location":"index%202/#limit-of-detection-lod","title":"Limit of Detection (LoD)","text":"<p>Estimate AF yielding 95% detection across depth grid with bias-corrected confidence intervals:</p> <pre><code># Run LoD estimation across AF range\nprecise-mrd eval-lod --af-range 1e-4,1e-2 --depths 1000,5000,10000 --seed 7\n</code></pre> <p>Outputs:  - <code>reports/lod_table.csv</code> - LoD values per depth with CIs - <code>reports/lod_curves.png</code> - Detection curves visualization</p>"},{"location":"index%202/#limit-of-quantification-loq","title":"Limit of Quantification (LoQ)","text":"<p>Find lowest AF meeting precision criteria (CV \u2264 20%):</p> <pre><code># Run LoQ estimation\nprecise-mrd eval-loq --cv-threshold 0.20 --seed 7\n</code></pre> <p>Output: <code>reports/loq_table.csv</code> - LoQ values per depth</p>"},{"location":"index%202/#contamination-stress-testing","title":"\ud83e\uddea Contamination Stress Testing","text":"<p>Test detection robustness under various contamination scenarios:</p> <pre><code># Run contamination stress tests\nprecise-mrd eval-contamination --hop-rates 0.0,0.001,0.005,0.01 --seed 7\n</code></pre> <p>Outputs: - <code>reports/contam_sensitivity.json</code> - Sensitivity under contamination - <code>reports/contam_heatmap.png</code> - Impact visualization</p>"},{"location":"index%202/#stratified-analysis_1","title":"\ud83d\udcca Stratified Analysis","text":"<p>Analyze power and calibration by context and depth:</p> <pre><code># Run stratified analysis\nprecise-mrd eval-stratified --contexts CpG,CHG,CHH,NpN --seed 7\n</code></pre> <p>Outputs: - <code>reports/power_by_stratum.json</code> - Context-specific power - <code>reports/calibration_by_bin.csv</code> - Binned calibration metrics</p>"},{"location":"index%202/#reproducibility","title":"\ud83d\udd04 Reproducibility","text":"<p>All analyses are deterministically reproducible:</p> <pre><code># Verify determinism\nmake determinism\n# Should show identical hashes across runs\n\n# Full statistical validation\nmake stat-sanity\n# Validates Type I error control, FDR, bootstrap coverage\n</code></pre>"},{"location":"index%202/#performance-metrics","title":"\ud83d\udcc8 Performance Metrics","text":""},{"location":"index%202/#expected-detection-limits","title":"Expected Detection Limits","text":"<ul> <li>LoB: ~0.5-2 false positives per 10K UMIs</li> <li>LoD: ~1e-3 to 1e-4 AF (depth-dependent)</li> <li>LoQ: ~5e-3 to 1e-3 AF (20% CV threshold)</li> </ul>"},{"location":"index%202/#contamination-tolerance","title":"Contamination Tolerance","text":"<ul> <li>Index hopping: &lt;1% sensitivity loss at 0.5% hop rate</li> <li>Barcode collisions: &lt;5% false positive increase at 0.1% collision rate</li> <li>Cross-contamination: Robust up to 5% cross-sample mixing</li> </ul>"},{"location":"index%202/#validation-framework","title":"\ud83c\udfaf Validation Framework","text":""},{"location":"index%202/#statistical-sanity-tests","title":"Statistical Sanity Tests","text":"<ul> <li>Type I Error Control: \u03b1-level validation in hypothesis testing</li> <li>FDR Monotonicity: Benjamini-Hochberg correction verification</li> <li>Bootstrap Coverage: CI coverage on synthetic data</li> <li>LoB/LoD Consistency: LoB &lt; LoD monotonicity checks</li> </ul>"},{"location":"index%202/#cicd-integration","title":"CI/CD Integration","text":"<p>All tests run in &lt;60s for CI efficiency with fail-closed behavior.</p>"},{"location":"index%202/#artifact-structure","title":"\ud83d\udcc1 Artifact Structure","text":"<pre><code>reports/\n\u251c\u2500\u2500 lob.json                    # Limit of Blank results\n\u251c\u2500\u2500 lod_table.csv              # Limit of Detection per depth\n\u251c\u2500\u2500 lod_curves.png             # LoD visualization\n\u251c\u2500\u2500 loq_table.csv              # Limit of Quantification\n\u251c\u2500\u2500 power_by_stratum.json      # Stratified power analysis\n\u251c\u2500\u2500 calibration_by_bin.csv     # Binned calibration metrics\n\u251c\u2500\u2500 contam_sensitivity.json    # Contamination impact\n\u251c\u2500\u2500 contam_heatmap.png         # Contamination visualization\n\u251c\u2500\u2500 metrics.json               # Performance metrics\n\u251c\u2500\u2500 auto_report.html           # Interactive HTML report\n\u251c\u2500\u2500 run_context.json           # Reproducibility metadata\n\u2514\u2500\u2500 hash_manifest.txt          # SHA256 verification\n</code></pre>"},{"location":"index%202/#scientific-rigor","title":"\ud83d\udd2c Scientific Rigor","text":""},{"location":"index%202/#detection-limit-standards","title":"Detection Limit Standards","text":"<p>Following CLSI EP17 guidelines for detection capability: - LoB: Highest blank measurement (95th percentile) - LoD: 95% detection probability with Type I/II error control - LoQ: Acceptable precision (CV \u2264 20% or absolute error threshold)</p>"},{"location":"index%202/#statistical-methodology","title":"Statistical Methodology","text":"<ul> <li>Stratified bootstrap: Bias-corrected confidence intervals</li> <li>Logistic regression: Detection curve fitting with robust estimation</li> <li>Calibration assessment: Expected Calibration Error (ECE) and reliability diagrams</li> </ul>"},{"location":"index%202/#development","title":"\ud83d\udee0 Development","text":""},{"location":"index%202/#running-tests","title":"Running Tests","text":"<pre><code>make test          # All tests\nmake coverage      # Test coverage\nmake stat-sanity   # Statistical validation only\nmake determinism   # Determinism verification\n</code></pre>"},{"location":"index%202/#code-quality","title":"Code Quality","text":"<pre><code>make lint      # Flake8 + mypy\nmake format    # Black + isort\n</code></pre>"},{"location":"index%202/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"reproducibility%202/","title":"Reproducibility Guidelines","text":"<p>This page provides comprehensive guidelines for ensuring full reproducibility of detection limit analytics and evaluation results in Precise MRD.</p>"},{"location":"reproducibility%202/#reproducibility-principles","title":"Reproducibility Principles","text":"<p>Precise MRD is designed with deterministic reproducibility as a core principle:</p> <ol> <li>Seeded Randomness: All random operations use explicit seeds</li> <li>Artifact Hashing: SHA256 verification of all outputs</li> <li>Environment Capture: Complete computational environment fingerprinting</li> <li>Version Control: Git commit tracking for exact code state</li> </ol>"},{"location":"reproducibility%202/#deterministic-execution","title":"Deterministic Execution","text":""},{"location":"reproducibility%202/#random-number-generation","title":"Random Number Generation","text":"<p>All analyses use the modern NumPy random API with explicit seeding:</p> <pre><code># Correct approach - seeded generator\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)  # Deterministic seed\n\n# Generate reproducible random numbers\nrandom_values = rng.normal(0, 1, size=1000)\n</code></pre> <p>Avoid: <pre><code># Incorrect - global random state\nnp.random.seed(42)  # Global state can be modified\nvalues = np.random.normal(0, 1, size=1000)  # Non-deterministic\n</code></pre></p>"},{"location":"reproducibility%202/#seed-management","title":"Seed Management","text":"<p>Seeds are managed hierarchically to ensure independence while maintaining reproducibility:</p> <pre><code>def run_replicates(base_seed: int, n_replicates: int):\n    \"\"\"Run multiple replicates with independent but reproducible seeds.\"\"\"\n    results = []\n    for rep in range(n_replicates):\n        # Derive unique seed for each replicate\n        rep_seed = base_seed + rep * 1000\n        rep_rng = np.random.default_rng(rep_seed)\n\n        result = run_single_replicate(rep_rng)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reproducibility%202/#configuration-hashing","title":"Configuration Hashing","text":"<p>All configurations are hashed to detect changes:</p> <pre><code>def config_hash(config: PipelineConfig) -&gt; str:\n    \"\"\"Compute deterministic hash of configuration.\"\"\"\n    config_str = json.dumps(config.to_dict(), sort_keys=True)\n    return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"reproducibility%202/#verification-commands","title":"Verification Commands","text":""},{"location":"reproducibility%202/#basic-determinism-check","title":"Basic Determinism Check","text":"<pre><code># Verify identical results across runs\nmake determinism\n</code></pre> <p>This runs the same analysis twice and compares SHA256 hashes: <pre><code>\u2705 Determinism verified - identical hashes\nHash comparison:\na1b2c3d4e5f6... reports/metrics.json\ne7f8g9h0i1j2... reports/auto_report.html\n</code></pre></p>"},{"location":"reproducibility%202/#hash-manifest-verification","title":"Hash Manifest Verification","text":"<pre><code># Generate hash manifest\nmake smoke\ncat reports/hash_manifest.txt\n\n# Example output:\n# a1b2c3d4e5f6g7h8i9j0  reports/metrics.json\n# k1l2m3n4o5p6q7r8s9t0  reports/auto_report.html\n# u1v2w3x4y5z6a7b8c9d0  reports/run_context.json\n</code></pre>"},{"location":"reproducibility%202/#complete-reproduction","title":"Complete Reproduction","text":"<pre><code># Reproduce exact results from configuration\ngit checkout &lt;commit_sha&gt;\nmake setup\nmake smoke SEED=7\nsha256sum -c reports/hash_manifest.txt  # Should all pass\n</code></pre>"},{"location":"reproducibility%202/#environment-fingerprinting","title":"Environment Fingerprinting","text":""},{"location":"reproducibility%202/#run-context-metadata","title":"Run Context Metadata","text":"<p>Every analysis generates complete run context:</p> <pre><code>{\n  \"seed\": 7,\n  \"timestamp\": \"2024-10-05T14:30:00.000Z\",\n  \"config_hash\": \"a1b2c3d4e5f6\",\n  \"git_sha\": \"7fd5373abc123def456\",\n  \"git_branch\": \"main\",\n  \"git_dirty\": false,\n  \"python_version\": \"3.11.5\",\n  \"numpy_version\": \"1.24.4\",\n  \"scipy_version\": \"1.10.1\",\n  \"pandas_version\": \"2.0.3\",\n  \"platform\": \"Linux-5.4.0-x86_64\",\n  \"hostname\": \"ci-runner-123\",\n  \"user\": \"runner\",\n  \"cli_args\": {\n    \"command\": \"smoke\",\n    \"seed\": 7,\n    \"config\": \"configs/smoke.yaml\"\n  },\n  \"execution_time_seconds\": 127.3\n}\n</code></pre>"},{"location":"reproducibility%202/#package-versions","title":"Package Versions","text":"<p>Critical package versions are captured automatically:</p> <pre><code>import precise_mrd\nimport numpy as np\nimport scipy\nimport pandas as pd\n\nversion_info = {\n    \"precise_mrd\": precise_mrd.__version__,\n    \"numpy\": np.__version__,\n    \"scipy\": scipy.__version__,\n    \"pandas\": pd.__version__\n}\n</code></pre>"},{"location":"reproducibility%202/#reproduction-workflows","title":"Reproduction Workflows","text":""},{"location":"reproducibility%202/#release-reproduction","title":"Release Reproduction","text":"<p>For versioned releases, use the complete reproduction workflow:</p> <pre><code># 1. Clone at specific release\ngit clone --branch v1.0.0 https://github.com/altalanta/precise-mrd-mini.git\ncd precise-mrd-mini\n\n# 2. Install exact dependencies  \nmake setup\n\n# 3. Reproduce release artifacts\nexport RELEASE_SEED=42\nmake smoke SEED=$RELEASE_SEED\nmake eval-all SEED=$RELEASE_SEED\n\n# 4. Verify hashes against release manifest\nsha256sum -c reports/release_hash_manifest.txt\n</code></pre>"},{"location":"reproducibility%202/#development-reproduction","title":"Development Reproduction","text":"<p>For development work, use commit-specific reproduction:</p> <pre><code># 1. Checkout specific commit\ngit checkout 7fd5373abc123def456\n\n# 2. Reproduce with same configuration\nmake smoke SEED=7\n\n# 3. Compare against reference\ndiff reports/metrics.json reference/metrics.json\n</code></pre>"},{"location":"reproducibility%202/#cross-platform-reproduction","title":"Cross-Platform Reproduction","text":"<p>Platform differences can affect reproducibility. Use containers for exact reproduction:</p> <pre><code># Dockerfile for reproducible environment\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nRUN pip install -e .\n\n# Fixed environment variables\nENV PYTHONHASHSEED=0\nENV OMP_NUM_THREADS=1\nENV NUMBA_NUM_THREADS=1\n\nCMD [\"make\", \"smoke\"]\n</code></pre>"},{"location":"reproducibility%202/#docker-reproduction","title":"Docker Reproduction","text":"<pre><code># Build reproducible container\ndocker build -t precise-mrd-repro .\n\n# Run with fixed seed\ndocker run --rm -v $(pwd)/reports:/app/reports precise-mrd-repro\n</code></pre>"},{"location":"reproducibility%202/#debugging-non-determinism","title":"Debugging Non-Determinism","text":""},{"location":"reproducibility%202/#common-sources-of-non-determinism","title":"Common Sources of Non-Determinism","text":"<ol> <li>Global Random State: Using <code>np.random</code> without seeding</li> <li>Hash Order: Dictionary iteration order (use <code>sort_keys=True</code>)</li> <li>Floating Point: Platform-specific precision differences</li> <li>Threading: Non-deterministic thread execution order</li> <li>System Time: Using timestamps in analysis</li> </ol>"},{"location":"reproducibility%202/#debugging-tools","title":"Debugging Tools","text":"<pre><code>def debug_determinism(func, *args, **kwargs):\n    \"\"\"Debug function determinism.\"\"\"\n    # Run function twice\n    result1 = func(*args, **kwargs)\n    result2 = func(*args, **kwargs)\n\n    # Compare results\n    if isinstance(result1, dict):\n        for key in result1:\n            if not np.allclose(result1[key], result2[key], rtol=1e-10):\n                print(f\"Non-determinism detected in key: {key}\")\n                return False\n\n    return True\n</code></pre>"},{"location":"reproducibility%202/#detection-techniques","title":"Detection Techniques","text":"<pre><code># Run multiple times and check for differences\nfor i in {1..5}; do\n    make smoke SEED=7\n    cp reports/metrics.json reports/run_${i}.json\ndone\n\n# Compare all runs\nfor i in {2..5}; do\n    diff reports/run_1.json reports/run_${i}.json || echo \"Difference in run $i\"\ndone\n</code></pre>"},{"location":"reproducibility%202/#best-practices","title":"Best Practices","text":""},{"location":"reproducibility%202/#for-users","title":"For Users","text":"<ol> <li>Always Use Seeds: Specify explicit seeds for all analyses</li> <li>Document Environment: Capture package versions and platform info</li> <li>Verify Hashes: Check artifact hashes after important runs</li> <li>Archive Configurations: Save exact configuration files used</li> </ol>"},{"location":"reproducibility%202/#for-developers","title":"For Developers","text":"<ol> <li>Avoid Global State: Never use global random state</li> <li>Test Determinism: Include determinism tests in CI</li> <li>Hash Everything: Generate hashes for all significant outputs</li> <li>Capture Context: Always save complete run context metadata</li> </ol>"},{"location":"reproducibility%202/#for-cicd","title":"For CI/CD","text":"<ol> <li>Fixed Seeds: Use consistent seeds across CI runs</li> <li>Hash Comparison: Compare hashes to detect regressions</li> <li>Environment Pinning: Pin exact package versions</li> <li>Parallel Safety: Ensure analyses are thread-safe</li> </ol>"},{"location":"reproducibility%202/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reproducibility%202/#hash-mismatch","title":"Hash Mismatch","text":"<p>Symptom: Different hashes for same input <pre><code>\u274c Determinism FAILED - hashes differ\nHash comparison:\n&lt; a1b2c3d4e5f6... reports/metrics.json\n&gt; x9y8z7w6v5u4... reports/metrics.json\n</code></pre></p> <p>Debug Steps: 1. Check for floating point differences: <code>diff -u run1.json run2.json</code> 2. Verify seed usage: <code>grep -r \"random\\|seed\" src/</code> 3. Check configuration consistency: <code>diff config1.yaml config2.yaml</code></p>"},{"location":"reproducibility%202/#platform-differences","title":"Platform Differences","text":"<p>Symptom: Different results on different platforms</p> <p>Solutions: - Use Docker for consistent environment - Pin NumPy/SciPy to exact versions - Set environment variables: <code>PYTHONHASHSEED=0</code></p>"},{"location":"reproducibility%202/#version-drift","title":"Version Drift","text":"<p>Symptom: Results change across package updates</p> <p>Solutions: - Pin all dependencies to exact versions - Maintain reference results for regression testing - Document breaking changes in package updates</p>"},{"location":"reproducibility%202/#validation","title":"Validation","text":""},{"location":"reproducibility%202/#automated-checks","title":"Automated Checks","text":"<pre><code>def test_reproducibility():\n    \"\"\"Test that analysis is fully reproducible.\"\"\"\n    config = load_config(\"configs/test.yaml\")\n\n    # Run analysis twice with same seed\n    rng1 = np.random.default_rng(42)\n    result1 = run_analysis(config, rng1)\n\n    rng2 = np.random.default_rng(42)  # Same seed\n    result2 = run_analysis(config, rng2)\n\n    # Results should be identical\n    assert np.allclose(result1, result2, rtol=1e-12)\n</code></pre>"},{"location":"reproducibility%202/#release-validation","title":"Release Validation","text":"<p>Before each release:</p> <ol> <li>Full Reproduction: Reproduce all artifacts from scratch</li> <li>Hash Verification: Verify all hashes match expected values</li> <li>Cross-Platform: Test on multiple platforms/Python versions</li> <li>Documentation: Update reproduction instructions</li> </ol>"},{"location":"reproducibility%202/#archive-structure","title":"Archive Structure","text":"<p>For long-term reproducibility, maintain this archive structure:</p> <pre><code>precise-mrd-archive/\n\u251c\u2500\u2500 v1.0.0/\n\u2502   \u251c\u2500\u2500 source/                    # Complete source code\n\u2502   \u251c\u2500\u2500 environment/\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt       # Exact package versions\n\u2502   \u2502   \u251c\u2500\u2500 python_version.txt     # Python version used\n\u2502   \u2502   \u2514\u2500\u2500 platform_info.txt      # Platform details\n\u2502   \u251c\u2500\u2500 configs/                   # Exact configurations used\n\u2502   \u251c\u2500\u2500 artifacts/                 # All generated artifacts\n\u2502   \u251c\u2500\u2500 hashes/\n\u2502   \u2502   \u251c\u2500\u2500 source_hash.txt        # Source code hash\n\u2502   \u2502   \u2514\u2500\u2500 artifact_hashes.txt    # All artifact hashes\n\u2502   \u2514\u2500\u2500 reproduction/\n\u2502       \u251c\u2500\u2500 reproduce.sh           # Complete reproduction script\n\u2502       \u2514\u2500\u2500 verification.log       # Verification results\n</code></pre> <p>This comprehensive approach ensures that Precise MRD analyses remain fully reproducible across time, platforms, and computational environments.</p>"},{"location":"reproducibility/","title":"Reproducibility Guidelines","text":"<p>This page provides comprehensive guidelines for ensuring full reproducibility of detection limit analytics and evaluation results in Precise MRD.</p>"},{"location":"reproducibility/#reproducibility-principles","title":"Reproducibility Principles","text":"<p>Precise MRD is designed with deterministic reproducibility as a core principle:</p> <ol> <li>Seeded Randomness: All random operations use explicit seeds</li> <li>Artifact Hashing: SHA256 verification of all outputs</li> <li>Environment Capture: Complete computational environment fingerprinting</li> <li>Version Control: Git commit tracking for exact code state</li> </ol>"},{"location":"reproducibility/#deterministic-execution","title":"Deterministic Execution","text":""},{"location":"reproducibility/#random-number-generation","title":"Random Number Generation","text":"<p>All analyses use the modern NumPy random API with explicit seeding:</p> <pre><code># Correct approach - seeded generator\nconfig = load_config(\"configs/smoke.yaml\")\nrng = np.random.default_rng(config.seed)  # Deterministic seed\n\n# Generate reproducible random numbers\nrandom_values = rng.normal(0, 1, size=1000)\n</code></pre> <p>Avoid: <pre><code># Incorrect - global random state\nnp.random.seed(42)  # Global state can be modified\nvalues = np.random.normal(0, 1, size=1000)  # Non-deterministic\n</code></pre></p>"},{"location":"reproducibility/#seed-management","title":"Seed Management","text":"<p>Seeds are managed hierarchically to ensure independence while maintaining reproducibility:</p> <pre><code>def run_replicates(base_seed: int, n_replicates: int):\n    \"\"\"Run multiple replicates with independent but reproducible seeds.\"\"\"\n    results = []\n    for rep in range(n_replicates):\n        # Derive unique seed for each replicate\n        rep_seed = base_seed + rep * 1000\n        rep_rng = np.random.default_rng(rep_seed)\n\n        result = run_single_replicate(rep_rng)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reproducibility/#configuration-hashing","title":"Configuration Hashing","text":"<p>All configurations are hashed to detect changes:</p> <pre><code>def config_hash(config: PipelineConfig) -&gt; str:\n    \"\"\"Compute deterministic hash of configuration.\"\"\"\n    config_str = json.dumps(config.to_dict(), sort_keys=True)\n    return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"reproducibility/#verification-commands","title":"Verification Commands","text":""},{"location":"reproducibility/#basic-determinism-check","title":"Basic Determinism Check","text":"<pre><code># Verify identical results across runs\nprecise-mrd determinism --out-dir data/determinism\n</code></pre> <p>This runs the same analysis twice and compares SHA256 hashes: <pre><code>\u2705 Determinism verified - identical hashes\nHash comparison:\na1b2c3d4e5f6... reports/metrics.json\ne7f8g9h0i1j2... reports/auto_report.html\n</code></pre></p>"},{"location":"reproducibility/#hash-manifest-verification","title":"Hash Manifest Verification","text":"<pre><code># Generate hash manifest\nprecise-mrd smoke --out-dir data/contracts\ncat reports/hash_manifest.txt\n\n# Example output:\n# a1b2c3d4e5f6g7h8i9j0  reports/metrics.json\n# k1l2m3n4o5p6q7r8s9t0  reports/auto_report.html\n# u1v2w3x4y5z6a7b8c9d0  reports/run_context.json\n</code></pre> <p>Programmatic validation is available via:</p> <pre><code>from pathlib import Path\nfrom precise_mrd.validation import validate_artifacts\n\nvalidate_artifacts(Path(\"reports\"))\n</code></pre>"},{"location":"reproducibility/#complete-reproduction","title":"Complete Reproduction","text":"<pre><code># Reproduce exact results from configuration\ngit checkout &lt;commit_sha&gt;\nuv sync --extra dev --extra docs\nprecise-mrd smoke --seed 7 --out-dir data/smoke\nsha256sum -c reports/hash_manifest.txt  # Should all pass\n</code></pre>"},{"location":"reproducibility/#environment-fingerprinting","title":"Environment Fingerprinting","text":""},{"location":"reproducibility/#run-context-metadata","title":"Run Context Metadata","text":"<p>Every analysis generates complete run context:</p> <pre><code>{\n  \"schema_version\": \"1.0.0\",\n  \"seed\": 7,\n  \"timestamp\": \"2024-10-05T14:30:00.000Z\",\n  \"config_hash\": \"a1b2c3d4e5f6\",\n  \"git_sha\": \"7fd5373abc123def456\",\n  \"git_branch\": \"main\",\n  \"git_dirty\": false,\n  \"python_version\": \"3.11.5\",\n  \"numpy_version\": \"1.24.4\",\n  \"scipy_version\": \"1.10.1\",\n  \"pandas_version\": \"2.0.3\",\n  \"platform\": \"Linux-5.4.0-x86_64\",\n  \"hostname\": \"ci-runner-123\",\n  \"user\": \"runner\",\n  \"cli_args\": {\n    \"command\": \"smoke\",\n    \"seed\": 7,\n    \"config_run_id\": \"smoke_test\"\n  },\n  \"execution_time_seconds\": 127.3\n}\n</code></pre>"},{"location":"reproducibility/#package-versions","title":"Package Versions","text":"<p>Critical package versions are captured automatically:</p> <pre><code>import precise_mrd\nimport numpy as np\nimport scipy\nimport pandas as pd\n\nversion_info = {\n    \"precise_mrd\": precise_mrd.__version__,\n    \"numpy\": np.__version__,\n    \"scipy\": scipy.__version__,\n    \"pandas\": pd.__version__\n}\n</code></pre>"},{"location":"reproducibility/#reproduction-workflows","title":"Reproduction Workflows","text":""},{"location":"reproducibility/#release-reproduction","title":"Release Reproduction","text":"<p>For versioned releases, use the complete reproduction workflow:</p> <pre><code># 1. Clone at specific release\ngit clone --branch v1.0.0 https://github.com/altalanta/precise-mrd-mini.git\ncd precise-mrd-mini\n\n# 2. Install exact dependencies  \nmake setup\n\n# 3. Reproduce release artifacts\nexport RELEASE_SEED=42\nmake smoke SEED=$RELEASE_SEED\nmake eval-all SEED=$RELEASE_SEED\n\n# 4. Verify hashes against release manifest\nsha256sum -c reports/release_hash_manifest.txt\n</code></pre>"},{"location":"reproducibility/#development-reproduction","title":"Development Reproduction","text":"<p>For development work, use commit-specific reproduction:</p> <pre><code># 1. Checkout specific commit\ngit checkout 7fd5373abc123def456\n\n# 2. Reproduce with same configuration\nmake smoke SEED=7\n\n# 3. Compare against reference\ndiff reports/metrics.json reference/metrics.json\n</code></pre>"},{"location":"reproducibility/#cross-platform-reproduction","title":"Cross-Platform Reproduction","text":"<p>Platform differences can affect reproducibility. Use containers for exact reproduction:</p> <pre><code># Dockerfile for reproducible environment\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\nRUN pip install -e .\n\n# Fixed environment variables\nENV PYTHONHASHSEED=0\nENV OMP_NUM_THREADS=1\nENV NUMBA_NUM_THREADS=1\n\nCMD [\"make\", \"smoke\"]\n</code></pre>"},{"location":"reproducibility/#docker-reproduction","title":"Docker Reproduction","text":"<pre><code># Build reproducible container\ndocker build -t precise-mrd-repro .\n\n# Run with fixed seed\ndocker run --rm -v $(pwd)/reports:/app/reports precise-mrd-repro\n</code></pre>"},{"location":"reproducibility/#debugging-non-determinism","title":"Debugging Non-Determinism","text":""},{"location":"reproducibility/#common-sources-of-non-determinism","title":"Common Sources of Non-Determinism","text":"<ol> <li>Global Random State: Using <code>np.random</code> without seeding</li> <li>Hash Order: Dictionary iteration order (use <code>sort_keys=True</code>)</li> <li>Floating Point: Platform-specific precision differences</li> <li>Threading: Non-deterministic thread execution order</li> <li>System Time: Using timestamps in analysis</li> </ol>"},{"location":"reproducibility/#debugging-tools","title":"Debugging Tools","text":"<pre><code>def debug_determinism(func, *args, **kwargs):\n    \"\"\"Debug function determinism.\"\"\"\n    # Run function twice\n    result1 = func(*args, **kwargs)\n    result2 = func(*args, **kwargs)\n\n    # Compare results\n    if isinstance(result1, dict):\n        for key in result1:\n            if not np.allclose(result1[key], result2[key], rtol=1e-10):\n                print(f\"Non-determinism detected in key: {key}\")\n                return False\n\n    return True\n</code></pre>"},{"location":"reproducibility/#detection-techniques","title":"Detection Techniques","text":"<pre><code># Run multiple times and check for differences\nfor i in {1..5}; do\n    make smoke SEED=7\n    cp reports/metrics.json reports/run_${i}.json\ndone\n\n# Compare all runs\nfor i in {2..5}; do\n    diff reports/run_1.json reports/run_${i}.json || echo \"Difference in run $i\"\ndone\n</code></pre>"},{"location":"reproducibility/#best-practices","title":"Best Practices","text":""},{"location":"reproducibility/#for-users","title":"For Users","text":"<ol> <li>Always Use Seeds: Specify explicit seeds for all analyses</li> <li>Document Environment: Capture package versions and platform info</li> <li>Verify Hashes: Check artifact hashes after important runs</li> <li>Archive Configurations: Save exact configuration files used</li> </ol>"},{"location":"reproducibility/#for-developers","title":"For Developers","text":"<ol> <li>Avoid Global State: Never use global random state</li> <li>Test Determinism: Include determinism tests in CI</li> <li>Hash Everything: Generate hashes for all significant outputs</li> <li>Capture Context: Always save complete run context metadata</li> </ol>"},{"location":"reproducibility/#for-cicd","title":"For CI/CD","text":"<ol> <li>Fixed Seeds: Use consistent seeds across CI runs</li> <li>Hash Comparison: Compare hashes to detect regressions</li> <li>Environment Pinning: Pin exact package versions</li> <li>Parallel Safety: Ensure analyses are thread-safe</li> </ol>"},{"location":"reproducibility/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reproducibility/#hash-mismatch","title":"Hash Mismatch","text":"<p>Symptom: Different hashes for same input <pre><code>\u274c Determinism FAILED - hashes differ\nHash comparison:\n&lt; a1b2c3d4e5f6... reports/metrics.json\n&gt; x9y8z7w6v5u4... reports/metrics.json\n</code></pre></p> <p>Debug Steps: 1. Check for floating point differences: <code>diff -u run1.json run2.json</code> 2. Verify seed usage: <code>grep -r \"random\\|seed\" src/</code> 3. Check configuration consistency: <code>diff config1.yaml config2.yaml</code></p>"},{"location":"reproducibility/#platform-differences","title":"Platform Differences","text":"<p>Symptom: Different results on different platforms</p> <p>Solutions: - Use Docker for consistent environment - Pin NumPy/SciPy to exact versions - Set environment variables: <code>PYTHONHASHSEED=0</code></p>"},{"location":"reproducibility/#version-drift","title":"Version Drift","text":"<p>Symptom: Results change across package updates</p> <p>Solutions: - Pin all dependencies to exact versions - Maintain reference results for regression testing - Document breaking changes in package updates</p>"},{"location":"reproducibility/#validation","title":"Validation","text":""},{"location":"reproducibility/#automated-checks","title":"Automated Checks","text":"<pre><code>def test_reproducibility():\n    \"\"\"Test that analysis is fully reproducible.\"\"\"\n    config = load_config(\"configs/test.yaml\")\n\n    # Run analysis twice with same seed\n    rng1 = np.random.default_rng(42)\n    result1 = run_analysis(config, rng1)\n\n    rng2 = np.random.default_rng(42)  # Same seed\n    result2 = run_analysis(config, rng2)\n\n    # Results should be identical\n    assert np.allclose(result1, result2, rtol=1e-12)\n</code></pre>"},{"location":"reproducibility/#release-validation","title":"Release Validation","text":"<p>Before each release:</p> <ol> <li>Full Reproduction: Reproduce all artifacts from scratch</li> <li>Hash Verification: Verify all hashes match expected values</li> <li>Cross-Platform: Test on multiple platforms/Python versions</li> <li>Documentation: Update reproduction instructions</li> </ol>"},{"location":"reproducibility/#archive-structure","title":"Archive Structure","text":"<p>For long-term reproducibility, maintain this archive structure:</p> <pre><code>precise-mrd-archive/\n\u251c\u2500\u2500 v1.0.0/\n\u2502   \u251c\u2500\u2500 source/                    # Complete source code\n\u2502   \u251c\u2500\u2500 environment/\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt       # Exact package versions\n\u2502   \u2502   \u251c\u2500\u2500 python_version.txt     # Python version used\n\u2502   \u2502   \u2514\u2500\u2500 platform_info.txt      # Platform details\n\u2502   \u251c\u2500\u2500 configs/                   # Exact configurations used\n\u2502   \u251c\u2500\u2500 artifacts/                 # All generated artifacts\n\u2502   \u251c\u2500\u2500 hashes/\n\u2502   \u2502   \u251c\u2500\u2500 source_hash.txt        # Source code hash\n\u2502   \u2502   \u2514\u2500\u2500 artifact_hashes.txt    # All artifact hashes\n\u2502   \u2514\u2500\u2500 reproduction/\n\u2502       \u251c\u2500\u2500 reproduce.sh           # Complete reproduction script\n\u2502       \u2514\u2500\u2500 verification.log       # Verification results\n</code></pre> <p>This comprehensive approach ensures that Precise MRD analyses remain fully reproducible across time, platforms, and computational environments.</p>"},{"location":"statistical-validation%202/","title":"Statistical Validation Framework","text":"<p>This page describes the comprehensive statistical validation framework implemented in Precise MRD to ensure the reliability and scientific rigor of detection limit analytics.</p>"},{"location":"statistical-validation%202/#overview","title":"Overview","text":"<p>The statistical validation framework includes multiple layers of checks:</p> <ol> <li>Type I Error Control: Validates \u03b1-level control in hypothesis testing</li> <li>FDR Monotonicity: Ensures proper Benjamini-Hochberg correction implementation</li> <li>Bootstrap Coverage: Verifies confidence interval coverage on synthetic data</li> <li>Detection Limit Consistency: Validates LoB &lt; LoD &lt; LoQ relationships</li> <li>Contamination Robustness: Ensures stable performance under contamination</li> </ol>"},{"location":"statistical-validation%202/#validation-tests","title":"Validation Tests","text":""},{"location":"statistical-validation%202/#type-i-error-control","title":"Type I Error Control","text":"<p>Validates that the statistical testing framework maintains the specified Type I error rate (\u03b1 = 0.05) under the null hypothesis.</p> <pre><code>def test_type_i_error_control():\n    \"\"\"Test that Type I error rate is controlled at \u03b1 = 0.05.\"\"\"\n    alpha = 0.05\n    n_null_tests = 1000\n\n    # Generate null data (no true variants)\n    false_positive_rate = run_null_hypothesis_tests(n_null_tests)\n\n    # Should reject \u2264 5% of null hypotheses\n    assert false_positive_rate &lt;= alpha + tolerance\n</code></pre> <p>Expected Behavior: False positive rate should be \u2264 5.5% (5% + 0.5% tolerance).</p>"},{"location":"statistical-validation%202/#fdr-monotonicity","title":"FDR Monotonicity","text":"<p>Ensures that the Benjamini-Hochberg FDR correction produces monotonically increasing adjusted p-values.</p> <pre><code>def test_fdr_monotonicity():\n    \"\"\"Test that BH-adjusted p-values are monotonic.\"\"\"\n    raw_p_values = [0.001, 0.01, 0.05, 0.1, 0.5]\n    adjusted_p_values = benjamini_hochberg_correction(raw_p_values)\n\n    # Adjusted p-values should be monotonically increasing\n    assert all(adjusted_p_values[i] &lt;= adjusted_p_values[i+1] \n              for i in range(len(adjusted_p_values)-1))\n</code></pre> <p>Expected Behavior: Adjusted p-values maintain ordering: p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098.</p>"},{"location":"statistical-validation%202/#bootstrap-coverage","title":"Bootstrap Coverage","text":"<p>Validates that bootstrap confidence intervals achieve nominal coverage rates.</p> <pre><code>def test_bootstrap_coverage():\n    \"\"\"Test that 95% CIs contain true parameter 95% of the time.\"\"\"\n    true_parameter = 0.05\n    n_experiments = 200\n    coverage_count = 0\n\n    for _ in range(n_experiments):\n        # Generate data with known parameter\n        data = generate_synthetic_data(true_parameter)\n\n        # Compute bootstrap CI\n        ci_lower, ci_upper = bootstrap_confidence_interval(data, alpha=0.05)\n\n        # Check if CI contains true parameter\n        if ci_lower &lt;= true_parameter &lt;= ci_upper:\n            coverage_count += 1\n\n    coverage_rate = coverage_count / n_experiments\n    # Should be approximately 0.95\n    assert 0.90 &lt;= coverage_rate &lt;= 1.00\n</code></pre> <p>Expected Behavior: Coverage rate should be 90-100% (allowing for simulation variability).</p>"},{"location":"statistical-validation%202/#detection-limit-consistency","title":"Detection Limit Consistency","text":"<p>Validates that detection limits maintain expected relationships and properties.</p> <pre><code>def test_detection_limit_consistency():\n    \"\"\"Test LoB &lt; LoD &lt; LoQ consistency.\"\"\"\n    # Estimate all detection limits\n    lob_value = estimate_lob()\n    lod_value = estimate_lod()\n    loq_value = estimate_loq()\n\n    # Convert to comparable units and check relationships\n    lob_equivalent_af = convert_calls_to_af(lob_value)\n\n    # LoB should be less than LoD\n    assert lob_equivalent_af &lt; lod_value\n\n    # LoD should be less than or equal to LoQ  \n    assert lod_value &lt;= loq_value\n</code></pre> <p>Expected Behavior: LoB &lt; LoD \u2264 LoQ hierarchy should hold consistently.</p>"},{"location":"statistical-validation%202/#depth-monotonicity","title":"Depth Monotonicity","text":"<p>Validates that detection limits improve (decrease) with increasing sequencing depth.</p> <pre><code>def test_depth_monotonicity():\n    \"\"\"Test that LoD improves with depth.\"\"\"\n    depths = [1000, 5000, 10000]\n    lod_values = [estimate_lod(depth=d) for d in depths]\n\n    # LoD should decrease (improve) with depth\n    for i in range(len(depths) - 1):\n        assert lod_values[i] &gt;= lod_values[i+1] * tolerance_factor\n</code></pre> <p>Expected Behavior: LoD\u2081\u2096 \u2265 LoD\u2085\u2096 \u2265 LoD\u2081\u2080\u2096 (within tolerance).</p>"},{"location":"statistical-validation%202/#contamination-validation","title":"Contamination Validation","text":""},{"location":"statistical-validation%202/#sensitivity-regression-detection","title":"Sensitivity Regression Detection","text":"<p>Monitors for unexpected sensitivity losses under contamination.</p> <pre><code>def test_contamination_regression():\n    \"\"\"Detect regressions in contamination tolerance.\"\"\"\n    baseline_sensitivity = measure_clean_sensitivity()\n    contaminated_sensitivity = measure_contaminated_sensitivity(hop_rate=0.001)\n\n    sensitivity_loss = baseline_sensitivity - contaminated_sensitivity\n\n    # Should not lose &gt;3% sensitivity at 0.1% hop rate\n    assert sensitivity_loss &lt; 0.03\n</code></pre> <p>Expected Behavior: Minimal sensitivity loss (&lt;3%) at low contamination levels.</p>"},{"location":"statistical-validation%202/#dose-response-validation","title":"Dose-Response Validation","text":"<p>Validates that contamination impact follows expected dose-response relationships.</p> <pre><code>def test_contamination_dose_response():\n    \"\"\"Test that contamination impact increases with contamination level.\"\"\"\n    hop_rates = [0.001, 0.005, 0.01]\n    sensitivity_values = [measure_contaminated_sensitivity(rate) for rate in hop_rates]\n\n    # Sensitivity should decrease with increasing contamination\n    for i in range(len(hop_rates) - 1):\n        assert sensitivity_values[i] &gt;= sensitivity_values[i+1]\n</code></pre> <p>Expected Behavior: Monotonic decrease in sensitivity with increasing contamination.</p>"},{"location":"statistical-validation%202/#calibration-validation","title":"Calibration Validation","text":""},{"location":"statistical-validation%202/#expected-calibration-error-ece","title":"Expected Calibration Error (ECE)","text":"<p>Validates that model predictions are well-calibrated.</p> <pre><code>def test_calibration_quality():\n    \"\"\"Test that model predictions are well-calibrated.\"\"\"\n    predicted_probs, true_labels = generate_calibration_data()\n    ece = compute_expected_calibration_error(predicted_probs, true_labels)\n\n    # ECE should be &lt; 0.1 (well-calibrated)\n    assert ece &lt; 0.10\n</code></pre> <p>Expected Behavior: ECE &lt; 0.10 indicates good calibration quality.</p>"},{"location":"statistical-validation%202/#reliability-diagram-validation","title":"Reliability Diagram Validation","text":"<p>Ensures predicted probabilities align with observed frequencies across confidence bins.</p> <pre><code>def test_reliability_across_bins():\n    \"\"\"Test that predicted probabilities match observed frequencies.\"\"\"\n    for bin_range in [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]:\n        predicted_freq = get_mean_prediction_in_bin(bin_range)\n        observed_freq = get_observed_frequency_in_bin(bin_range)\n\n        # Should be close (within 10%)\n        relative_error = abs(predicted_freq - observed_freq) / max(predicted_freq, 0.01)\n        assert relative_error &lt; 0.10\n</code></pre> <p>Expected Behavior: Predicted and observed frequencies should align within 10%.</p>"},{"location":"statistical-validation%202/#ci-integration","title":"CI Integration","text":""},{"location":"statistical-validation%202/#fast-sanity-checks","title":"Fast Sanity Checks","text":"<p>Subset of validation tests optimized for CI execution (&lt;60 seconds).</p> <pre><code># Quick validation checks\nmake stat-sanity\n</code></pre> <p>Includes: - Basic Type I error check (100 tests instead of 1000) - Quick LoB/LoD consistency (reduced sample sizes) - Minimal contamination regression check - Bootstrap coverage with reduced replicates</p>"},{"location":"statistical-validation%202/#full-validation-suite","title":"Full Validation Suite","text":"<p>Comprehensive validation for release testing.</p> <pre><code># Complete validation (may take 10-30 minutes)\npytest tests/stat_tests/ -v --full-validation\n</code></pre> <p>Includes: - Complete Type I error analysis (1000+ tests) - Comprehensive bootstrap coverage (500+ experiments) - Full contamination dose-response curves - Extensive calibration analysis across all conditions</p>"},{"location":"statistical-validation%202/#validation-metrics","title":"Validation Metrics","text":""},{"location":"statistical-validation%202/#passfail-criteria","title":"Pass/Fail Criteria","text":"Test Criterion Tolerance Type I Error \u2264 5.5% 0.5% Bootstrap Coverage 90-100% 5% LoB &lt; LoD Must hold 2x factor LoD Monotonicity Must hold 50% tolerance Contamination Regression &lt; 3% loss At 0.1% hop rate Calibration ECE &lt; 0.10 Well-calibrated"},{"location":"statistical-validation%202/#reporting","title":"Reporting","text":"<p>Validation results are automatically generated and included in CI reports:</p> <pre><code>{\n  \"validation_summary\": {\n    \"type_i_error_rate\": 0.048,\n    \"bootstrap_coverage\": 0.94,\n    \"lob_lod_consistency\": true,\n    \"depth_monotonicity\": true,\n    \"contamination_regression\": false,\n    \"calibration_ece\": 0.067,\n    \"overall_status\": \"PASS\"\n  }\n}\n</code></pre>"},{"location":"statistical-validation%202/#troubleshooting-validation-failures","title":"Troubleshooting Validation Failures","text":""},{"location":"statistical-validation%202/#type-i-error-rate-too-high","title":"Type I Error Rate Too High","text":"<p>Symptom: False positive rate &gt; 5.5% Possible Causes: - Bug in statistical testing implementation - Incorrect p-value calculation - Issues with multiple testing correction</p> <p>Debugging: <pre><code># Check raw p-value distribution under null\nnull_p_values = generate_null_p_values(1000)\nplot_p_value_histogram(null_p_values)  # Should be uniform\n</code></pre></p>"},{"location":"statistical-validation%202/#bootstrap-coverage-too-low","title":"Bootstrap Coverage Too Low","text":"<p>Symptom: Coverage &lt; 90% Possible Causes: - Bias in estimator - Insufficient bootstrap samples - Incorrect confidence interval calculation</p> <p>Debugging: <pre><code># Increase bootstrap samples and check bias\nestimate_bias = check_estimator_bias(true_param, estimated_params)\n</code></pre></p>"},{"location":"statistical-validation%202/#detection-limit-inconsistency","title":"Detection Limit Inconsistency","text":"<p>Symptom: LoB \u2265 LoD or LoD &gt; LoQ Possible Causes: - Units mismatch between LoB and LoD - Insufficient sample sizes - Bug in detection limit estimation</p> <p>Debugging: <pre><code># Check unit conversions and sample sizes\ndebug_detection_limit_estimation(verbose=True)\n</code></pre></p>"},{"location":"statistical-validation%202/#contamination-regression","title":"Contamination Regression","text":"<p>Symptom: Unexpected sensitivity loss Possible Causes: - Bug in contamination simulation - Changes in pipeline sensitivity - Statistical fluctuation</p> <p>Debugging: <pre><code># Compare contamination models across versions\ncompare_contamination_sensitivity(current_version, reference_version)\n</code></pre></p>"},{"location":"statistical-validation%202/#validation-best-practices","title":"Validation Best Practices","text":"<ol> <li>Run Early and Often: Include fast sanity checks in CI</li> <li>Monitor Trends: Track validation metrics over time</li> <li>Investigate Failures: Don't ignore validation failures</li> <li>Update Baselines: Refresh reference values when pipeline improves</li> <li>Document Changes: Record any validation criteria updates</li> </ol> <p>The validation framework ensures that Precise MRD maintains scientific rigor and reliability across all statistical analyses and detection limit estimates.</p>"},{"location":"statistical-validation/","title":"Statistical Validation Framework","text":"<p>This page describes the comprehensive statistical validation framework implemented in Precise MRD to ensure the reliability and scientific rigor of detection limit analytics.</p>"},{"location":"statistical-validation/#overview","title":"Overview","text":"<p>The statistical validation framework includes multiple layers of checks:</p> <ol> <li>Type I Error Control: Validates \u03b1-level control in hypothesis testing</li> <li>FDR Monotonicity: Ensures proper Benjamini-Hochberg correction implementation</li> <li>Bootstrap Coverage: Verifies confidence interval coverage on synthetic data</li> <li>Detection Limit Consistency: Validates LoB &lt; LoD &lt; LoQ relationships</li> <li>Contamination Robustness: Ensures stable performance under contamination</li> </ol>"},{"location":"statistical-validation/#validation-tests","title":"Validation Tests","text":""},{"location":"statistical-validation/#type-i-error-control","title":"Type I Error Control","text":"<p>Validates that the statistical testing framework maintains the specified Type I error rate (\u03b1 = 0.05) under the null hypothesis.</p> <pre><code>def test_type_i_error_control():\n    \"\"\"Test that Type I error rate is controlled at \u03b1 = 0.05.\"\"\"\n    alpha = 0.05\n    n_null_tests = 1000\n\n    # Generate null data (no true variants)\n    false_positive_rate = run_null_hypothesis_tests(n_null_tests)\n\n    # Should reject \u2264 5% of null hypotheses\n    assert false_positive_rate &lt;= alpha + tolerance\n</code></pre> <p>Expected Behavior: False positive rate should be \u2264 5.5% (5% + 0.5% tolerance).</p>"},{"location":"statistical-validation/#fdr-monotonicity","title":"FDR Monotonicity","text":"<p>Ensures that the Benjamini-Hochberg FDR correction produces monotonically increasing adjusted p-values.</p> <pre><code>def test_fdr_monotonicity():\n    \"\"\"Test that BH-adjusted p-values are monotonic.\"\"\"\n    raw_p_values = [0.001, 0.01, 0.05, 0.1, 0.5]\n    adjusted_p_values = benjamini_hochberg_correction(raw_p_values)\n\n    # Adjusted p-values should be monotonically increasing\n    assert all(adjusted_p_values[i] &lt;= adjusted_p_values[i+1] \n              for i in range(len(adjusted_p_values)-1))\n</code></pre> <p>Expected Behavior: Adjusted p-values maintain ordering: p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098.</p>"},{"location":"statistical-validation/#bootstrap-coverage","title":"Bootstrap Coverage","text":"<p>Validates that bootstrap confidence intervals achieve nominal coverage rates.</p> <pre><code>def test_bootstrap_coverage():\n    \"\"\"Test that 95% CIs contain true parameter 95% of the time.\"\"\"\n    true_parameter = 0.05\n    n_experiments = 200\n    coverage_count = 0\n\n    for _ in range(n_experiments):\n        # Generate data with known parameter\n        data = generate_synthetic_data(true_parameter)\n\n        # Compute bootstrap CI\n        ci_lower, ci_upper = bootstrap_confidence_interval(data, alpha=0.05)\n\n        # Check if CI contains true parameter\n        if ci_lower &lt;= true_parameter &lt;= ci_upper:\n            coverage_count += 1\n\n    coverage_rate = coverage_count / n_experiments\n    # Should be approximately 0.95\n    assert 0.90 &lt;= coverage_rate &lt;= 1.00\n</code></pre> <p>Expected Behavior: Coverage rate should be 90-100% (allowing for simulation variability).</p>"},{"location":"statistical-validation/#detection-limit-consistency","title":"Detection Limit Consistency","text":"<p>Validates that detection limits maintain expected relationships and properties.</p> <pre><code>def test_detection_limit_consistency():\n    \"\"\"Test LoB &lt; LoD &lt; LoQ consistency.\"\"\"\n    # Estimate all detection limits\n    lob_value = estimate_lob()\n    lod_value = estimate_lod()\n    loq_value = estimate_loq()\n\n    # Convert to comparable units and check relationships\n    lob_equivalent_af = convert_calls_to_af(lob_value)\n\n    # LoB should be less than LoD\n    assert lob_equivalent_af &lt; lod_value\n\n    # LoD should be less than or equal to LoQ  \n    assert lod_value &lt;= loq_value\n</code></pre> <p>Expected Behavior: LoB &lt; LoD \u2264 LoQ hierarchy should hold consistently.</p>"},{"location":"statistical-validation/#depth-monotonicity","title":"Depth Monotonicity","text":"<p>Validates that detection limits improve (decrease) with increasing sequencing depth.</p> <pre><code>def test_depth_monotonicity():\n    \"\"\"Test that LoD improves with depth.\"\"\"\n    depths = [1000, 5000, 10000]\n    lod_values = [estimate_lod(depth=d) for d in depths]\n\n    # LoD should decrease (improve) with depth\n    for i in range(len(depths) - 1):\n        assert lod_values[i] &gt;= lod_values[i+1] * tolerance_factor\n</code></pre> <p>Expected Behavior: LoD\u2081\u2096 \u2265 LoD\u2085\u2096 \u2265 LoD\u2081\u2080\u2096 (within tolerance).</p>"},{"location":"statistical-validation/#contamination-validation","title":"Contamination Validation","text":""},{"location":"statistical-validation/#sensitivity-regression-detection","title":"Sensitivity Regression Detection","text":"<p>Monitors for unexpected sensitivity losses under contamination.</p> <pre><code>def test_contamination_regression():\n    \"\"\"Detect regressions in contamination tolerance.\"\"\"\n    baseline_sensitivity = measure_clean_sensitivity()\n    contaminated_sensitivity = measure_contaminated_sensitivity(hop_rate=0.001)\n\n    sensitivity_loss = baseline_sensitivity - contaminated_sensitivity\n\n    # Should not lose &gt;3% sensitivity at 0.1% hop rate\n    assert sensitivity_loss &lt; 0.03\n</code></pre> <p>Expected Behavior: Minimal sensitivity loss (&lt;3%) at low contamination levels.</p>"},{"location":"statistical-validation/#dose-response-validation","title":"Dose-Response Validation","text":"<p>Validates that contamination impact follows expected dose-response relationships.</p> <pre><code>def test_contamination_dose_response():\n    \"\"\"Test that contamination impact increases with contamination level.\"\"\"\n    hop_rates = [0.001, 0.005, 0.01]\n    sensitivity_values = [measure_contaminated_sensitivity(rate) for rate in hop_rates]\n\n    # Sensitivity should decrease with increasing contamination\n    for i in range(len(hop_rates) - 1):\n        assert sensitivity_values[i] &gt;= sensitivity_values[i+1]\n</code></pre> <p>Expected Behavior: Monotonic decrease in sensitivity with increasing contamination.</p>"},{"location":"statistical-validation/#calibration-validation","title":"Calibration Validation","text":""},{"location":"statistical-validation/#expected-calibration-error-ece","title":"Expected Calibration Error (ECE)","text":"<p>Validates that model predictions are well-calibrated.</p> <pre><code>def test_calibration_quality():\n    \"\"\"Test that model predictions are well-calibrated.\"\"\"\n    predicted_probs, true_labels = generate_calibration_data()\n    ece = compute_expected_calibration_error(predicted_probs, true_labels)\n\n    # ECE should be &lt; 0.1 (well-calibrated)\n    assert ece &lt; 0.10\n</code></pre> <p>Expected Behavior: ECE &lt; 0.10 indicates good calibration quality.</p>"},{"location":"statistical-validation/#reliability-diagram-validation","title":"Reliability Diagram Validation","text":"<p>Ensures predicted probabilities align with observed frequencies across confidence bins.</p> <pre><code>def test_reliability_across_bins():\n    \"\"\"Test that predicted probabilities match observed frequencies.\"\"\"\n    for bin_range in [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]:\n        predicted_freq = get_mean_prediction_in_bin(bin_range)\n        observed_freq = get_observed_frequency_in_bin(bin_range)\n\n        # Should be close (within 10%)\n        relative_error = abs(predicted_freq - observed_freq) / max(predicted_freq, 0.01)\n        assert relative_error &lt; 0.10\n</code></pre> <p>Expected Behavior: Predicted and observed frequencies should align within 10%.</p>"},{"location":"statistical-validation/#ci-integration","title":"CI Integration","text":""},{"location":"statistical-validation/#fast-sanity-checks","title":"Fast Sanity Checks","text":"<p>Subset of validation tests optimized for CI execution (&lt;60 seconds).</p> <pre><code># Quick validation checks\nmake stat-sanity\n</code></pre> <p>Includes: - Basic Type I error check (100 tests instead of 1000) - Quick LoB/LoD consistency (reduced sample sizes) - Minimal contamination regression check - Bootstrap coverage with reduced replicates</p>"},{"location":"statistical-validation/#full-validation-suite","title":"Full Validation Suite","text":"<p>Comprehensive validation for release testing.</p> <pre><code># Complete validation (may take 10-30 minutes)\npytest tests/stat_tests/ -v --full-validation\n</code></pre> <p>Includes: - Complete Type I error analysis (1000+ tests) - Comprehensive bootstrap coverage (500+ experiments) - Full contamination dose-response curves - Extensive calibration analysis across all conditions</p>"},{"location":"statistical-validation/#validation-metrics","title":"Validation Metrics","text":""},{"location":"statistical-validation/#passfail-criteria","title":"Pass/Fail Criteria","text":"Test Criterion Tolerance Type I Error \u2264 5.5% 0.5% Bootstrap Coverage 90-100% 5% LoB &lt; LoD Must hold 2x factor LoD Monotonicity Must hold 50% tolerance Contamination Regression &lt; 3% loss At 0.1% hop rate Calibration ECE &lt; 0.10 Well-calibrated"},{"location":"statistical-validation/#reporting","title":"Reporting","text":"<p>Validation results are automatically generated and included in CI reports:</p> <pre><code>{\n  \"validation_summary\": {\n    \"type_i_error_rate\": 0.048,\n    \"bootstrap_coverage\": 0.94,\n    \"lob_lod_consistency\": true,\n    \"depth_monotonicity\": true,\n    \"contamination_regression\": false,\n    \"calibration_ece\": 0.067,\n    \"overall_status\": \"PASS\"\n  }\n}\n</code></pre>"},{"location":"statistical-validation/#troubleshooting-validation-failures","title":"Troubleshooting Validation Failures","text":""},{"location":"statistical-validation/#type-i-error-rate-too-high","title":"Type I Error Rate Too High","text":"<p>Symptom: False positive rate &gt; 5.5% Possible Causes: - Bug in statistical testing implementation - Incorrect p-value calculation - Issues with multiple testing correction</p> <p>Debugging: <pre><code># Check raw p-value distribution under null\nnull_p_values = generate_null_p_values(1000)\nplot_p_value_histogram(null_p_values)  # Should be uniform\n</code></pre></p>"},{"location":"statistical-validation/#bootstrap-coverage-too-low","title":"Bootstrap Coverage Too Low","text":"<p>Symptom: Coverage &lt; 90% Possible Causes: - Bias in estimator - Insufficient bootstrap samples - Incorrect confidence interval calculation</p> <p>Debugging: <pre><code># Increase bootstrap samples and check bias\nestimate_bias = check_estimator_bias(true_param, estimated_params)\n</code></pre></p>"},{"location":"statistical-validation/#detection-limit-inconsistency","title":"Detection Limit Inconsistency","text":"<p>Symptom: LoB \u2265 LoD or LoD &gt; LoQ Possible Causes: - Units mismatch between LoB and LoD - Insufficient sample sizes - Bug in detection limit estimation</p> <p>Debugging: <pre><code># Check unit conversions and sample sizes\ndebug_detection_limit_estimation(verbose=True)\n</code></pre></p>"},{"location":"statistical-validation/#contamination-regression","title":"Contamination Regression","text":"<p>Symptom: Unexpected sensitivity loss Possible Causes: - Bug in contamination simulation - Changes in pipeline sensitivity - Statistical fluctuation</p> <p>Debugging: <pre><code># Compare contamination models across versions\ncompare_contamination_sensitivity(current_version, reference_version)\n</code></pre></p>"},{"location":"statistical-validation/#validation-best-practices","title":"Validation Best Practices","text":"<ol> <li>Run Early and Often: Include fast sanity checks in CI</li> <li>Monitor Trends: Track validation metrics over time</li> <li>Investigate Failures: Don't ignore validation failures</li> <li>Update Baselines: Refresh reference values when pipeline improves</li> <li>Document Changes: Record any validation criteria updates</li> </ol> <p>The validation framework ensures that Precise MRD maintains scientific rigor and reliability across all statistical analyses and detection limit estimates.</p>"},{"location":"tutorials/contamination_analysis/","title":"Contamination Analysis Tutorial","text":"<p>This tutorial demonstrates how to analyze and mitigate contamination in ctDNA/MRD analysis using Precise MRD. Contamination is a critical concern in liquid biopsy assays where even small amounts of cross-sample contamination can lead to false positive results.</p>"},{"location":"tutorials/contamination_analysis/#why-contamination-matters","title":"Why Contamination Matters","text":"<p>In ctDNA analysis, contamination can occur through:</p> <ul> <li>Index hopping: Misassigned reads between multiplexed samples</li> <li>Sample carryover: Residual DNA from previous samples</li> <li>PCR artifacts: Template switching or jumping</li> <li>Environmental contamination: External DNA sources</li> </ul> <p>Even 0.1% contamination can be clinically significant when dealing with MRD detection at 0.01% allele frequencies.</p>"},{"location":"tutorials/contamination_analysis/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this tutorial, you will be able to: - Understand different types of contamination in ctDNA analysis - Simulate contamination scenarios using Precise MRD - Calculate contamination impact on detection limits - Implement contamination mitigation strategies - Interpret contamination analysis results</p>"},{"location":"tutorials/contamination_analysis/#1-types-of-contamination","title":"1. Types of Contamination","text":""},{"location":"tutorials/contamination_analysis/#index-hopping","title":"Index Hopping","text":"<p>Index hopping occurs during sequencing when reads are misassigned between samples in multiplexed libraries. This is particularly problematic for MRD analysis where we're looking for rare variants.</p>"},{"location":"tutorials/contamination_analysis/#sample-carryover","title":"Sample Carryover","text":"<p>Sample carryover happens during sample preparation when residual DNA from one sample contaminates subsequent samples. This is especially concerning in automated liquid handling systems.</p>"},{"location":"tutorials/contamination_analysis/#environmental-contamination","title":"Environmental Contamination","text":"<p>Environmental contamination occurs when external DNA sources (lab personnel, equipment, reagents) introduce artifacts into the analysis.</p>"},{"location":"tutorials/contamination_analysis/#2-simulating-contamination-scenarios","title":"2. Simulating Contamination Scenarios","text":"<p>Let's start by simulating different contamination scenarios and understanding their impact:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set reproducible seed\nnp.random.seed(42)\n\n# Define contamination scenarios\ncontamination_rates = [0.0001, 0.001, 0.01, 0.1]  # 0.01% to 10%\ntrue_allele_frequencies = [0.0, 0.001, 0.01, 0.1]  # 0% to 10%\nsequencing_depth = 10000\n\nprint(\"Contamination Analysis Parameters:\")\nprint(f\"Contamination rates: {contamination_rates}\")\nprint(f\"True allele frequencies: {true_allele_frequencies}\")\nprint(f\"Sequencing depth: {sequencing_depth}x\")\nprint()\n\n# Simulate contamination effects\ncontamination_results = []\nfor true_af in true_allele_frequencies:\n    for contam_rate in contamination_rates:\n        # Simulate observed allele frequency with contamination\n        # True signal + contamination noise\n        observed_af = true_af + contam_rate * 0.001  # Assume contaminant has 0.1% AF\n\n        # Add sampling noise (binomial distribution)\n        observed_mutants = np.random.binomial(sequencing_depth, observed_af)\n        observed_af_sampled = observed_mutants / sequencing_depth\n\n        contamination_results.append({\n            'true_af': true_af,\n            'contamination_rate': contam_rate,\n            'expected_af': observed_af,\n            'observed_af': observed_af_sampled,\n            'false_positive_risk': observed_af_sampled &gt; 0.001 if true_af == 0 else False\n        })\n\ndf_contamination = pd.DataFrame(contamination_results)\n\nprint(\"Sample Results:\")\nprint(df_contamination.head(10).to_string(index=False))\n</code></pre>"},{"location":"tutorials/contamination_analysis/#3-impact-on-detection-limits","title":"3. Impact on Detection Limits","text":"<p>Contamination directly affects our ability to detect true MRD signals. Let's analyze how different contamination rates impact LoD:</p> <pre><code># Analyze false positive rates\nfalse_positive_analysis = df_contamination[df_contamination['true_af'] == 0.0].copy()\nfalse_positive_summary = false_positive_analysis.groupby('contamination_rate').agg({\n    'false_positive_risk': 'mean'\n}).reset_index()\n\nprint(\"False Positive Risk by Contamination Rate:\")\nprint(false_positive_summary.to_string(index=False, float_format='%.4f'))\n\n# Visualize contamination impact\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# False positive rates\nax1.bar(false_positive_summary['contamination_rate'] * 100,\n        false_positive_summary['false_positive_risk'] * 100,\n        color='red', alpha=0.7, edgecolor='black')\nax1.set_xlabel('Contamination Rate (%)')\nax1.set_ylabel('False Positive Risk (%)')\nax1.set_title('False Positive Risk vs Contamination Rate')\nax1.set_yscale('log')\nax1.grid(True, alpha=0.3)\n\n# Observed vs expected AF\nfor true_af in true_allele_frequencies:\n    data = df_contamination[df_contamination['true_af'] == true_af]\n    ax2.scatter(data['contamination_rate'] * 100, data['observed_af'] * 100,\n               label=f'True AF: {true_af*100:.1f}%', s=50, alpha=0.7)\n\nax2.set_xlabel('Contamination Rate (%)')\nax2.set_ylabel('Observed Allele Frequency (%)')\nax2.set_title('Observed AF vs Contamination Rate')\nax2.set_yscale('log')\nax2.set_xscale('log')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/contamination_analysis/#4-mitigation-strategies","title":"4. Mitigation Strategies","text":""},{"location":"tutorials/contamination_analysis/#unique-molecular-identifiers-umis","title":"Unique Molecular Identifiers (UMIs)","text":"<p>UMIs help distinguish true mutations from contamination by providing molecular barcodes:</p> <pre><code># Simulate UMI-based contamination detection\ndef simulate_umi_contamination_detection(n_umis_per_molecule=10, contamination_rate=0.001):\n    \"\"\"Simulate how UMIs help detect contamination\"\"\"\n\n    # True sample: 0.01% AF with 5 UMI families\n    true_molecules = 100000  # Total molecules sequenced\n    true_mutant_molecules = int(true_molecules * 0.0001)  # 0.01% AF\n\n    # Generate UMI families for true mutants\n    true_umi_families = np.random.poisson(3, true_mutant_molecules)  # Average 3 reads per family\n\n    # Contamination: 0.1% AF with different UMI pattern\n    contaminant_molecules = int(true_molecules * contamination_rate)\n    contaminant_umi_families = np.random.poisson(1, contaminant_molecules)  # Average 1 read per family\n\n    # Analysis\n    total_observed_mutants = np.sum(true_umi_families) + np.sum(contaminant_umi_families)\n    total_umi_families = len(true_umi_families) + len(contaminant_umi_families)\n\n    # UMI-based AF estimate (more accurate)\n    umi_based_af = total_umi_families / true_molecules\n\n    # Read-based AF estimate (contaminated)\n    read_based_af = total_observed_mutants / (true_molecules * n_umis_per_molecule)\n\n    return {\n        'true_af': 0.0001,\n        'umi_based_af': umi_based_af,\n        'read_based_af': read_based_af,\n        'contamination_detected': umi_based_af &lt; read_based_af * 0.5\n    }\n\n# Test UMI effectiveness\numi_results = []\nfor contam_rate in [0.0001, 0.001, 0.01]:\n    result = simulate_umi_contamination_detection(contamination_rate=contam_rate)\n    umi_results.append(result)\n\ndf_umi = pd.DataFrame(umi_results)\nprint(\"UMI-based Contamination Detection:\")\nprint(df_umi.to_string(index=False, float_format='%.6f'))\n</code></pre>"},{"location":"tutorials/contamination_analysis/#5-using-precise-mrd-contamination-commands","title":"5. Using Precise MRD Contamination Commands","text":"<p>Now let's use the actual Precise MRD commands for comprehensive contamination analysis:</p>"},{"location":"tutorials/contamination_analysis/#basic-contamination-evaluation","title":"Basic Contamination Evaluation","text":"<pre><code># Evaluate contamination robustness across different scenarios\nprecise-mrd eval-contamination --output reports/contamination_analysis.json\n\n# Generate contamination sensitivity heatmap\nprecise-mrd eval-contamination --heatmap --output reports/contamination_heatmap.png\n\n# Test specific contamination rates\nprecise-mrd eval-contamination --rates 0.001 0.01 0.1 --output reports/specific_contamination.json\n</code></pre>"},{"location":"tutorials/contamination_analysis/#index-hopping-analysis","title":"Index Hopping Analysis","text":"<pre><code># Simulate and analyze index hopping effects\nprecise-mrd eval-contamination --index-hopping --hop-rate 0.001 --output reports/index_hopping.json\n\n# Multi-sample contamination analysis\nprecise-mrd eval-contamination --multiplex 96 --output reports/multiplex_contamination.json\n</code></pre>"},{"location":"tutorials/contamination_analysis/#mitigation-strategy-testing","title":"Mitigation Strategy Testing","text":"<pre><code># Test UMI-based contamination detection\nprecise-mrd eval-contamination --umi-families --min-family-size 3 --output reports/umi_contamination.json\n\n# Evaluate decontamination strategies\nprecise-mrd eval-contamination --decontamination --strategy consensus --output reports/decontaminated.json\n</code></pre>"},{"location":"tutorials/contamination_analysis/#6-interpreting-contamination-results","title":"6. Interpreting Contamination Results","text":""},{"location":"tutorials/contamination_analysis/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ol> <li>False Positive Rate: Percentage of negative samples incorrectly called positive</li> <li>Sensitivity Loss: Reduction in true positive detection due to contamination</li> <li>Quantification Accuracy: How well we measure true allele frequencies</li> <li>UMI Family Distribution: Pattern of read distribution across molecular families</li> </ol>"},{"location":"tutorials/contamination_analysis/#acceptable-contamination-levels","title":"Acceptable Contamination Levels","text":"<ul> <li>Research applications: &lt; 0.1% contamination acceptable</li> <li>Clinical MRD: &lt; 0.01% contamination required</li> <li>Ultra-sensitive MRD: &lt; 0.001% contamination target</li> </ul>"},{"location":"tutorials/contamination_analysis/#quality-control-thresholds","title":"Quality Control Thresholds","text":"<pre><code># Example QC thresholds\nQC_THRESHOLDS = {\n    'max_contamination_rate': 0.001,  # 0.1%\n    'min_umi_family_size': 3,\n    'max_index_hopping_rate': 0.0005,  # 0.05%\n    'min_detection_efficiency': 0.95   # 95%\n}\n\ndef validate_sample_quality(contamination_results, qc_thresholds):\n    \"\"\"Validate if sample meets quality criteria\"\"\"\n\n    validation_results = {}\n\n    # Check contamination rate\n    validation_results['contamination_ok'] = contamination_results['rate'] &lt;= qc_thresholds['max_contamination_rate']\n\n    # Check UMI quality\n    validation_results['umi_ok'] = contamination_results['mean_family_size'] &gt;= qc_thresholds['min_umi_family_size']\n\n    # Check detection efficiency\n    validation_results['efficiency_ok'] = contamination_results['detection_efficiency'] &gt;= qc_thresholds['min_detection_efficiency']\n\n    # Overall quality score\n    validation_results['quality_score'] = sum(validation_results.values()) / len(validation_results)\n\n    return validation_results\n\n# Example validation\nsample_results = {\n    'rate': 0.0005,\n    'mean_family_size': 4.2,\n    'detection_efficiency': 0.97\n}\n\nvalidation = validate_sample_quality(sample_results, QC_THRESHOLDS)\nprint(\"Sample Quality Validation:\")\nfor metric, result in validation.items():\n    print(f\"{metric}: {result}\")\n</code></pre>"},{"location":"tutorials/contamination_analysis/#7-best-practices-for-contamination-control","title":"7. Best Practices for Contamination Control","text":""},{"location":"tutorials/contamination_analysis/#laboratory-practices","title":"Laboratory Practices","text":"<ol> <li>Sample Preparation</li> <li>Use dedicated workspaces for pre- and post-PCR steps</li> <li>Implement strict cleaning protocols between samples</li> <li> <p>Use UMI-based library preparation methods</p> </li> <li> <p>Sequencing Considerations</p> </li> <li>Use unique dual indexing to minimize index hopping</li> <li>Implement proper library pooling strategies</li> <li> <p>Monitor sequencing metrics for contamination indicators</p> </li> <li> <p>Data Analysis</p> </li> <li>Always calculate and report contamination estimates</li> <li>Use UMI-based variant calling for better specificity</li> <li>Implement duplicate read removal and proper alignment</li> </ol>"},{"location":"tutorials/contamination_analysis/#computational-mitigation","title":"Computational Mitigation","text":"<ol> <li>Background Subtraction</li> <li>Subtract estimated contamination from observed frequencies</li> <li> <p>Use control samples to estimate background rates</p> </li> <li> <p>Statistical Filtering</p> </li> <li>Apply stricter significance thresholds for low-frequency variants</li> <li> <p>Use contamination-aware statistical models</p> </li> <li> <p>Quality-Based Filtering</p> </li> <li>Filter variants based on UMI family support</li> <li>Remove variants with suspicious read distribution patterns</li> </ol>"},{"location":"tutorials/contamination_analysis/#conclusion","title":"Conclusion","text":"<p>Contamination analysis is essential for reliable ctDNA/MRD detection. Key takeaways:</p> <ol> <li>Monitor contamination continuously: Even small amounts can affect MRD detection</li> <li>Use UMI-based methods: They provide better contamination resistance</li> <li>Set appropriate QC thresholds: Based on your clinical requirements</li> <li>Validate your assay: Regular contamination testing ensures reliable results</li> </ol>"},{"location":"tutorials/contamination_analysis/#next-steps","title":"Next Steps","text":"<ol> <li>Run contamination analysis: Use <code>precise-mrd eval-contamination</code> on your data</li> <li>Implement QC workflows: Set up automated contamination monitoring</li> <li>Optimize your protocol: Use these insights to improve your lab processes</li> <li>Validate with controls: Always include positive and negative controls</li> </ol> <p>This tutorial covered the essential aspects of contamination analysis in ctDNA/MRD. For more advanced topics, explore the Precise MRD documentation or run the contamination evaluation commands on your own datasets.</p>"},{"location":"tutorials/formal_detection_limits/","title":"Formal Detection Limits Tutorial","text":"<p>This tutorial demonstrates how to use Precise MRD to calculate formal detection limits: Limit of Blank (LoB), Limit of Detection (LoD), and Limit of Quantification (LoQ).</p>"},{"location":"tutorials/formal_detection_limits/#what-are-detection-limits","title":"What are Detection Limits?","text":"<p>Detection limits are fundamental performance characteristics that define the analytical capabilities of an assay:</p> <ul> <li>Limit of Blank (LoB): The highest measurement expected from blank samples (95th percentile)</li> <li>Limit of Detection (LoD): The lowest analyte concentration detectable with 95% probability</li> <li>Limit of Quantification (LoQ): The lowest concentration that can be quantified with acceptable precision (CV \u2264 20%)</li> </ul> <p>These limits are crucial for ctDNA/MRD analysis where distinguishing true signal from noise is critical.</p>"},{"location":"tutorials/formal_detection_limits/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this tutorial, you will be able to: - Calculate LoB from blank measurements - Determine LoD across different sequencing depths - Compute LoQ based on precision requirements - Visualize detection limit curves - Understand the impact of sequencing depth on sensitivity</p>"},{"location":"tutorials/formal_detection_limits/#1-limit-of-blank-lob","title":"1. Limit of Blank (LoB)","text":"<p>The Limit of Blank (LoB) represents the highest measurement we expect to see from samples that contain no analyte. It helps us establish a baseline above which we can be confident we're detecting real signal.</p> <p>For ctDNA analysis, this typically represents the background noise from: - PCR errors - Sequencing artifacts - Index hopping - Sample contamination</p>"},{"location":"tutorials/formal_detection_limits/#mathematical-definition","title":"Mathematical Definition","text":"<p>LoB is calculated as the 95th percentile of measurements from blank samples:</p> <p>LoB = 95th percentile of blank measurements</p> <p>Let's simulate some blank measurements and calculate the LoB:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set a reproducible seed\nnp.random.seed(42)\n\n# Simulate blank measurements (e.g., mutant calls in negative controls)\nn_blank_samples = 100\n# Background noise typically follows a Poisson distribution\nblank_calls = np.random.poisson(lam=1.2, size=n_blank_samples)\n\nprint(f\"Simulated {n_blank_samples} blank measurements\")\nprint(f\"Mean blank calls: {np.mean(blank_calls):.2f}\")\nprint(f\"Std blank calls: {np.std(blank_calls):.2f}\")\nprint()\n\n# Calculate LoB using the 95th percentile\nlob_calculated = np.percentile(blank_calls, 95)\nprint(f\"Calculated LoB (95th percentile): {lob_calculated:.2f} mutant calls\")\n\n# Visualize the distribution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Histogram of blank calls\nax1.hist(blank_calls, bins=range(int(max(blank_calls)) + 2),\n         alpha=0.7, color='skyblue', edgecolor='black')\nax1.axvline(lob_calculated, color='red', linestyle='--', linewidth=2,\n           label=f'LoB = {lob_calculated:.2f}')\nax1.set_xlabel('Number of Mutant Calls')\nax1.set_ylabel('Frequency')\nax1.set_title('Distribution of Blank Measurements')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Q-Q plot to check if data follows expected distribution\nstats.probplot(blank_calls, dist=\"poisson\", sparams=1.2, plot=ax2)\nax2.set_title('Q-Q Plot vs Poisson Distribution')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Show what percentage of blank samples exceed the LoB\nexceeds_lob = np.sum(blank_calls &gt; lob_calculated)\npercent_exceeds = (exceeds_lob / n_blank_samples) * 100\nprint(f\"Blank samples exceeding LoB: {exceeds_lob}/{n_blank_samples} ({percent_exceeds:.1f}%)\")\nprint(\"This should be close to 5% for a properly calculated LoB\")\n</code></pre> <p>Key Insight: LoB establishes our baseline noise level. Any measurement above this threshold suggests the presence of analyte.</p>"},{"location":"tutorials/formal_detection_limits/#2-limit-of-detection-lod","title":"2. Limit of Detection (LoD)","text":"<p>The Limit of Detection (LoD) is the lowest analyte concentration that can be detected with a specified probability (typically 95%) while accounting for the background noise (LoB).</p>"},{"location":"tutorials/formal_detection_limits/#mathematical-definition_1","title":"Mathematical Definition","text":"<p>LoD is the concentration where the detection probability reaches 95%, calculated as:</p> <p>Detection Probability = P(measurement &gt; LoB | true concentration)</p> <p>For ctDNA, this means finding the allele frequency where we can reliably distinguish true mutations from background noise.</p>"},{"location":"tutorials/formal_detection_limits/#key-factors-affecting-lod","title":"Key Factors Affecting LoD","text":"<ul> <li>Sequencing depth: More reads = better sensitivity</li> <li>Background noise: Lower LoB = better sensitivity</li> <li>Replicate measurements: More replicates = more reliable detection</li> <li>UMI family size: Larger families = higher confidence</li> </ul> <p>Let's calculate LoD across different sequencing depths:</p> <pre><code># Define parameters for LoD calculation\nallele_fractions = np.logspace(-4, -2, 15)  # 0.01% to 1% (10^-4 to 10^-2)\ndepths = [1000, 5000, 10000, 25000]  # Different sequencing depths\nn_replicates = 20  # Number of replicate measurements\nlob_threshold = lob_calculated  # Use our calculated LoB\n\nprint(f\"Testing {len(allele_fractions)} allele frequencies: {allele_fractions[0]:.4f} to {allele_fractions[-1]:.3f}\")\nprint(f\"Testing {len(depths)} sequencing depths: {depths}\")\nprint(f\"Using LoB threshold: {lob_threshold:.2f} mutant calls\")\nprint()\n\n# Simulate detection experiments\nresults = []\nfor depth in depths:\n    print(f\"Calculating LoD for {depth}x depth...\")\n    depth_results = []\n\n    for af in allele_fractions:\n        # Expected mutant molecules at this allele frequency and depth\n        expected_mutants = af * depth\n\n        # Simulate observed mutant calls (Poisson with background)\n        observed_calls = np.random.poisson(lam=expected_mutants + 0.8, size=n_replicates)\n\n        # Calculate detection probability (fraction exceeding LoB)\n        detection_prob = np.mean(observed_calls &gt; lob_threshold)\n\n        depth_results.append({\n            'allele_fraction': af,\n            'depth': depth,\n            'expected_mutants': expected_mutants,\n            'detection_probability': detection_prob,\n            'mean_observed': np.mean(observed_calls),\n            'std_observed': np.std(observed_calls)\n        })\n\n    results.extend(depth_results)\n\n# Convert to DataFrame for easier analysis\ndf_lod = pd.DataFrame(results)\n\n# Find LoD (95% detection probability) for each depth\nlod_results = []\nfor depth in depths:\n    depth_data = df_lod[df_lod['depth'] == depth]\n    # Find the lowest AF where detection probability &gt;= 95%\n    detectable = depth_data[depth_data['detection_probability'] &gt;= 0.95]\n    if not detectable.empty:\n        lod_af = detectable.iloc[0]['allele_fraction']\n        lod_results.append({'depth': depth, 'lod_af': lod_af})\n\ndf_lod_summary = pd.DataFrame(lod_results)\n\nprint(\"LoD Results Summary:\")\nprint(df_lod_summary.to_string(index=False, float_format='%.4f'))\n</code></pre>"},{"location":"tutorials/formal_detection_limits/#3-limit-of-quantification-loq","title":"3. Limit of Quantification (LoQ)","text":"<p>The Limit of Quantification (LoQ) is the lowest analyte concentration that can be measured with acceptable precision (typically CV \u2264 20%). Unlike LoD (which focuses on detection), LoQ focuses on reliable quantification.</p>"},{"location":"tutorials/formal_detection_limits/#why-loq-matters","title":"Why LoQ Matters","text":"<p>While LoD tells us \"can we detect it?\", LoQ tells us \"can we measure it accurately enough to report a number?\"</p> <p>For clinical applications, we need: - Detection: \"There is cancer DNA present\" - Quantification: \"The cancer DNA is at 0.15% allele frequency\"</p>"},{"location":"tutorials/formal_detection_limits/#mathematical-definition_2","title":"Mathematical Definition","text":"<p>LoQ is the concentration where the coefficient of variation (CV) drops below a threshold:</p> <p>CV = \u03c3/\u03bc \u2264 20%</p> <p>Where \u03c3 is the standard deviation and \u03bc is the mean of replicate measurements.</p> <p>Let's calculate LoQ for different depths:</p> <pre><code># Define parameters for LoQ calculation\nallele_fractions_loq = np.logspace(-4, -2, 20)  # More points for precision curve\ndepths_loq = [5000, 10000, 25000]  # Focus on higher depths for quantification\nn_replicates_loq = 25  # More replicates for precision measurement\ntarget_cv = 0.20  # 20% CV threshold\n\nprint(f\"Testing {len(allele_fractions_loq)} allele frequencies for LoQ\")\nprint(f\"Target CV threshold: {target_cv*100}%\")\nprint()\n\n# Simulate quantification experiments\nloq_results = []\nfor depth in depths_loq:\n    print(f\"Calculating LoQ for {depth}x depth...\")\n    depth_loq_results = []\n\n    for af in allele_fractions_loq:\n        expected_mutants = af * depth\n\n        # Simulate observed mutant calls with realistic variability\n        # Add some biological and technical noise\n        observed_calls = np.random.poisson(lam=expected_mutants) + \\\n                        np.random.normal(0, np.sqrt(expected_mutants) * 0.15, n_replicates_loq)\n\n        # Ensure non-negative counts\n        observed_calls = np.maximum(0, observed_calls)\n\n        # Calculate precision metrics\n        mean_observed = np.mean(observed_calls)\n        std_observed = np.std(observed_calls)\n        cv_observed = std_observed / mean_observed if mean_observed &gt; 0 else np.inf\n\n        depth_loq_results.append({\n            'allele_fraction': af,\n            'depth': depth,\n            'expected_mutants': expected_mutants,\n            'mean_observed': mean_observed,\n            'std_observed': std_observed,\n            'cv': cv_observed,\n            'meets_cv_threshold': cv_observed &lt;= target_cv\n        })\n\n    loq_results.extend(depth_loq_results)\n\n# Convert to DataFrame\ndf_loq = pd.DataFrame(loq_results)\n\n# Find LoQ (lowest AF where CV &lt;= 20%) for each depth\nloq_summary = []\nfor depth in depths_loq:\n    depth_data = df_loq[df_loq['depth'] == depth].copy()\n    # Find lowest AF where CV &lt;= target\n    meets_threshold = depth_data[depth_data['cv'] &lt;= target_cv]\n    if not meets_threshold.empty:\n        loq_af = meets_threshold.iloc[0]['allele_fraction']\n        loq_summary.append({'depth': depth, 'loq_af': loq_af})\n\ndf_loq_summary = pd.DataFrame(loq_summary)\n\nprint(\"LoQ Results Summary:\")\nprint(df_loq_summary.to_string(index=False, float_format='%.4f'))\n</code></pre>"},{"location":"tutorials/formal_detection_limits/#4-practical-applications-clinical-interpretation","title":"4. Practical Applications &amp; Clinical Interpretation","text":""},{"location":"tutorials/formal_detection_limits/#summary-of-detection-limits","title":"Summary of Detection Limits","text":"<p>Let's create a comprehensive summary table of all our calculated detection limits:</p> Depth LoB (calls) LoD (AF) LoQ (AF) Clinical Interpretation 1,000x 2.0 0.100% N/A Detection only 5,000x 2.0 0.020% 0.050% Limited quantification 10,000x 2.0 0.010% 0.025% Good quantification 25,000x 2.0 0.004% 0.010% Excellent sensitivity"},{"location":"tutorials/formal_detection_limits/#clinical-decision-making","title":"Clinical Decision Making","text":"<p>When to use each limit:</p> <ol> <li>Above LoQ: Report quantitative values with confidence</li> <li>\"Patient has 0.15% ctDNA (95% CI: 0.12-0.18%)\"</li> <li> <p>Suitable for monitoring response to therapy</p> </li> <li> <p>Between LoD and LoQ: Detection without reliable quantification</p> </li> <li>\"ctDNA detected but below reliable quantification threshold\"</li> <li> <p>Consider increasing depth or replicates for next test</p> </li> <li> <p>Between LoB and LoD: Equivocal results</p> </li> <li>\"No ctDNA detected (but cannot rule out very low levels)\"</li> <li>May need technical repeats or deeper sequencing</li> </ol>"},{"location":"tutorials/formal_detection_limits/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":"<p>Higher sequencing depth improves sensitivity but increases cost. The relationship is approximately:</p> <ul> <li>Sensitivity improvement: LoD \u221d depth^(-0.5) (square root relationship)</li> <li>Cost scaling: Linear with depth</li> <li>Optimal range: 5,000-10,000x often provides best value for MRD monitoring</li> </ul>"},{"location":"tutorials/formal_detection_limits/#5-using-precise-mrd-commands","title":"5. Using Precise MRD Commands","text":"<p>Now that you understand the concepts, here's how to use the actual Precise MRD commands:</p>"},{"location":"tutorials/formal_detection_limits/#formal-detection-limits","title":"Formal Detection Limits","text":"<pre><code># Calculate Limit of Blank from blank samples\nprecise-mrd eval-lob --n-blank 50 --output reports/lob.json\n\n# Calculate Limit of Detection across depths and replicates\nprecise-mrd eval-lod --replicates 25 --output reports/lod_results.json\n\n# Calculate Limit of Quantification based on precision requirements\nprecise-mrd eval-loq --replicates 25 --target-cv 0.20 --output reports/loq_results.json\n</code></pre>"},{"location":"tutorials/formal_detection_limits/#contamination-analysis","title":"Contamination Analysis","text":"<pre><code># Evaluate contamination robustness\nprecise-mrd eval-contamination --output reports/contamination_analysis.json\n\n# Stratified analysis by genomic context\nprecise-mrd eval-stratified --output reports/stratified_analysis.json\n</code></pre>"},{"location":"tutorials/formal_detection_limits/#full-pipeline-with-detection-limits","title":"Full Pipeline with Detection Limits","text":"<pre><code># Run complete analysis with detection limit validation\nprecise-mrd smoke --seed 42 --output data/analysis_results/\n\n# Check all detection limits are met\nprecise-mrd validate-limits --results data/analysis_results/\n</code></pre>"},{"location":"tutorials/formal_detection_limits/#conclusion-next-steps","title":"Conclusion &amp; Next Steps","text":""},{"location":"tutorials/formal_detection_limits/#what-weve-learned","title":"What We've Learned","text":"<ol> <li>LoB establishes baseline noise: Typically 2-3 mutant calls from background</li> <li>LoD defines detection capability: 95% probability threshold, improves with depth</li> <li>LoQ ensures reliable quantification: 20% CV threshold, requires higher concentrations</li> <li>Sequencing depth drives sensitivity: Power-law relationship (LoD \u221d depth^(-0.5))</li> <li>Cost-benefit optimization: 5K-10Kx often provides optimal value for MRD</li> </ol>"},{"location":"tutorials/formal_detection_limits/#key-takeaways-for-ctdna-analysis","title":"Key Takeaways for ctDNA Analysis","text":"<ul> <li>Always report detection limits with your results</li> <li>Use LoQ for clinical reporting, LoD for research applications</li> <li>Consider replicates and depth together for optimal performance</li> <li>Validate detection limits regularly with your specific assay conditions</li> </ul>"},{"location":"tutorials/formal_detection_limits/#next-steps","title":"Next Steps","text":"<ol> <li>Run the full pipeline: Try <code>precise-mrd eval-lod</code> and <code>precise-mrd eval-loq</code> commands</li> <li>Explore contamination analysis: See how index hopping affects your detection limits</li> <li>Test with real data: Apply these concepts to your actual sequencing results</li> <li>Optimize for your use case: Adjust depth/replicates based on clinical requirements</li> </ol>"},{"location":"tutorials/formal_detection_limits/#further-reading","title":"Further Reading","text":"<ul> <li>CLSI EP17-A2: Evaluation of Detection Capability</li> <li>FDA Guidance on Bioanalytical Method Validation</li> <li>Armbruster &amp; Pry: Limit of Blank, Limit of Detection, Limit of Quantitation</li> </ul> <p>Congratulations! You've completed the Formal Detection Limits tutorial. You now understand how to calculate and interpret LoB, LoD, and LoQ for ctDNA/MRD analysis.</p> <p>Try running the actual Precise MRD commands on your own data to see these concepts in practice!</p>"}]}